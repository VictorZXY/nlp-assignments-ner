{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_L90_practical_session3_MT2021_xz398",
      "provenance": [],
      "collapsed_sections": [
        "_JjS421cSvIV",
        "fyx4FzrBSPWk",
        "3l3r1hj9tsv-",
        "gCiMJUG0t15b",
        "xY_dgC3cEqed",
        "IMj7LjWVikBs",
        "pLyhy2siCg0f"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y2GAIotpgwn"
      },
      "source": [
        "# Overview of NLP (L90) Practical Session 3\n",
        "\n",
        "Welcome to the third practical accompanying the Overview of NLP (L90) lecture course. Overall, the purpose of the practicals has been to build and evaluate an NLP system, specifically for named entity recognition (NER).\n",
        "\n",
        "This session is the final one of our 3-part practical task.\n",
        "\n",
        "1.   Explore and annotate a named entity recognition dataset (worth 10%) [link to colab](https://colab.research.google.com/drive/1J_jXBEFfxbDDuI_NcJpZPmubpNQCG6Kf?usp=sharing)\n",
        "2.   Attempt feature-based NER (last time, worth 10%) [link to colab](https://colab.research.google.com/drive/1wUJ-9bmNmSZUlgA6TGQII6cJeGi9sBmw?usp=sharing)\n",
        "3.   **Attempt NER with neural networks and write a report (this session, assignment due 3 December 3pm, worth 80%)**\n",
        "\n",
        "You might find it useful to watch the short video we recorded last year (on the [part II](https://www.cl.cam.ac.uk/teaching/2122/NLP/video/) and [ACS](https://www.cl.cam.ac.uk/teaching/2122/L90/video/) teaching pages) -- there's a minor difference in that last year we had 40 on the course, this year 26. Also there will be an in-person discussion session on 10 November at 3pm in the Intel Lab: please come along to discuss the previous assignment and this new one!\n",
        "\n",
        "Note that all submissions are made via the [course Moodle page](https://www.vle.cam.ac.uk/course/view.php?id=206751). We explain more what we expect in your reports at the end of this notebook. Ok, let's continue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTXDWNfjxcHH"
      },
      "source": [
        "## Recap: Practical 1\n",
        "\n",
        "First let's look back at the first assignment from practical 1, when we asked you to annotate 50 tweets from the training set, now that we've received all your submissions. Hopefully you received some feedback on Moodle, including your agreement kappa with the original file (if not, let us know: apc38).\n",
        "\n",
        "Overall, the agreement among the 26 of you was a Fleiss's kappa of .708 (= 'substantial' agreement per Landis & Koch 1977, 'The measurement of observer agreement for categorical data' -- this is pretty good for NLP annotation, in our experience).\n",
        "\n",
        "If we take the same max-recall approach as the task organisers did -- accepting all named entity labels (with a bit of downstream inspection) -- then we'd have a new training file in which there are 111 named entities (compared to 41 in the original file). If we instead take a majority vote approach then we'd have 47 named entities. So we could end up with quite different training data, with a greater variety of named entities identified, if we were to continue this group annotation process on the whole training file.\n",
        "\n",
        "This is obviously not complete agreement but it would be unrealistic to expect that for all but the most trivial of natural language annotation tasks, as you probably realise by now. The more practical question is: how do we handle the inevitable disagreements in language annotation? Note that besides the majority vote or max-recall ways of handling multiple annotation, there has been work on making use of disagreement as a useful training signal in computational modelling (for just one example see [Plank et al 2014](https://aclanthology.org/E14-1078/)), and also a review of how to use such information in evaluation ([Basile et al 2021](https://aclanthology.org/2021.bppf-1.3/)).\n",
        "\n",
        "There was one entity in the original file which you unanimously decided is not in fact a named entity: `vodka B-product`. And there were 16 word tokens which you labelled as being part of named entities, but which were not in the gold file:\n",
        "\n",
        "```\n",
        " 1 Green\n",
        " 2 Newsfeed\n",
        " 3 london\n",
        " 4 eurovision\n",
        " 5 cody\n",
        " 6 NY\n",
        " 7 Epix      \n",
        " 8 Movie     \n",
        " 9 Channel   \n",
        "10 The       \n",
        "11 DeAndre   \n",
        "12 Way       \n",
        "13 1ST       \n",
        "14 AVE       \n",
        "15 HKY       \n",
        "16 Break\n",
        "```\n",
        "\n",
        "Most of these indeed seem to be named entities: it's not clear why 'Green Newsfeed', 'london fashion week', and 'eurovision' weren't identified as named entities in the original file (perhaps regional effects: recall that the training file annotator was US-based).\n",
        "\n",
        "'cody' is debatable, because it's clearly a person (the context: \"hopefully we got cody's ipod in the mail today\") but not as far as we know a well known person. So this touches on a philosophical problem with NER: at what point does someone become sufficiently widely known for them to be considered a _person_ in a named entity sense? (see also 'wood', someone's nickname: \"Wingo tellin me and wood...\").\n",
        "\n",
        "'1ST AVE HKY' appears to refer to an address in Hickory, North Carolina, USA (or at least: that's what a Google search indicates). The reason for this entity not being identified by the annotators is another common problem: that of local places not being generally known. In a sense, this is one way to decide if a location is of interest, by revealing how well-known it is: but this leads to a problem of bias relating to the geographical knowledge and experience of the annotators.\n",
        "\n",
        "We'll let you look at the texts again and discuss these proposed additional named entities in your final report, if you wish to (no requirement, but it might enrich it). Here's the [sample file](https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_sample_blank.txt) again for convenience.\n",
        "\n",
        "Remember, however high or low your agreement score with the original annotation, it's not so much a matter of 'correct' or 'incorrect', but more 'agreement' and 'disagreement'. What that tells us about language annotation, and what we do with disagreements, are the interesting questions we think."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2ky1iGMu4bI"
      },
      "source": [
        "### Assignment 2\n",
        "\n",
        "We'll be in touch soon with feedback relating to assignment 2 (remember that it's a tick). If you don't see feedback on Moodle within one week of the deadline please get in touch (apc38) -- also we've had reports that you may need to clear your Moodle cache so that you can see our comments (we reported this problem to the Moodle helpdesk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBFw8sHOsgvu"
      },
      "source": [
        "## Today: Practical 3\n",
        "\n",
        "Today we'll set your final assignment which counts for 80% of your grade. It involves a neural network approach to the NER task, and you are required to submit a written report about the practical work as a whole. We'll explain the task and the assignment requirements some more below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G0VXjKb_sh7"
      },
      "source": [
        "### Neural network approaches to NLP\n",
        "\n",
        "You'll no doubt be aware from the lectures and from your general knowledge that neural networks have had a transformative effect on NLP in recent years. So we thought it apt to build a neural network classifier for NER in this practical. We can't assume that it will lead to an improvement in performance, because \"deep learning\" isn't always best (despite the hype), but wanted to have a look at the implementation details of a neural network in practice. Note that we assume that you've picked up a conceptual framework for neural networks from the lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_OcWnnxq4st"
      },
      "source": [
        "### Preparing the training data\n",
        "\n",
        "Ok let's return to what we did in practical 2 when we prepared the training texts by converting all PoS-tags and named entity labels to integer values. This is what the training data looks like at first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dvj1o6eDhuH"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "def init_random_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nc6MmGBrWvU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "outputId": "5cba322c-11fb-482b-a116-e98004f3360d"
      },
      "source": [
        "import pandas as pd\n",
        "wnuttrain = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt'\n",
        "train = pd.read_table(wnuttrain, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "# NB: don't drop the empty lines between texts yet, they are needed for sequence splits (they show up as NaN in the data frame)\n",
        "train.head(n=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@paulwalk</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'s</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>AUX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>view</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>where</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>'m</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>living</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>for</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>two</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NUM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>weeks</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Empire</td>\n",
              "      <td>B-location</td>\n",
              "      <td>B</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>State</td>\n",
              "      <td>I-location</td>\n",
              "      <td>I</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Building</td>\n",
              "      <td>I-location</td>\n",
              "      <td>I</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>=</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>SYM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ESB</td>\n",
              "      <td>B-location</td>\n",
              "      <td>B</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Pretty</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>bad</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>storm</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>here</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>last</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>evening</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>From</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Green</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token       label bio_only   upos\n",
              "0   @paulwalk           O        O   NOUN\n",
              "1          It           O        O   PRON\n",
              "2          's           O        O    AUX\n",
              "3         the           O        O    DET\n",
              "4        view           O        O   NOUN\n",
              "5        from           O        O    ADP\n",
              "6       where           O        O    ADV\n",
              "7           I           O        O   PRON\n",
              "8          'm           O        O      X\n",
              "9      living           O        O   NOUN\n",
              "10        for           O        O    ADP\n",
              "11        two           O        O    NUM\n",
              "12      weeks           O        O   NOUN\n",
              "13          .           O        O  PUNCT\n",
              "14     Empire  B-location        B  PROPN\n",
              "15      State  I-location        I  PROPN\n",
              "16   Building  I-location        I  PROPN\n",
              "17          =           O        O    SYM\n",
              "18        ESB  B-location        B  PROPN\n",
              "19          .           O        O  PUNCT\n",
              "20     Pretty           O        O    ADV\n",
              "21        bad           O        O    ADJ\n",
              "22      storm           O        O   NOUN\n",
              "23       here           O        O    ADV\n",
              "24       last           O        O    ADJ\n",
              "25    evening           O        O   NOUN\n",
              "26          .           O        O  PUNCT\n",
              "27        NaN         NaN      NaN    NaN\n",
              "28       From           O        O    ADP\n",
              "29      Green           O        O  PROPN"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeXaMEjQruL-"
      },
      "source": [
        "This time, the input to our neural classifier will be the words themselves, so let's create an index for our word types and then use this to convert our word tokens into integers. As a result, every instance of word token 'It' will be replaced with integer 1 (note that our vocabulary is case sensitive; quite often in NLP all words are lower-cased before indexing but for the task of NER we know that character casing is useful information to retain)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwMBhBFZrm3Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "outputId": "bb4b657a-e323-4703-9032-5fd47d24c022"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# in order to convert word tokens to integers: list the set of token types\n",
        "token_vocab = train.token.unique().tolist()\n",
        "oov = len(token_vocab)  # OOV (out of vocabulary) token as vocab length (because that's max.index + 1)\n",
        "\n",
        "# convert word tokens to integers\n",
        "def token_index(tok):\n",
        "    ind = tok\n",
        "    if not pd.isnull(tok):  # new since last time: deal with the empty lines which we didn't drop yet\n",
        "        if tok in token_vocab:  # if token in vocabulary\n",
        "            ind = token_vocab.index(tok)\n",
        "        else:  # else it's OOV\n",
        "            ind = oov\n",
        "    return ind\n",
        "\n",
        "# training labels: convert BIO to integers\n",
        "def bio_index(bio):\n",
        "    ind = bio\n",
        "    if not pd.isnull(bio):  # deal with empty lines\n",
        "        if bio=='B':\n",
        "            ind = 0\n",
        "        elif bio=='I':\n",
        "            ind = 1\n",
        "        elif bio=='O':\n",
        "            ind = 2\n",
        "    return ind\n",
        "\n",
        "# pass a data frame through our feature extractor\n",
        "def extract_features(txt,istest=False):\n",
        "    txt_copy = txt.copy()\n",
        "    tokinds = [token_index(u) for u in txt_copy['token']]\n",
        "    txt_copy['token_indices'] = tokinds\n",
        "    if not istest:  # can't do this with the test set\n",
        "        bioints = [bio_index(b) for b in txt_copy['bio_only']]\n",
        "        txt_copy['bio_only'] = bioints\n",
        "    return txt_copy\n",
        "\n",
        "train_copy = extract_features(train)\n",
        "train_copy.head(n=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "      <th>token_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@paulwalk</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PRON</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'s</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>AUX</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>DET</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>view</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>where</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PRON</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>'m</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>X</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>living</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>for</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>two</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NUM</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>weeks</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Empire</td>\n",
              "      <td>B-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>State</td>\n",
              "      <td>I-location</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Building</td>\n",
              "      <td>I-location</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>=</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>SYM</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ESB</td>\n",
              "      <td>B-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Pretty</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>bad</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>storm</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>here</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>last</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>evening</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>From</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Green</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token       label  bio_only   upos  token_indices\n",
              "0   @paulwalk           O       2.0   NOUN            0.0\n",
              "1          It           O       2.0   PRON            1.0\n",
              "2          's           O       2.0    AUX            2.0\n",
              "3         the           O       2.0    DET            3.0\n",
              "4        view           O       2.0   NOUN            4.0\n",
              "5        from           O       2.0    ADP            5.0\n",
              "6       where           O       2.0    ADV            6.0\n",
              "7           I           O       2.0   PRON            7.0\n",
              "8          'm           O       2.0      X            8.0\n",
              "9      living           O       2.0   NOUN            9.0\n",
              "10        for           O       2.0    ADP           10.0\n",
              "11        two           O       2.0    NUM           11.0\n",
              "12      weeks           O       2.0   NOUN           12.0\n",
              "13          .           O       2.0  PUNCT           13.0\n",
              "14     Empire  B-location       0.0  PROPN           14.0\n",
              "15      State  I-location       1.0  PROPN           15.0\n",
              "16   Building  I-location       1.0  PROPN           16.0\n",
              "17          =           O       2.0    SYM           17.0\n",
              "18        ESB  B-location       0.0  PROPN           18.0\n",
              "19          .           O       2.0  PUNCT           13.0\n",
              "20     Pretty           O       2.0    ADV           19.0\n",
              "21        bad           O       2.0    ADJ           20.0\n",
              "22      storm           O       2.0   NOUN           21.0\n",
              "23       here           O       2.0    ADV           22.0\n",
              "24       last           O       2.0    ADJ           23.0\n",
              "25    evening           O       2.0   NOUN           24.0\n",
              "26          .           O       2.0  PUNCT           13.0\n",
              "27        NaN         NaN       NaN    NaN            NaN\n",
              "28       From           O       2.0    ADP           26.0\n",
              "29      Green           O       2.0  PROPN           27.0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SwUfFS8-mZz"
      },
      "source": [
        "Now we need to convert the table format to text sequences, ready for input to our neural network classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF6c9JrvYTa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "c7d70ddb-77d6-4d44-b25a-8e3dcbbeb49c"
      },
      "source": [
        "def tokens2sequences(txt_in,istest=False):\n",
        "    '''\n",
        "    Takes panda dataframe as input, copies, and adds a sequence index based on full-stops.\n",
        "    Outputs a dataframe with sequences of tokens, named entity labels, and token indices as lists.\n",
        "    '''\n",
        "    txt = txt_in.copy()\n",
        "    txt['sequence_num'] = 0\n",
        "    seqcount = 0\n",
        "    for i in txt.index:  # in each row...\n",
        "        txt.loc[i,'sequence_num'] = seqcount  # set the sequence number\n",
        "        if pd.isnull(txt.loc[i,'token']):  # increment sequence counter at empty lines\n",
        "            seqcount += 1\n",
        "    # now drop the empty lines, group by sequence number and output df of sequence lists\n",
        "    txt = txt.dropna()\n",
        "    if istest:  # looking ahead: the test set doesn't have labels\n",
        "        txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'token_indices']].agg(lambda x: list(x))\n",
        "    else:  # the dev and training sets do have labels\n",
        "        txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'bio_only', 'token_indices']].agg(lambda x: list(x))\n",
        "    return txt_seqs\n",
        "\n",
        "print(\"This cell takes a little while to run: be patient :)\")\n",
        "train_seqs = tokens2sequences(train_copy)\n",
        "train_seqs.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This cell takes a little while to run: be patient :)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[@paulwalk, It, 's, the, view, from, where, I,...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[From, Green, Newsfeed, :, AHFA, extends, dead...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Pxleyes, Top, 50, Photography, Contest, Pictu...</td>\n",
              "      <td>[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[today, is, my, last, day, at, the, office, .]</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]</td>\n",
              "      <td>[51.0, 52.0, 53.0, 23.0, 54.0, 55.0, 3.0, 56.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[4Dbling, 's, place, til, monday, ,, party, pa...</td>\n",
              "      <td>[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                      token_indices\n",
              "0             0  ...  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...\n",
              "1             1  ...  [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
              "2             2  ...  [39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....\n",
              "3             3  ...  [51.0, 52.0, 53.0, 23.0, 54.0, 55.0, 3.0, 56.0...\n",
              "4             4  ...  [57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crAB1VtPw6Mh"
      },
      "source": [
        "Next we need to figure out the input text length we'll be passing to our neural networks. Neural nets are of pre-defined dimensions, and so we need all texts to carry the same number of tokens. So our first question is: what's the longest sequence in our dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "263V10sYg2l-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a46e5c-f39d-42b0-f524-782077606b4f"
      },
      "source": [
        "def find_longest_sequence(txt,longest_seq):\n",
        "    '''find the longest sequence in the dataframe'''\n",
        "    for i in txt.index:\n",
        "        seqlen = len(txt['token'][i])\n",
        "        if seqlen > longest_seq:  # update high water mark if new longest sequence encountered\n",
        "            longest_seq = seqlen\n",
        "    return longest_seq\n",
        "\n",
        "train_longest = find_longest_sequence(train_seqs, 0)\n",
        "print('The longest sequence in the training set is %i tokens long' % train_longest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The longest sequence in the training set is 41 tokens long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNRd4QQ_O8Vh"
      },
      "source": [
        "Let's also load and check the longest sequence length in the dev and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQzXQaJpkyfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d713b907-fc4e-4664-e2af-1ef5aa03c6ea"
      },
      "source": [
        "# the dev set\n",
        "wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'\n",
        "dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "dev_copy = extract_features(dev)\n",
        "dev_seqs = tokens2sequences(dev_copy)\n",
        "dev_longest = find_longest_sequence(dev_seqs, 0)\n",
        "print('The longest sequence in the dev set is %i tokens long' % dev_longest)\n",
        "\n",
        "# the test set\n",
        "wnuttest = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_clean_tagged.txt'\n",
        "test = pd.read_table(wnuttest, header=None, names=['token', 'upos'])\n",
        "test_copy = extract_features(test, True)\n",
        "test_seqs = tokens2sequences(test_copy, True)\n",
        "test_longest = find_longest_sequence(test_seqs, 0)\n",
        "print('The longest sequence in the test set is %i tokens long' % test_longest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The longest sequence in the dev set is 82 tokens long\n",
            "The longest sequence in the test set is 105 tokens long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd8AZTiqUmI8",
        "outputId": "02bb745e-1bed-4f69-e5e1-b9c449e6cf83"
      },
      "source": [
        "def find_sequence_lengths(txt):\n",
        "    seq_lengths = []\n",
        "    for i in txt.index:\n",
        "        seq_lengths.append(len(txt['token'][i]))\n",
        "    return seq_lengths\n",
        "\n",
        "train_seq_lengths = find_sequence_lengths(train_seqs)\n",
        "print('Training set sequences: ')\n",
        "print(f'n = {len(train_seq_lengths)}, min = {min(train_seq_lengths)}, max = {max(train_seq_lengths)}, mean = {np.mean(train_seq_lengths)}, median = {np.median(train_seq_lengths)}')\n",
        "print()\n",
        "\n",
        "dev_seq_lengths = find_sequence_lengths(dev_seqs)\n",
        "print('Dev set sequences: ')\n",
        "print(f'n = {len(dev_seq_lengths)}, min = {min(dev_seq_lengths)}, max = {max(dev_seq_lengths)}, mean = {np.mean(dev_seq_lengths)}, median = {np.median(dev_seq_lengths)}')\n",
        "print()\n",
        "\n",
        "test_seq_lengths = find_sequence_lengths(test_seqs)\n",
        "print('Test set sequences: ')\n",
        "print(f'n = {len(test_seq_lengths)}, min = {min(test_seq_lengths)}, max = {max(test_seq_lengths)}, mean = {np.mean(test_seq_lengths)}, median = {np.median(test_seq_lengths)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set sequences: \n",
            "n = 3375, min = 1, max = 41, mean = 18.440296296296296, median = 18.0\n",
            "\n",
            "Dev set sequences: \n",
            "n = 993, min = 1, max = 82, mean = 15.49043303121853, median = 13.0\n",
            "\n",
            "Test set sequences: \n",
            "n = 1283, min = 1, max = 105, mean = 18.178487918939986, median = 15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "0Y5qZ0XLWFlO",
        "outputId": "25bc672d-2f00-4b97-d1a7-ded33c138420"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 4), sharex=True, sharey=True)\n",
        "\n",
        "axes[0].hist(train_seq_lengths, bins=40, color='tab:blue')\n",
        "axes[0].set_ylabel('Frequency', fontsize=16)\n",
        "axes[0].set_title('Training set', fontsize=16)\n",
        "\n",
        "axes[1].hist(dev_seq_lengths, bins=40, color='tab:orange')\n",
        "axes[1].set_xlabel('Sequence length', fontsize=16)\n",
        "axes[1].set_title('Dev set', fontsize=16)\n",
        "\n",
        "axes[2].hist(test_seq_lengths, bins=40, color='tab:green')\n",
        "axes[2].set_title('Test set', fontsize=16)\n",
        "\n",
        "plt.savefig('sequence-lengths-histogram.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwiId7sckuHL"
      },
      "source": [
        "Overall the 105 length sequence in the test set is the longest. So for our initial experiments let's fix the input length at 105 tokens, padding out any sequences which are shorter than this length (note that texts can be truncated too, they don't have to be padded). We do have access to the test set in this shared task, so let's use the information available to us; but in some shared tasks you don't get access to the test set (which is why your code needs to be able to handle truncation as well as padding). Feel free to experiment with this sequence length in your assignment.\n",
        "\n",
        "To pad or truncate the sequences, we can use the handy `pad_sequences` function in Keras, a deep learning API written in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Spc-hutkTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82077d9c-897c-4def-ede1-82e84b65b34d"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# set maximum sequence length\n",
        "seq_length = test_longest\n",
        "\n",
        "# a new dummy token index, one more than OOV\n",
        "padtok = oov + 1\n",
        "print('The padding token index is %i' % padtok)\n",
        "\n",
        "# use pad_sequences, padding or truncating at the end of the sequence (default is 'pre')\n",
        "train_seqs_padded = pad_sequences(train_seqs['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                  dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "print('Example of padded token sequence:')\n",
        "print(train_seqs_padded[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The padding token index is 14802\n",
            "Example of padded token sequence:\n",
            "[   26    27    28    29    30    31    32    10    33    34    35    36\n",
            "    13    37    38 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLOANQCQLucN"
      },
      "source": [
        "All those 14802s after 10 (the full stop) are padding, so that's our training texts now ready as fixed-length sequences.\n",
        "\n",
        "Now in named entity recognition we are predicting a label for each input token -- a 'many-to-many' or 'sequence labelling' scenario analogous to time series prediction (which is why you sometimes see 'time' alluded to in the network layers themselves). Every input token requires an output token: the words in the original sentence have a label 0, 1 or 2, and we'll add the label 3 for padding tokens.\n",
        "\n",
        "So that the labels are prepared as binary values, we convert the label sequences to a one-hot encoding using the keras `to_categorical` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iRzVlQ8MKxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c2f166-9d22-4a54-dc82-627b460a3e60"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# get lists of named entity labels, padded with a null label (=3)\n",
        "padlab = 3\n",
        "train_labs_padded = pad_sequences(train_seqs['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                  dtype='int32', padding='post', truncating='post', value=padlab)\n",
        "\n",
        "# convert those labels to one-hot encoding\n",
        "n_labs = 4  # we have 3 labels: B, I, O (0, 1, 2) + the pad label 3\n",
        "train_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_padded]\n",
        "\n",
        "# follow the print outputs below to see how the labels are transformed\n",
        "print('Example of padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(train_seqs.loc[1])\n",
        "print('Length of input sequence: %i' % len(train_seqs_padded[1]))\n",
        "print('Length of label sequence: %i' % len(train_labs_onehot[1]))\n",
        "print(train_labs_padded[1][:11])\n",
        "print(train_labs_onehot[1][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     1\n",
            "token            [From, Green, Newsfeed, :, AHFA, extends, dead...\n",
            "bio_only         [2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, ...\n",
            "token_indices    [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
            "Name: 1, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of label sequence: 105\n",
            "[2 2 2 2 0 2 2 2 2 2 2]\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVDxGnypO_Rd"
      },
      "source": [
        "Now we need to do the same to the dev set (pad the token and label sequences, and convert the latter to one-hot encoding):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBy8RT93PG6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ed4c44-b596-4c11-c3ca-2d49d675ee9b"
      },
      "source": [
        "# now process the dev set in the same way: padding the tokens & labels, and one-hot encoding the labels\n",
        "dev_seqs_padded = pad_sequences(dev_seqs['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "dev_labs_padded = pad_sequences(dev_seqs['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                dtype='int32', padding='post', truncating='post', value=padlab)\n",
        "dev_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in dev_labs_padded]\n",
        "\n",
        "print('Dev set padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(dev_seqs.loc[2])\n",
        "print('Length of input sequence: %i' % len(dev_seqs_padded[1]))\n",
        "print('Length of label sequence: %i' % len(dev_labs_onehot[1]))\n",
        "print(dev_labs_padded[2][:11])\n",
        "print(dev_labs_onehot[2][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev set padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     2\n",
            "token            [All, I, ', ve, been, doing, is, BINGE, watchi...\n",
            "bio_only         [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...\n",
            "token_indices    [405.0, 7.0, 573.0, 12927.0, 90.0, 848.0, 52.0...\n",
            "Name: 2, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of label sequence: 105\n",
            "[2 2 2 2 2 2 2 2 2 0 1]\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JjS421cSvIV"
      },
      "source": [
        "### Building a neural network classifier\n",
        "\n",
        "Ok that's everything prepared for neural network classification. Let's build a basic model using Keras and Tensorflow, two popular libraries for neural network machine learning.\n",
        "\n",
        "First we prepare the input data as numpy arrays, list the metrics for model evaluation, define some important hyperparameters, then build the model layer by layer. It's a sequential model with an embedding layer followed by bidirectional LSTM, then a dropout layer before a final dense layer with softmax activation. We have early stopping criteria to halt training if improvements are not seen after a certain number of epochs, and at the end we print the model summary which shows the shape of the model layer by layer and number of parameters we are training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi8Xpmfd75-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260d1a55-042a-430c-aa31-778b8f7cda29"
      },
      "source": [
        "# load Keras and TensorFlow\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# prepare sequences and labels as numpy arrays, check dimensions\n",
        "X = np.array(train_seqs_padded)\n",
        "y = np.array(train_labs_onehot)\n",
        "print('Input sequence dimensions (n.docs, seq.length):')\n",
        "print(X.shape)\n",
        "print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n",
        "print(y.shape)\n",
        "\n",
        "# our final vocab size is the padding token + 1 (OR length of vocab + OOV + PAD)\n",
        "vocab_size = padtok+1\n",
        "print(vocab_size==len(token_vocab)+2)\n",
        "embed_size = 128  # try an embedding size of 128 (could tune this)\n",
        "\n",
        "#list of metrics to use: true & false positives, negatives, accuracy, precision, recall, area under the curve\n",
        "METRICS = [\n",
        "    keras.metrics.TruePositives(name='tp'),\n",
        "    keras.metrics.FalsePositives(name='fp'),\n",
        "    keras.metrics.TrueNegatives(name='tn'),\n",
        "    keras.metrics.FalseNegatives(name='fn'), \n",
        "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    keras.metrics.Precision(name='precision'),\n",
        "    keras.metrics.Recall(name='recall'),\n",
        "    keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "# our model has the option for an label prediction bias, it's sequential, starts with an embedding layer, then bi-LSTM,\n",
        "# a dropout layer follows for regularisation, and a dense final layer with softmax activation to output class probabilities\n",
        "# we compile with the Adam optimizer at a low learning rate, use categorical cross-entropy as our loss function\n",
        "def make_model(metrics=METRICS, output_bias=None, seed=42):\n",
        "    init_random_seed(seed)\n",
        "    if output_bias is not None:\n",
        "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_length, mask_zero=True, trainable=True),\n",
        "        keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),  # 2 directions, 50 units each, concatenated (can change this)\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias)),\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)\n",
        "    return model\n",
        "\n",
        "# early stopping criteria based on area under the curve: will stop if no improvement after 10 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', verbose=1, patience=10, mode='max', restore_best_weights=True)\n",
        "\n",
        "# the number of training epochs we'll use, and the batch size (how many texts are input at once)\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print('**Defining a neural network**')\n",
        "model = make_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence dimensions (n.docs, seq.length):\n",
            "(3375, 105)\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):\n",
            "(3375, 105, 4)\n",
            "True\n",
            "**Defining a neural network**\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 105, 128)          1894784   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 105, 100)         71600     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 105, 100)          0         \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 105, 4)           404       \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,966,788\n",
            "Trainable params: 1,966,788\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "CkVYfQMBeX2S",
        "outputId": "28c9bfc8-2790-4f6f-c10f-d8a465090331"
      },
      "source": [
        "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPTMXiJFgDeA"
      },
      "source": [
        "Check the model works, emitting predictions even without being fitted to our training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Asi7y7Z9Gnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49412fc5-bbd0-423f-b244-2cf13a8274f5"
      },
      "source": [
        "# check the model works on the first 10 texts\n",
        "model.predict(X[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.26264954, 0.30658948, 0.29201552, 0.13874543],\n",
              "        [0.25982302, 0.30482107, 0.295707  , 0.13964888],\n",
              "        [0.25995025, 0.30405137, 0.29404813, 0.14195028],\n",
              "        ...,\n",
              "        [0.26684064, 0.29940453, 0.29173052, 0.14202441],\n",
              "        [0.2661049 , 0.30062246, 0.2918345 , 0.14143816],\n",
              "        [0.26505673, 0.30189705, 0.29235023, 0.140696  ]],\n",
              "\n",
              "       [[0.26011866, 0.30325598, 0.2946085 , 0.14201683],\n",
              "        [0.25707033, 0.30424473, 0.29590333, 0.14278154],\n",
              "        [0.2617999 , 0.30128443, 0.2960353 , 0.14088042],\n",
              "        ...,\n",
              "        [0.26684064, 0.29940453, 0.29173052, 0.14202441],\n",
              "        [0.2661049 , 0.30062246, 0.2918345 , 0.14143816],\n",
              "        [0.26505673, 0.30189705, 0.29235023, 0.14069602]],\n",
              "\n",
              "       [[0.2588833 , 0.30413416, 0.29500148, 0.14198105],\n",
              "        [0.2634686 , 0.30092794, 0.2928198 , 0.14278367],\n",
              "        [0.2602056 , 0.30339673, 0.29468152, 0.14171618],\n",
              "        ...,\n",
              "        [0.26684064, 0.29940453, 0.29173052, 0.14202441],\n",
              "        [0.2661049 , 0.30062246, 0.2918345 , 0.14143816],\n",
              "        [0.26505673, 0.30189705, 0.29235023, 0.14069602]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.26499185, 0.30143282, 0.29344985, 0.14012547],\n",
              "        [0.26141384, 0.30397668, 0.29241797, 0.14219159],\n",
              "        [0.26079166, 0.30629632, 0.29287544, 0.14003663],\n",
              "        ...,\n",
              "        [0.26684064, 0.29940453, 0.29173052, 0.14202441],\n",
              "        [0.2661049 , 0.30062246, 0.2918345 , 0.14143816],\n",
              "        [0.26505673, 0.30189705, 0.29235023, 0.14069602]],\n",
              "\n",
              "       [[0.2677787 , 0.29599652, 0.29773805, 0.13848677],\n",
              "        [0.26553634, 0.29859847, 0.29752013, 0.1383451 ],\n",
              "        [0.26381212, 0.29777697, 0.2975811 , 0.14082982],\n",
              "        ...,\n",
              "        [0.26684064, 0.29940453, 0.29173052, 0.14202441],\n",
              "        [0.2661049 , 0.30062246, 0.2918345 , 0.14143816],\n",
              "        [0.26505673, 0.30189705, 0.29235023, 0.14069602]],\n",
              "\n",
              "       [[0.26147053, 0.30297798, 0.2980711 , 0.13748036],\n",
              "        [0.25838646, 0.30271167, 0.299229  , 0.13967288],\n",
              "        [0.25947708, 0.29847795, 0.30328366, 0.1387613 ],\n",
              "        ...,\n",
              "        [0.26684064, 0.29940453, 0.29173052, 0.14202441],\n",
              "        [0.2661049 , 0.30062246, 0.2918345 , 0.14143816],\n",
              "        [0.26505673, 0.30189705, 0.29235023, 0.14069602]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Uq3fMOgF5l"
      },
      "source": [
        "We can even evaluate these initial predictions and calculate loss (lower is better)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Vj2GDWBKG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec11819-8407-4ee9-fbab-11b275020e9b"
      },
      "source": [
        "# evaluate our initial model\n",
        "results = model.evaluate(X, y, batch_size=BATCH_SIZE, verbose=0)\n",
        "print(\"Loss: {:0.4f}\".format(results[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.8116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY38eyZngHot"
      },
      "source": [
        "We know we have imbalanced classes in our dataset (only a few named entities), a problem exacerbated by padding the sequences. So let's try and help the situation by setting an intial bias based on the prior distribution of labels in our training set. The following code block calculates those proportions on the padded training sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEfwpTXVBLOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b35310-05d6-4027-c9d2-0305e18c8a48"
      },
      "source": [
        "# figure out the label distribution in our fixed-length texts\n",
        "from collections import Counter\n",
        "\n",
        "all_labs = [l for lab in train_labs_padded for l in lab]\n",
        "label_count = Counter(all_labs)\n",
        "total_labs = len(all_labs)\n",
        "print(label_count)\n",
        "print(total_labs)\n",
        "\n",
        "# use this to define an initial model bias\n",
        "initial_bias=[(label_count[0]/total_labs), (label_count[1]/total_labs),\n",
        "              (label_count[2]/total_labs), (label_count[3]/total_labs)]\n",
        "print('Initial bias:')\n",
        "print(initial_bias)\n",
        "\n",
        "# pass the bias to the model and re-evaluate\n",
        "model = make_model(output_bias=initial_bias)\n",
        "results = model.evaluate(X, y, batch_size=BATCH_SIZE, verbose=0)\n",
        "print(\"Loss: {:0.4f}\".format(results[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({3: 292139, 2: 59095, 0: 1964, 1: 1177})\n",
            "354375\n",
            "Initial bias:\n",
            "[0.005542151675485009, 0.0033213403880070548, 0.1667583774250441, 0.8243781305114638]\n",
            "Loss: 0.1107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6z-8-e_gMUJ"
      },
      "source": [
        "Now we can use that initial bias and fit the model on the training set, validating on our dev set. The print statements tell us our epoch number, batch number and metrics on the training and validation sets. With up to 100 epochs this could take a while, so make sure your machine won't go to sleep, start the next code block running, and [take a break](https://xkcd.com/1838/) :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1GdpAQT9LOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269c00c3-6f07-49e2-f53e-d7a79f828a80"
      },
      "source": [
        "# prepare the dev sequences and labels as numpy arrays\n",
        "dev_X = np.array(dev_seqs_padded)\n",
        "dev_y = np.array(dev_labs_onehot)\n",
        "\n",
        "# re-initiate model with bias\n",
        "model = make_model(output_bias=initial_bias)\n",
        "\n",
        "# and fit...\n",
        "model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(dev_X, dev_y))\n",
        "\n",
        "# save the model\n",
        "model.save('BiLSTM/BiLSTM.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "106/106 [==============================] - 52s 413ms/step - loss: 0.1885 - tp: 322346.0000 - fp: 12896.0000 - tn: 2113342.0000 - fn: 386400.0000 - accuracy: 0.8592 - precision: 0.9615 - recall: 0.4548 - auc: 0.9817 - val_loss: 0.0605 - val_tp: 103025.0000 - val_fp: 1240.0000 - val_tn: 311555.0000 - val_fn: 1240.0000 - val_accuracy: 0.9941 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 41s 388ms/step - loss: 0.0459 - tp: 351055.0000 - fp: 3240.0000 - tn: 1059879.0000 - fn: 3318.0000 - accuracy: 0.9954 - precision: 0.9909 - recall: 0.9906 - auc: 0.9992 - val_loss: 0.0505 - val_tp: 103022.0000 - val_fp: 1242.0000 - val_tn: 311553.0000 - val_fn: 1243.0000 - val_accuracy: 0.9940 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9994\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 41s 391ms/step - loss: 0.0327 - tp: 351134.0000 - fp: 2914.0000 - tn: 1060205.0000 - fn: 3239.0000 - accuracy: 0.9957 - precision: 0.9918 - recall: 0.9909 - auc: 0.9997 - val_loss: 0.0451 - val_tp: 103024.0000 - val_fp: 1236.0000 - val_tn: 311559.0000 - val_fn: 1241.0000 - val_accuracy: 0.9941 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9994\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 41s 388ms/step - loss: 0.0205 - tp: 351276.0000 - fp: 1769.0000 - tn: 1061350.0000 - fn: 3097.0000 - accuracy: 0.9966 - precision: 0.9950 - recall: 0.9913 - auc: 0.9999 - val_loss: 0.0449 - val_tp: 103030.0000 - val_fp: 1182.0000 - val_tn: 311613.0000 - val_fn: 1235.0000 - val_accuracy: 0.9942 - val_precision: 0.9887 - val_recall: 0.9882 - val_auc: 0.9990\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 41s 391ms/step - loss: 0.0133 - tp: 352165.0000 - fp: 949.0000 - tn: 1062170.0000 - fn: 2208.0000 - accuracy: 0.9978 - precision: 0.9973 - recall: 0.9938 - auc: 0.9999 - val_loss: 0.0520 - val_tp: 103036.0000 - val_fp: 1188.0000 - val_tn: 311607.0000 - val_fn: 1229.0000 - val_accuracy: 0.9942 - val_precision: 0.9886 - val_recall: 0.9882 - val_auc: 0.9983\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 41s 389ms/step - loss: 0.0100 - tp: 352820.0000 - fp: 760.0000 - tn: 1062359.0000 - fn: 1553.0000 - accuracy: 0.9984 - precision: 0.9979 - recall: 0.9956 - auc: 0.9999 - val_loss: 0.0526 - val_tp: 103051.0000 - val_fp: 1177.0000 - val_tn: 311618.0000 - val_fn: 1214.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9884 - val_auc: 0.9982\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 41s 390ms/step - loss: 0.0075 - tp: 353304.0000 - fp: 586.0000 - tn: 1062533.0000 - fn: 1069.0000 - accuracy: 0.9988 - precision: 0.9983 - recall: 0.9970 - auc: 1.0000 - val_loss: 0.0605 - val_tp: 103057.0000 - val_fp: 1176.0000 - val_tn: 311619.0000 - val_fn: 1208.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9884 - val_auc: 0.9969\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 42s 392ms/step - loss: 0.0059 - tp: 353570.0000 - fp: 485.0000 - tn: 1062634.0000 - fn: 803.0000 - accuracy: 0.9991 - precision: 0.9986 - recall: 0.9977 - auc: 1.0000 - val_loss: 0.0600 - val_tp: 103059.0000 - val_fp: 1173.0000 - val_tn: 311622.0000 - val_fn: 1206.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9884 - val_auc: 0.9971\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 41s 389ms/step - loss: 0.0049 - tp: 353767.0000 - fp: 406.0000 - tn: 1062713.0000 - fn: 606.0000 - accuracy: 0.9993 - precision: 0.9989 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0651 - val_tp: 103062.0000 - val_fp: 1174.0000 - val_tn: 311621.0000 - val_fn: 1203.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9885 - val_auc: 0.9963\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 41s 389ms/step - loss: 0.0038 - tp: 353920.0000 - fp: 300.0000 - tn: 1062819.0000 - fn: 453.0000 - accuracy: 0.9995 - precision: 0.9992 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0668 - val_tp: 103060.0000 - val_fp: 1172.0000 - val_tn: 311623.0000 - val_fn: 1205.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9884 - val_auc: 0.9961\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 41s 390ms/step - loss: 0.0031 - tp: 353991.0000 - fp: 260.0000 - tn: 1062859.0000 - fn: 382.0000 - accuracy: 0.9995 - precision: 0.9993 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0673 - val_tp: 103024.0000 - val_fp: 1194.0000 - val_tn: 311601.0000 - val_fn: 1241.0000 - val_accuracy: 0.9942 - val_precision: 0.9885 - val_recall: 0.9881 - val_auc: 0.9962\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 41s 388ms/step - loss: 0.0027 - tp: 354048.0000 - fp: 253.0000 - tn: 1062866.0000 - fn: 325.0000 - accuracy: 0.9996 - precision: 0.9993 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0781 - val_tp: 103066.0000 - val_fp: 1180.0000 - val_tn: 311615.0000 - val_fn: 1199.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9885 - val_auc: 0.9949\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0023 - tp: 354112.0000 - fp: 192.0000 - tn: 1062927.0000 - fn: 261.0000 - accuracy: 0.9997 - precision: 0.9995 - recall: 0.9993 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 41s 387ms/step - loss: 0.0023 - tp: 354112.0000 - fp: 192.0000 - tn: 1062927.0000 - fn: 261.0000 - accuracy: 0.9997 - precision: 0.9995 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0768 - val_tp: 103074.0000 - val_fp: 1175.0000 - val_tn: 311620.0000 - val_fn: 1191.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9886 - val_auc: 0.9951\n",
            "Epoch 00013: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "260ZQ5ZvgRxu"
      },
      "source": [
        "We can now evaluate our model on the dev set in full. Let's check the distribution of predicted labels before going further, flattening the list of predictions so as to count the labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzEn-s_B8_Z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9281536-fa8f-4af6-8909-5c9a1069feeb"
      },
      "source": [
        "# use argmax to figure out the class with highest probability per token\n",
        "preds = np.argmax(model.predict(dev_seqs_padded), axis=-1)\n",
        "flat_preds = [p for pred in preds for p in pred]\n",
        "print(Counter(flat_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({3: 88887, 2: 15377, 0: 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbekBE7dgULG"
      },
      "source": [
        "But we encounter the same old problem as last time: that of not predicting 0s or 1s, our named entity but very rare B and I labels.\n",
        "\n",
        "Now, there are various techniques you can try to deal with class imbalance with neural networks. This might be the subject of your report for assignment 3.\n",
        "\n",
        "Here, we'll try one of those techniques which is to down-weight our non named entity labels 2 and 3 ('outside' and 'pad'). To do this we will re-train the model using weighted encoded labels, obtained by multiplying positions 0 and 1 (B and I) in our previous one-hot encoded labels by 1 and positions 2 and 3 by .1 (these are arbitrary weights; you could try tuning them)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzFn-ZoBtUUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecd732c-3cad-45a6-d419-0e87e2b9009c"
      },
      "source": [
        "# use deep copy to ensure we aren't updating original values\n",
        "import copy\n",
        "train_weights_onehot = copy.deepcopy(train_labs_onehot)\n",
        "\n",
        "# our first-pass class weights: normal for named entities (0 and 1), down-weighted for non named entities (2 and 3)\n",
        "class_wts = [1, 1, .1, .1]\n",
        "\n",
        "# apply our weights to the label lists\n",
        "for i, labs in enumerate(train_weights_onehot):\n",
        "    for j, lablist in enumerate(labs):\n",
        "        lablistaslist = lablist.tolist()\n",
        "        whichismax = lablistaslist.index(max(lablistaslist))\n",
        "        train_weights_onehot[i][j][whichismax] = class_wts[whichismax]\n",
        "\n",
        "# what's this like, before and after?\n",
        "print('Initial one-hot label encoding:')\n",
        "print(train_labs_onehot[1][:11])\n",
        "\n",
        "print('Weighted label encoding:')\n",
        "print(train_weights_onehot[1][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial one-hot label encoding:\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]]\n",
            "Weighted label encoding:\n",
            "[[0.  0.  0.1 0. ]\n",
            " [0.  0.  0.1 0. ]\n",
            " [0.  0.  0.1 0. ]\n",
            " [0.  0.  0.1 0. ]\n",
            " [1.  0.  0.  0. ]\n",
            " [0.  0.  0.1 0. ]\n",
            " [0.  0.  0.1 0. ]\n",
            " [0.  0.  0.1 0. ]\n",
            " [0.  0.  0.1 0. ]\n",
            " [0.  0.  0.1 0. ]\n",
            " [0.  0.  0.1 0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rrLmsysgun2"
      },
      "source": [
        "Ok let's use those weighted encodings to fit a new model, again this will take some time, so take another break away from your screen (but don't let your machine go to sleep otherwise you'll lose your Colab session) unless you enjoy watching the epochs progress... (it can be a little mesmerising, I sometimes think)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWOrYnW_yJhK",
        "outputId": "cc75263d-fa14-4e2d-8ce3-53be30d28053"
      },
      "source": [
        "# now try the weighted one-hot encoding\n",
        "y = np.array(train_weights_onehot)\n",
        "print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n",
        "print(np.shape(y))\n",
        "\n",
        "model2 = make_model(output_bias=initial_bias)\n",
        "model2.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(dev_X, dev_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):\n",
            "(3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 53s 413ms/step - loss: 0.0377 - tp: 311051.0000 - fp: 9201.0000 - tn: 2117037.0000 - fn: 397695.0000 - accuracy: 0.7468 - precision: 0.9713 - recall: 0.4389 - auc: 0.9819 - val_loss: 0.0771 - val_tp: 102844.0000 - val_fp: 1216.0000 - val_tn: 311579.0000 - val_fn: 1421.0000 - val_accuracy: 0.9937 - val_precision: 0.9883 - val_recall: 0.9864 - val_auc: 0.9993\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 0.0194 - tp: 345436.0000 - fp: 1790.0000 - tn: 1061329.0000 - fn: 8937.0000 - accuracy: 0.7488 - precision: 0.9948 - recall: 0.9748 - auc: 0.9996 - val_loss: 0.0563 - val_tp: 102042.0000 - val_fp: 886.0000 - val_tn: 311909.0000 - val_fn: 2223.0000 - val_accuracy: 0.9925 - val_precision: 0.9914 - val_recall: 0.9787 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0119 - tp: 348103.0000 - fp: 1652.0000 - tn: 1061467.0000 - fn: 6270.0000 - accuracy: 0.7496 - precision: 0.9953 - recall: 0.9823 - auc: 0.9998 - val_loss: 0.0369 - val_tp: 102691.0000 - val_fp: 1030.0000 - val_tn: 311765.0000 - val_fn: 1574.0000 - val_accuracy: 0.9938 - val_precision: 0.9901 - val_recall: 0.9849 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0063 - tp: 351707.0000 - fp: 1492.0000 - tn: 1061627.0000 - fn: 2666.0000 - accuracy: 0.7506 - precision: 0.9958 - recall: 0.9925 - auc: 0.9999 - val_loss: 0.0381 - val_tp: 102959.0000 - val_fp: 1112.0000 - val_tn: 311683.0000 - val_fn: 1306.0000 - val_accuracy: 0.9942 - val_precision: 0.9893 - val_recall: 0.9875 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0034 - tp: 352920.0000 - fp: 1034.0000 - tn: 1062085.0000 - fn: 1453.0000 - accuracy: 0.7513 - precision: 0.9971 - recall: 0.9959 - auc: 0.9999 - val_loss: 0.0420 - val_tp: 102999.0000 - val_fp: 1100.0000 - val_tn: 311695.0000 - val_fn: 1266.0000 - val_accuracy: 0.9943 - val_precision: 0.9894 - val_recall: 0.9879 - val_auc: 0.9987\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0022 - tp: 353394.0000 - fp: 762.0000 - tn: 1062357.0000 - fn: 979.0000 - accuracy: 0.7516 - precision: 0.9978 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0418 - val_tp: 103028.0000 - val_fp: 1125.0000 - val_tn: 311670.0000 - val_fn: 1237.0000 - val_accuracy: 0.9943 - val_precision: 0.9892 - val_recall: 0.9881 - val_auc: 0.9986\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 42s 398ms/step - loss: 0.0015 - tp: 353646.0000 - fp: 610.0000 - tn: 1062509.0000 - fn: 727.0000 - accuracy: 0.7517 - precision: 0.9983 - recall: 0.9979 - auc: 0.9999 - val_loss: 0.0464 - val_tp: 103041.0000 - val_fp: 1140.0000 - val_tn: 311655.0000 - val_fn: 1224.0000 - val_accuracy: 0.9943 - val_precision: 0.9891 - val_recall: 0.9883 - val_auc: 0.9983\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0011 - tp: 353801.0000 - fp: 486.0000 - tn: 1062633.0000 - fn: 572.0000 - accuracy: 0.7518 - precision: 0.9986 - recall: 0.9984 - auc: 0.9999 - val_loss: 0.0521 - val_tp: 103063.0000 - val_fp: 1154.0000 - val_tn: 311641.0000 - val_fn: 1202.0000 - val_accuracy: 0.9944 - val_precision: 0.9889 - val_recall: 0.9885 - val_auc: 0.9978\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 8.5261e-04 - tp: 353963.0000 - fp: 345.0000 - tn: 1062774.0000 - fn: 410.0000 - accuracy: 0.7519 - precision: 0.9990 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0473 - val_tp: 103006.0000 - val_fp: 1180.0000 - val_tn: 311615.0000 - val_fn: 1259.0000 - val_accuracy: 0.9942 - val_precision: 0.9887 - val_recall: 0.9879 - val_auc: 0.9982\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 7.0509e-04 - tp: 354026.0000 - fp: 310.0000 - tn: 1062809.0000 - fn: 347.0000 - accuracy: 0.7520 - precision: 0.9991 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0508 - val_tp: 103023.0000 - val_fp: 1160.0000 - val_tn: 311635.0000 - val_fn: 1242.0000 - val_accuracy: 0.9942 - val_precision: 0.9889 - val_recall: 0.9881 - val_auc: 0.9979\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 42s 392ms/step - loss: 5.7283e-04 - tp: 354086.0000 - fp: 252.0000 - tn: 1062867.0000 - fn: 287.0000 - accuracy: 0.7520 - precision: 0.9993 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0531 - val_tp: 103017.0000 - val_fp: 1177.0000 - val_tn: 311618.0000 - val_fn: 1248.0000 - val_accuracy: 0.9942 - val_precision: 0.9887 - val_recall: 0.9880 - val_auc: 0.9977\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 5.2609e-04 - tp: 354117.0000 - fp: 226.0000 - tn: 1062893.0000 - fn: 256.0000 - accuracy: 0.7520 - precision: 0.9994 - recall: 0.9993 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 5.2609e-04 - tp: 354117.0000 - fp: 226.0000 - tn: 1062893.0000 - fn: 256.0000 - accuracy: 0.7520 - precision: 0.9994 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0577 - val_tp: 103046.0000 - val_fp: 1164.0000 - val_tn: 311631.0000 - val_fn: 1219.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9883 - val_auc: 0.9973\n",
            "Epoch 00012: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9eb9a1e50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdoVhiNRg0Sk"
      },
      "source": [
        "And let's re-evaluate this second model on the dev set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sf97sWMzdka",
        "outputId": "2ce94b11-00e6-4e24-f06f-fd7a9c743934"
      },
      "source": [
        "preds = np.argmax(model2.predict(dev_seqs_padded), axis=-1)\n",
        "flat_preds = [p for pred in preds for p in pred]\n",
        "print(Counter(flat_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({3: 88894, 2: 14866, 0: 505})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-6BohjCij9Q"
      },
      "source": [
        "That's better: our model has now predicted some named entities.\n",
        "\n",
        "Remember that these predictions are still padded to a fixed number of tokens. We need to convert these sequences back to their original lengths so that we can properly evaluate the predictions against the gold labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD3-NK606v_I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "89624783-b875-486a-8c91-17f201109786"
      },
      "source": [
        "# start a new column for the model predictions\n",
        "dev_seqs['prediction'] = ''\n",
        "\n",
        "# for each text: get original sequence length and trim predictions accordingly\n",
        "# (_trim_ because we know that our seq length is longer than the longest seq in dev)\n",
        "for i in dev_seqs.index:\n",
        "    this_seq_length = len(dev_seqs['token'][i])\n",
        "    dev_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "\n",
        "dev_seqs.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[Stabilized, approach, or, not, ?, That, , s,...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[14801.0, 10361.0, 414.0, 556.0, 131.0, 1740.0...</td>\n",
              "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[You, should, ', ve, stayed, on, Redondo, Beac...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 1.0, ...</td>\n",
              "      <td>[151.0, 1018.0, 573.0, 12927.0, 9346.0, 137.0,...</td>\n",
              "      <td>[2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[All, I, ', ve, been, doing, is, BINGE, watchi...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[405.0, 7.0, 573.0, 12927.0, 90.0, 848.0, 52.0...</td>\n",
              "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[wow, emma, and, kaite, is, so, very, cute, an...</td>\n",
              "      <td>[2.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[4777.0, 14801.0, 113.0, 14801.0, 52.0, 79.0, ...</td>\n",
              "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[THIS, IS, SO, GOOD]</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0]</td>\n",
              "      <td>[2239.0, 1567.0, 1089.0, 9176.0]</td>\n",
              "      <td>[2, 2, 2, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                         prediction\n",
              "0             0  ...               [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
              "1             1  ...  [2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, ...\n",
              "2             2  ...            [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
              "3             3  ...  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, ...\n",
              "4             4  ...                                       [2, 2, 2, 2]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rh8lgmsmKHV"
      },
      "source": [
        "Then we need to 'explode' the sequences back into long tabular format ready for our evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9qny7OsnkBl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6c4d1f59-5ac0-4830-b0c7-82dfcf990518"
      },
      "source": [
        "# use sequence number as the index and apply pandas explode to all other columns\n",
        "dev_long = dev_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "dev_long.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Stabilized</td>\n",
              "      <td>2</td>\n",
              "      <td>14801</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>approach</td>\n",
              "      <td>2</td>\n",
              "      <td>10361</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>or</td>\n",
              "      <td>2</td>\n",
              "      <td>414</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>not</td>\n",
              "      <td>2</td>\n",
              "      <td>556</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>?</td>\n",
              "      <td>2</td>\n",
              "      <td>131</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num       token bio_only token_indices prediction\n",
              "0             0  Stabilized        2         14801          2\n",
              "1             0    approach        2         10361          2\n",
              "2             0          or        2           414          2\n",
              "3             0         not        2           556          2\n",
              "4             0           ?        2           131          2"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1vzIvgRzQ39"
      },
      "source": [
        "Finally we need to convert the named entity labels from integers back to BIO characters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXbybZ6wzUB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbc12e1-baa6-4326-8867-556ce42b2d02"
      },
      "source": [
        "# re-using the BIO integer-to-character function from last time\n",
        "def reverse_bio(ind):\n",
        "    bio = 'O'  # for any pad=3 predictions\n",
        "    if ind==0:\n",
        "        bio = 'B'\n",
        "    elif ind==1:\n",
        "        bio = 'I'\n",
        "    elif ind==2:\n",
        "        bio = 'O'\n",
        "    return bio\n",
        "\n",
        "bio_labs = [reverse_bio(b) for b in dev_long['bio_only']]\n",
        "dev_long['bio_only'] = bio_labs\n",
        "pred_labs = [reverse_bio(b) for b in dev_long['prediction']]\n",
        "dev_long['prediction'] = pred_labs\n",
        "\n",
        "dev_long.head()\n",
        "dev_long.prediction.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O    14877\n",
              "B      505\n",
              "Name: prediction, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyJX2mzoztwW"
      },
      "source": [
        "And we can re-use the evaluation function from practical 2 to calculate precision, recall and F1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsVoZB2JlXtB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d106435-b22f-4e84-c490-318de233bbea"
      },
      "source": [
        "def wnut_evaluate(txt):\n",
        "    '''entity evaluation: we evaluate by whole named entities'''\n",
        "    npred = 0; ngold = 0; tp = 0\n",
        "    nrows = len(txt)\n",
        "    for i in txt.index:\n",
        "        if txt['prediction'][i]=='B' and txt['bio_only'][i]=='B':\n",
        "            npred += 1\n",
        "            ngold += 1\n",
        "            for predfindbo in range((i+1),nrows):\n",
        "                if txt['prediction'][predfindbo]=='O' or txt['prediction'][predfindbo]=='B':\n",
        "                    break  # find index of first O (end of entity) or B (new entity)\n",
        "            for goldfindbo in range((i+1),nrows):\n",
        "                if txt['bio_only'][goldfindbo]=='O' or txt['bio_only'][goldfindbo]=='B':\n",
        "                    break  # find index of first O (end of entity) or B (new entity)\n",
        "            if predfindbo==goldfindbo:  # only count a true positive if the whole entity phrase matches\n",
        "                tp += 1\n",
        "        elif txt['prediction'][i]=='B':\n",
        "            npred += 1\n",
        "        elif txt['bio_only'][i]=='B':\n",
        "            ngold += 1\n",
        "  \n",
        "    fp = npred - tp  # n false predictions\n",
        "    fn = ngold - tp  # n missing gold entities\n",
        "    prec = tp / (tp+fp+1e-7)\n",
        "    rec = tp / (tp+fn+1e-7)\n",
        "    f1 = (2*(prec*rec)) / (prec+rec+1e-7)\n",
        "    print('Sum of TP and FP = %i' % (tp+fp))\n",
        "    print('Sum of TP and FN = %i' % (tp+fn))\n",
        "    print('True positives = %i, False positives = %i, False negatives = %i' % (tp, fp, fn))\n",
        "    print('Precision = %.3f, Recall = %.3f, F1 = %.3f' % (prec, rec, f1))\n",
        "\n",
        "wnut_evaluate(dev_long)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of TP and FP = 505\n",
            "Sum of TP and FN = 826\n",
            "True positives = 59, False positives = 446, False negatives = 767\n",
            "Precision = 0.117, Recall = 0.071, F1 = 0.089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmSYosm4jUmB"
      },
      "source": [
        "This first attempt at a neural network classifier is not very good! We still aren't able to beat that baseline which relied on proper noun PoS-tags only, even with _deep learning_..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB82FnNUAC2I"
      },
      "source": [
        "## Over to you: Assignment 3\n",
        "\n",
        "Ok now it's over to you for your final assignment. \n",
        "\n",
        "**For Part II students**, there is no need to further develop the neural network classifier demonstrated in this notebook. Instead, the purpose of the assignment is to do a **close error analysis** of both your classifier from assignment 2 and this neural network classifier to some extent -- you can choose to focus more on one or the other, but both should be covered. Error analyses include but are not limited to: confusion matrices, examples of texts containing errors, discussion of why those named entities are not captured by the classifier in question. Discuss the errors with reference to the linguistic properties of the words involved. You might also reflect on the annotation process and the resulting training and dev files, based on your experience in assignment 1 and the group-level agreement / disagreement over named entities. Note that you are only required to undertake this analysis on the dev set, as we think there will be enough that's interesting to talk about in these texts alone. If you are short of things to talk about, you can extend to look at the test set too ([link to the annotated test set](https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_annotated_clean_tagged.txt)), but there's _no requirement_ to do so.\n",
        "\n",
        "Marks will _not_ be awarded for the performance of your system(s) from assignment 2, but for understanding the performance of your system(s) and the one demonstrated in this notebook. This is because the purpose of the NLP course for Part II is to introduce you to linguistic ideas through computational approaches and this is what you will be evaluated upon. We require that you write a maximum of **three** A4 pages, excluding references, for this assignment (12 pt font, min margins 2cm, min 1.0 line spacing) and submit the document as a PDF. Remember not to put anything personally identifying on your report, just your blind candidate number (BCN) available from Student Admin.\n",
        "\n",
        "Your work will be graded out of 100, with a maximum of 25 for each of 4 criteria: quantitative error analysis, qualitative error analysis, explaining errors in terms of the models, and discussing the improvements you would make to resolve some of the errors. There's more information about unit of assessment marking generally [here](https://www.cst.cam.ac.uk/teaching/exams/marking-and-classing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NWvI8-wumpW"
      },
      "source": [
        "**For ACS students**, we expect that you will go beyond the neural network classifier defined in this notebook. You might try one or two of the following in your project (but note that other ideas of your own are welcome):\n",
        "- different label weights and network configurations and/or hyperparameter tuning\n",
        "- using oversampling to address class imbalance\n",
        "- adding texts from other NER datasets (e.g. [OntoNotes](https://github.com/yuchenlin/OntoNotes-5.0-NER-BIO))\n",
        "- using error analyses to compare the neural network with the feature-based classifier from practical 2, or your own from assignment 2\n",
        "- merging B and I labels so that the problem becomes 2-class classification, then some post-processing to try and split named entity labels into B and I\n",
        "- or as a more advanced approach, look into conditional random fields\n",
        "- full entity type prediction and evaluation (see the W-NUT 2017 shared task overview paper: this is the 'surface F1' metric described there)\n",
        "- focusing on different linguistic representations, for example by experimenting with different settings for the embedding layer, making use of PoS-tags and/or sub-word (character) information; or through the use of pre-trained word representations such as those available from [word2vec](https://code.google.com/archive/p/word2vec/) [GloVe](https://nlp.stanford.edu/projects/glove/), [fastText](https://fasttext.cc/docs/en/english-vectors.html); or even contextual word representations, for example via [Flair NLP](https://github.com/flairNLP/flair) (without using their pre-trained tagger!)\n",
        "- fine-tuning transformer models for NER, as in the [Hugging Face documentation](https://huggingface.co/transformers/custom_datasets.html#tok-ner) using WNUT-17 as an example (!) but note that you can't just lift this code because (a) they split a validation set from the training set, contrary to our use of the proper dev set; (b) they tokenize the texts in a certain way and you'd still need to figure out how to undo that tokenization ready for evaluation; (c) it'll be obvious that you only used the given code and didn't contribute any of your own\n",
        "- look into [autoregressive models](https://www.tensorflow.org/tutorials/structured_data/time_series#advanced_autoregressive_model) which use previous predictions as input to the next prediction\n",
        "- you might look at the entries to the shared task as described in the ACL Anthology for [W-NUT 2017](https://aclanthology.org/events/ws-2017/#w17-44) (not all the papers at the workshop were about the shared task; look for relevant titles, or look in the overview results paper which will tell you who entered the competition and wrote a paper about it) -- this will give you ideas about successful approaches to the task, though note that a lot has changed in NLP since 2017!\n",
        "- you might also look at relevant work in the [RepL4NLP](https://aclanthology.org/sigs/sigrep/) or [BlackBoxNLP](https://aclanthology.org/events/emnlp-2020/#2020-blackboxnlp-1) workshops, the [*ACL and EMNLP](https://aclanthology.org/) conferences on computational linguistics, or even at major ML events such as NeurIPS, ICML, ICLR\n",
        "- note also this summary of [state-of-the-art research for NER](https://paperswithcode.com/task/named-entity-recognition-ner)\n",
        "\n",
        "You only need to try working on one or two ways to develop the classifier: any more than this and you'd find it difficult to properly fit everything into your report.\n",
        "\n",
        "The purpose of the assignment is to prepare you for research work in NLP. Therefore, we ask that you submit a report in ACL short paper format. This will be a **maximum of six** A4 pages plus references and appendices (for supplementary tables or code). As such, please use the ACL LaTeX template which is available from Overleaf to download [here](https://www.overleaf.com/read/jtdrtqyszzxd), or you can make a copy if you have an Overleaf account (in both cases, use the Menu at top-left). The report should be structured like an experimental write up as follows: short introduction & background section, overview of models used, detail of methods / feature extraction / network properties, data description, evaluation method, results, discussion and conclusion. For examples of such papers please refer to the [ACL Anthology](https://aclanthology.org/), but note we do not expect the work to be a research contribution.\n",
        "\n",
        "The report will be marked as described on [this page](https://www.cst.cam.ac.uk/teaching/exams/acs_assessment) (same marking criteria for both MPhil and Part III students). It might also be helpful to know how ACL submissions are reviewed: recent guidance for paper reviewers is given [here](https://acl2020.org/reviewers/#the-review-form).\n",
        "\n",
        "A reminder of where to find the W-NUT 2017 [training set](https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt) and [development set](https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt). When you have finished working on the dev set, you can apply your model(s) to the [test set](https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_annotated_clean_tagged.txt) and report the performance of your classifier(s), but note that it's good practice not to do so until you've finished development. Again, note that you _won't_ be assessed on how well your model(s) do on the dev or test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOLPkE0IOWTT"
      },
      "source": [
        "**Assignment submissions**\n",
        "\n",
        "The deadline for this assignment is 3pm on Friday 3rd December, the final day of full term (we are aware of some individual extensions re SSDs). Please submit your files in the appropriate place for assignment 3 on [Moodle](https://www.vle.cam.ac.uk/course/view.php?id=206751) (**Part II students**: remember to use your candidate number as your filename, and complete a cover sheet). We expect to receive your report as a PDF along with any supplementary information you wish to submit (e.g. code, output files). Any queries please get in touch (apc38)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyx4FzrBSPWk"
      },
      "source": [
        "### 0. Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYUmJ-0kSTU9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l3r1hj9tsv-"
      },
      "source": [
        "### 1. Different label weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIqnXug9mi7_"
      },
      "source": [
        "def down_weight(labs_onehot, weight=1):\n",
        "    weights_onehot = copy.deepcopy(labs_onehot)\n",
        "\n",
        "    # our first-pass class weights: normal for named entities (0 and 1), down-weighted for non named entities (2 and 3)\n",
        "    class_wts = [1, 1, weight, weight]\n",
        "\n",
        "    # apply our weights to the label lists\n",
        "    for i, labs in enumerate(weights_onehot):\n",
        "        for j, lablist in enumerate(labs):\n",
        "            lablistaslist = lablist.tolist()\n",
        "            whichismax = lablistaslist.index(max(lablistaslist))\n",
        "            weights_onehot[i][j][whichismax] = class_wts[whichismax]\n",
        "    \n",
        "    return weights_onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY3THnFKWG-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d23e4cc-89b8-4ce6-c0ad-4b3db61b6d68"
      },
      "source": [
        "# now try the weighted one-hot encoding\n",
        "downweight_models = {}\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9]:\n",
        "    train_weights_onehot = down_weight(train_labs_onehot, weight)\n",
        "    y = np.array(train_weights_onehot)\n",
        "    print('Down-weighting:', weight)\n",
        "    print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):', np.shape(y))\n",
        "\n",
        "    downweight_model = make_model(output_bias=initial_bias)\n",
        "    downweight_model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(dev_X, dev_y))\n",
        "\n",
        "    downweight_models[weight] = downweight_model\n",
        "    downweight_model.save(f'BiLSTM/BiLSTM_downweight_{weight}.h5')\n",
        "    print(f'Model saved at BiLSTM/BiLSTM_downweight_{weight}.h5')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Down-weighting: 0.1\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 52s 410ms/step - loss: 0.0377 - tp: 414143.0000 - fp: 10381.0000 - tn: 1365533.0000 - fn: 44495.0000 - accuracy: 0.8005 - precision: 0.9755 - recall: 0.9030 - auc: 0.9951 - val_loss: 0.0770 - val_tp: 102845.0000 - val_fp: 1217.0000 - val_tn: 311578.0000 - val_fn: 1420.0000 - val_accuracy: 0.9937 - val_precision: 0.9883 - val_recall: 0.9864 - val_auc: 0.9993\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 42s 392ms/step - loss: 0.0194 - tp: 345395.0000 - fp: 1799.0000 - tn: 1061320.0000 - fn: 8978.0000 - accuracy: 0.7488 - precision: 0.9948 - recall: 0.9747 - auc: 0.9996 - val_loss: 0.0562 - val_tp: 102048.0000 - val_fp: 885.0000 - val_tn: 311910.0000 - val_fn: 2217.0000 - val_accuracy: 0.9926 - val_precision: 0.9914 - val_recall: 0.9787 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 0.0119 - tp: 348083.0000 - fp: 1645.0000 - tn: 1061474.0000 - fn: 6290.0000 - accuracy: 0.7496 - precision: 0.9953 - recall: 0.9823 - auc: 0.9998 - val_loss: 0.0370 - val_tp: 102689.0000 - val_fp: 1030.0000 - val_tn: 311765.0000 - val_fn: 1576.0000 - val_accuracy: 0.9938 - val_precision: 0.9901 - val_recall: 0.9849 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 41s 392ms/step - loss: 0.0063 - tp: 351694.0000 - fp: 1466.0000 - tn: 1061653.0000 - fn: 2679.0000 - accuracy: 0.7506 - precision: 0.9958 - recall: 0.9924 - auc: 0.9999 - val_loss: 0.0381 - val_tp: 102972.0000 - val_fp: 1116.0000 - val_tn: 311679.0000 - val_fn: 1293.0000 - val_accuracy: 0.9942 - val_precision: 0.9893 - val_recall: 0.9876 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0034 - tp: 352915.0000 - fp: 1056.0000 - tn: 1062063.0000 - fn: 1458.0000 - accuracy: 0.7512 - precision: 0.9970 - recall: 0.9959 - auc: 0.9999 - val_loss: 0.0419 - val_tp: 102998.0000 - val_fp: 1091.0000 - val_tn: 311704.0000 - val_fn: 1267.0000 - val_accuracy: 0.9943 - val_precision: 0.9895 - val_recall: 0.9878 - val_auc: 0.9987\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0022 - tp: 353375.0000 - fp: 784.0000 - tn: 1062335.0000 - fn: 998.0000 - accuracy: 0.7516 - precision: 0.9978 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0418 - val_tp: 103033.0000 - val_fp: 1117.0000 - val_tn: 311678.0000 - val_fn: 1232.0000 - val_accuracy: 0.9944 - val_precision: 0.9893 - val_recall: 0.9882 - val_auc: 0.9986\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 42s 399ms/step - loss: 0.0015 - tp: 353654.0000 - fp: 602.0000 - tn: 1062517.0000 - fn: 719.0000 - accuracy: 0.7517 - precision: 0.9983 - recall: 0.9980 - auc: 0.9999 - val_loss: 0.0465 - val_tp: 103046.0000 - val_fp: 1140.0000 - val_tn: 311655.0000 - val_fn: 1219.0000 - val_accuracy: 0.9943 - val_precision: 0.9891 - val_recall: 0.9883 - val_auc: 0.9983\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 0.0011 - tp: 353823.0000 - fp: 469.0000 - tn: 1062650.0000 - fn: 550.0000 - accuracy: 0.7518 - precision: 0.9987 - recall: 0.9984 - auc: 0.9999 - val_loss: 0.0516 - val_tp: 103075.0000 - val_fp: 1153.0000 - val_tn: 311642.0000 - val_fn: 1190.0000 - val_accuracy: 0.9944 - val_precision: 0.9889 - val_recall: 0.9886 - val_auc: 0.9979\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 8.6805e-04 - tp: 353956.0000 - fp: 355.0000 - tn: 1062764.0000 - fn: 417.0000 - accuracy: 0.7519 - precision: 0.9990 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0464 - val_tp: 103019.0000 - val_fp: 1169.0000 - val_tn: 311626.0000 - val_fn: 1246.0000 - val_accuracy: 0.9942 - val_precision: 0.9888 - val_recall: 0.9880 - val_auc: 0.9983\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 6.9046e-04 - tp: 354032.0000 - fp: 299.0000 - tn: 1062820.0000 - fn: 341.0000 - accuracy: 0.7520 - precision: 0.9992 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0502 - val_tp: 103049.0000 - val_fp: 1153.0000 - val_tn: 311642.0000 - val_fn: 1216.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9883 - val_auc: 0.9979\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 5.8628e-04 - tp: 354075.0000 - fp: 251.0000 - tn: 1062868.0000 - fn: 298.0000 - accuracy: 0.7520 - precision: 0.9993 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0517 - val_tp: 103007.0000 - val_fp: 1177.0000 - val_tn: 311618.0000 - val_fn: 1258.0000 - val_accuracy: 0.9942 - val_precision: 0.9887 - val_recall: 0.9879 - val_auc: 0.9978\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 5.4203e-04 - tp: 354098.0000 - fp: 238.0000 - tn: 1062881.0000 - fn: 275.0000 - accuracy: 0.7520 - precision: 0.9993 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0563 - val_tp: 103049.0000 - val_fp: 1169.0000 - val_tn: 311626.0000 - val_fn: 1216.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9883 - val_auc: 0.9975\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 4.2931e-04 - tp: 354185.0000 - fp: 167.0000 - tn: 1062952.0000 - fn: 188.0000 - accuracy: 0.7521 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 4.2931e-04 - tp: 354185.0000 - fp: 167.0000 - tn: 1062952.0000 - fn: 188.0000 - accuracy: 0.7521 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0567 - val_tp: 103029.0000 - val_fp: 1190.0000 - val_tn: 311605.0000 - val_fn: 1236.0000 - val_accuracy: 0.9942 - val_precision: 0.9886 - val_recall: 0.9881 - val_auc: 0.9973\n",
            "Epoch 00013: early stopping\n",
            "Model saved at BiLSTM/BiLSTM_downweight_0.1.h5\n",
            "\n",
            "Down-weighting: 0.2\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 52s 414ms/step - loss: 0.0571 - tp: 421552.0000 - fp: 12278.0000 - tn: 1363636.0000 - fn: 37086.0000 - accuracy: 0.7995 - precision: 0.9717 - recall: 0.9191 - auc: 0.9960 - val_loss: 0.0621 - val_tp: 102987.0000 - val_fp: 1227.0000 - val_tn: 311568.0000 - val_fn: 1278.0000 - val_accuracy: 0.9940 - val_precision: 0.9882 - val_recall: 0.9877 - val_auc: 0.9992\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 42s 399ms/step - loss: 0.0250 - tp: 349778.0000 - fp: 2598.0000 - tn: 1060521.0000 - fn: 4595.0000 - accuracy: 0.7482 - precision: 0.9926 - recall: 0.9870 - auc: 0.9996 - val_loss: 0.0476 - val_tp: 102882.0000 - val_fp: 1180.0000 - val_tn: 311615.0000 - val_fn: 1383.0000 - val_accuracy: 0.9939 - val_precision: 0.9887 - val_recall: 0.9867 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 42s 399ms/step - loss: 0.0157 - tp: 349716.0000 - fp: 1338.0000 - tn: 1061781.0000 - fn: 4657.0000 - accuracy: 0.7494 - precision: 0.9962 - recall: 0.9869 - auc: 0.9999 - val_loss: 0.0372 - val_tp: 102906.0000 - val_fp: 1116.0000 - val_tn: 311679.0000 - val_fn: 1359.0000 - val_accuracy: 0.9941 - val_precision: 0.9893 - val_recall: 0.9870 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0087 - tp: 351775.0000 - fp: 1098.0000 - tn: 1062021.0000 - fn: 2598.0000 - accuracy: 0.7505 - precision: 0.9969 - recall: 0.9927 - auc: 0.9999 - val_loss: 0.0390 - val_tp: 103019.0000 - val_fp: 1122.0000 - val_tn: 311673.0000 - val_fn: 1246.0000 - val_accuracy: 0.9943 - val_precision: 0.9892 - val_recall: 0.9880 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 0.0051 - tp: 353005.0000 - fp: 807.0000 - tn: 1062312.0000 - fn: 1368.0000 - accuracy: 0.7513 - precision: 0.9977 - recall: 0.9961 - auc: 1.0000 - val_loss: 0.0446 - val_tp: 103024.0000 - val_fp: 1110.0000 - val_tn: 311685.0000 - val_fn: 1241.0000 - val_accuracy: 0.9944 - val_precision: 0.9893 - val_recall: 0.9881 - val_auc: 0.9986\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0035 - tp: 353447.0000 - fp: 641.0000 - tn: 1062478.0000 - fn: 926.0000 - accuracy: 0.7515 - precision: 0.9982 - recall: 0.9974 - auc: 1.0000 - val_loss: 0.0467 - val_tp: 103065.0000 - val_fp: 1136.0000 - val_tn: 311659.0000 - val_fn: 1200.0000 - val_accuracy: 0.9944 - val_precision: 0.9891 - val_recall: 0.9885 - val_auc: 0.9983\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0025 - tp: 353683.0000 - fp: 514.0000 - tn: 1062605.0000 - fn: 690.0000 - accuracy: 0.7517 - precision: 0.9985 - recall: 0.9981 - auc: 1.0000 - val_loss: 0.0500 - val_tp: 103070.0000 - val_fp: 1142.0000 - val_tn: 311653.0000 - val_fn: 1195.0000 - val_accuracy: 0.9944 - val_precision: 0.9890 - val_recall: 0.9885 - val_auc: 0.9981\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 41s 391ms/step - loss: 0.0019 - tp: 353867.0000 - fp: 393.0000 - tn: 1062726.0000 - fn: 506.0000 - accuracy: 0.7519 - precision: 0.9989 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0538 - val_tp: 103074.0000 - val_fp: 1145.0000 - val_tn: 311650.0000 - val_fn: 1191.0000 - val_accuracy: 0.9944 - val_precision: 0.9890 - val_recall: 0.9886 - val_auc: 0.9977\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0014 - tp: 354015.0000 - fp: 275.0000 - tn: 1062844.0000 - fn: 358.0000 - accuracy: 0.7520 - precision: 0.9992 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0520 - val_tp: 103056.0000 - val_fp: 1160.0000 - val_tn: 311635.0000 - val_fn: 1209.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9884 - val_auc: 0.9979\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0011 - tp: 354093.0000 - fp: 243.0000 - tn: 1062876.0000 - fn: 280.0000 - accuracy: 0.7520 - precision: 0.9993 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0559 - val_tp: 103072.0000 - val_fp: 1145.0000 - val_tn: 311650.0000 - val_fn: 1193.0000 - val_accuracy: 0.9944 - val_precision: 0.9890 - val_recall: 0.9886 - val_auc: 0.9975\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 8.8772e-04 - tp: 354128.0000 - fp: 188.0000 - tn: 1062931.0000 - fn: 245.0000 - accuracy: 0.7521 - precision: 0.9995 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0586 - val_tp: 103066.0000 - val_fp: 1154.0000 - val_tn: 311641.0000 - val_fn: 1199.0000 - val_accuracy: 0.9944 - val_precision: 0.9889 - val_recall: 0.9885 - val_auc: 0.9972\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 8.0754e-04 - tp: 354175.0000 - fp: 169.0000 - tn: 1062950.0000 - fn: 198.0000 - accuracy: 0.7521 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 41s 392ms/step - loss: 8.0754e-04 - tp: 354175.0000 - fp: 169.0000 - tn: 1062950.0000 - fn: 198.0000 - accuracy: 0.7521 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0604 - val_tp: 103061.0000 - val_fp: 1157.0000 - val_tn: 311638.0000 - val_fn: 1204.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9885 - val_auc: 0.9972\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM/BiLSTM_downweight_0.2.h5\n",
            "\n",
            "Down-weighting: 0.3\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 52s 415ms/step - loss: 0.0749 - tp: 423534.0000 - fp: 12916.0000 - tn: 1362998.0000 - fn: 35104.0000 - accuracy: 0.7991 - precision: 0.9704 - recall: 0.9235 - auc: 0.9960 - val_loss: 0.0581 - val_tp: 103010.0000 - val_fp: 1235.0000 - val_tn: 311560.0000 - val_fn: 1255.0000 - val_accuracy: 0.9940 - val_precision: 0.9882 - val_recall: 0.9880 - val_auc: 0.9992\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 42s 392ms/step - loss: 0.0293 - tp: 350661.0000 - fp: 2943.0000 - tn: 1060176.0000 - fn: 3712.0000 - accuracy: 0.7479 - precision: 0.9917 - recall: 0.9895 - auc: 0.9996 - val_loss: 0.0458 - val_tp: 102986.0000 - val_fp: 1229.0000 - val_tn: 311566.0000 - val_fn: 1279.0000 - val_accuracy: 0.9940 - val_precision: 0.9882 - val_recall: 0.9877 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0189 - tp: 350345.0000 - fp: 1629.0000 - tn: 1061490.0000 - fn: 4028.0000 - accuracy: 0.7490 - precision: 0.9954 - recall: 0.9886 - auc: 0.9999 - val_loss: 0.0385 - val_tp: 102947.0000 - val_fp: 1146.0000 - val_tn: 311649.0000 - val_fn: 1318.0000 - val_accuracy: 0.9941 - val_precision: 0.9890 - val_recall: 0.9874 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0109 - tp: 351597.0000 - fp: 1025.0000 - tn: 1062094.0000 - fn: 2776.0000 - accuracy: 0.7503 - precision: 0.9971 - recall: 0.9922 - auc: 0.9999 - val_loss: 0.0394 - val_tp: 103010.0000 - val_fp: 1131.0000 - val_tn: 311664.0000 - val_fn: 1255.0000 - val_accuracy: 0.9943 - val_precision: 0.9891 - val_recall: 0.9880 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 0.0069 - tp: 352851.0000 - fp: 783.0000 - tn: 1062336.0000 - fn: 1522.0000 - accuracy: 0.7511 - precision: 0.9978 - recall: 0.9957 - auc: 0.9999 - val_loss: 0.0467 - val_tp: 103036.0000 - val_fp: 1143.0000 - val_tn: 311652.0000 - val_fn: 1229.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9882 - val_auc: 0.9986\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0049 - tp: 353335.0000 - fp: 658.0000 - tn: 1062461.0000 - fn: 1038.0000 - accuracy: 0.7514 - precision: 0.9981 - recall: 0.9971 - auc: 1.0000 - val_loss: 0.0477 - val_tp: 103055.0000 - val_fp: 1144.0000 - val_tn: 311651.0000 - val_fn: 1210.0000 - val_accuracy: 0.9944 - val_precision: 0.9890 - val_recall: 0.9884 - val_auc: 0.9985\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 0.0036 - tp: 353602.0000 - fp: 546.0000 - tn: 1062573.0000 - fn: 771.0000 - accuracy: 0.7516 - precision: 0.9985 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0537 - val_tp: 103069.0000 - val_fp: 1146.0000 - val_tn: 311649.0000 - val_fn: 1196.0000 - val_accuracy: 0.9944 - val_precision: 0.9890 - val_recall: 0.9885 - val_auc: 0.9979\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0028 - tp: 353784.0000 - fp: 441.0000 - tn: 1062678.0000 - fn: 589.0000 - accuracy: 0.7518 - precision: 0.9988 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0527 - val_tp: 103070.0000 - val_fp: 1141.0000 - val_tn: 311654.0000 - val_fn: 1195.0000 - val_accuracy: 0.9944 - val_precision: 0.9891 - val_recall: 0.9885 - val_auc: 0.9979\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0021 - tp: 353932.0000 - fp: 329.0000 - tn: 1062790.0000 - fn: 441.0000 - accuracy: 0.7519 - precision: 0.9991 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0553 - val_tp: 103078.0000 - val_fp: 1150.0000 - val_tn: 311645.0000 - val_fn: 1187.0000 - val_accuracy: 0.9944 - val_precision: 0.9890 - val_recall: 0.9886 - val_auc: 0.9977\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0017 - tp: 354034.0000 - fp: 274.0000 - tn: 1062845.0000 - fn: 339.0000 - accuracy: 0.7520 - precision: 0.9992 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0577 - val_tp: 103058.0000 - val_fp: 1153.0000 - val_tn: 311642.0000 - val_fn: 1207.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9884 - val_auc: 0.9973\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0013 - tp: 354096.0000 - fp: 224.0000 - tn: 1062895.0000 - fn: 277.0000 - accuracy: 0.7520 - precision: 0.9994 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0623 - val_tp: 103066.0000 - val_fp: 1164.0000 - val_tn: 311631.0000 - val_fn: 1199.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9969\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0012 - tp: 354135.0000 - fp: 194.0000 - tn: 1062925.0000 - fn: 238.0000 - accuracy: 0.7520 - precision: 0.9995 - recall: 0.9993 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0012 - tp: 354135.0000 - fp: 194.0000 - tn: 1062925.0000 - fn: 238.0000 - accuracy: 0.7520 - precision: 0.9995 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0619 - val_tp: 103083.0000 - val_fp: 1155.0000 - val_tn: 311640.0000 - val_fn: 1182.0000 - val_accuracy: 0.9944 - val_precision: 0.9889 - val_recall: 0.9887 - val_auc: 0.9970\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM/BiLSTM_downweight_0.3.h5\n",
            "\n",
            "Down-weighting: 0.4\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 52s 413ms/step - loss: 0.0918 - tp: 424390.0000 - fp: 13269.0000 - tn: 1362645.0000 - fn: 34248.0000 - accuracy: 0.7990 - precision: 0.9697 - recall: 0.9253 - auc: 0.9959 - val_loss: 0.0571 - val_tp: 103024.0000 - val_fp: 1240.0000 - val_tn: 311555.0000 - val_fn: 1241.0000 - val_accuracy: 0.9941 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9992\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 42s 399ms/step - loss: 0.0326 - tp: 350894.0000 - fp: 3092.0000 - tn: 1060027.0000 - fn: 3479.0000 - accuracy: 0.7478 - precision: 0.9913 - recall: 0.9902 - auc: 0.9995 - val_loss: 0.0460 - val_tp: 103008.0000 - val_fp: 1238.0000 - val_tn: 311557.0000 - val_fn: 1257.0000 - val_accuracy: 0.9940 - val_precision: 0.9881 - val_recall: 0.9879 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0215 - tp: 350661.0000 - fp: 1978.0000 - tn: 1061141.0000 - fn: 3712.0000 - accuracy: 0.7487 - precision: 0.9944 - recall: 0.9895 - auc: 0.9999 - val_loss: 0.0393 - val_tp: 102977.0000 - val_fp: 1174.0000 - val_tn: 311621.0000 - val_fn: 1288.0000 - val_accuracy: 0.9941 - val_precision: 0.9887 - val_recall: 0.9876 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0126 - tp: 351524.0000 - fp: 1045.0000 - tn: 1062074.0000 - fn: 2849.0000 - accuracy: 0.7500 - precision: 0.9970 - recall: 0.9920 - auc: 0.9999 - val_loss: 0.0405 - val_tp: 103014.0000 - val_fp: 1142.0000 - val_tn: 311653.0000 - val_fn: 1251.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9880 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0081 - tp: 352730.0000 - fp: 776.0000 - tn: 1062343.0000 - fn: 1643.0000 - accuracy: 0.7509 - precision: 0.9978 - recall: 0.9954 - auc: 0.9999 - val_loss: 0.0476 - val_tp: 103047.0000 - val_fp: 1155.0000 - val_tn: 311640.0000 - val_fn: 1218.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9883 - val_auc: 0.9986\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0060 - tp: 353249.0000 - fp: 681.0000 - tn: 1062438.0000 - fn: 1124.0000 - accuracy: 0.7513 - precision: 0.9981 - recall: 0.9968 - auc: 1.0000 - val_loss: 0.0478 - val_tp: 103053.0000 - val_fp: 1156.0000 - val_tn: 311639.0000 - val_fn: 1212.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9884 - val_auc: 0.9986\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0045 - tp: 353547.0000 - fp: 572.0000 - tn: 1062547.0000 - fn: 826.0000 - accuracy: 0.7515 - precision: 0.9984 - recall: 0.9977 - auc: 1.0000 - val_loss: 0.0555 - val_tp: 103049.0000 - val_fp: 1157.0000 - val_tn: 311638.0000 - val_fn: 1216.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9883 - val_auc: 0.9978\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0035 - tp: 353735.0000 - fp: 453.0000 - tn: 1062666.0000 - fn: 638.0000 - accuracy: 0.7517 - precision: 0.9987 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0562 - val_tp: 103052.0000 - val_fp: 1150.0000 - val_tn: 311645.0000 - val_fn: 1213.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9884 - val_auc: 0.9976\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 42s 398ms/step - loss: 0.0028 - tp: 353872.0000 - fp: 367.0000 - tn: 1062752.0000 - fn: 501.0000 - accuracy: 0.7518 - precision: 0.9990 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0567 - val_tp: 103050.0000 - val_fp: 1161.0000 - val_tn: 311634.0000 - val_fn: 1215.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9883 - val_auc: 0.9976\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 42s 400ms/step - loss: 0.0022 - tp: 353989.0000 - fp: 299.0000 - tn: 1062820.0000 - fn: 384.0000 - accuracy: 0.7519 - precision: 0.9992 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0607 - val_tp: 103049.0000 - val_fp: 1151.0000 - val_tn: 311644.0000 - val_fn: 1216.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9883 - val_auc: 0.9971\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 42s 398ms/step - loss: 0.0017 - tp: 354072.0000 - fp: 226.0000 - tn: 1062893.0000 - fn: 301.0000 - accuracy: 0.7520 - precision: 0.9994 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0637 - val_tp: 103007.0000 - val_fp: 1184.0000 - val_tn: 311611.0000 - val_fn: 1258.0000 - val_accuracy: 0.9941 - val_precision: 0.9886 - val_recall: 0.9879 - val_auc: 0.9968\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0015 - tp: 354124.0000 - fp: 210.0000 - tn: 1062909.0000 - fn: 249.0000 - accuracy: 0.7520 - precision: 0.9994 - recall: 0.9993 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0015 - tp: 354124.0000 - fp: 210.0000 - tn: 1062909.0000 - fn: 249.0000 - accuracy: 0.7520 - precision: 0.9994 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0694 - val_tp: 103074.0000 - val_fp: 1162.0000 - val_tn: 311633.0000 - val_fn: 1191.0000 - val_accuracy: 0.9944 - val_precision: 0.9889 - val_recall: 0.9886 - val_auc: 0.9962\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM/BiLSTM_downweight_0.4.h5\n",
            "\n",
            "Down-weighting: 0.5\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 52s 414ms/step - loss: 0.1084 - tp: 424740.0000 - fp: 13526.0000 - tn: 1362388.0000 - fn: 33898.0000 - accuracy: 0.7988 - precision: 0.9691 - recall: 0.9261 - auc: 0.9956 - val_loss: 0.0571 - val_tp: 103025.0000 - val_fp: 1240.0000 - val_tn: 311555.0000 - val_fn: 1240.0000 - val_accuracy: 0.9941 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9991\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0354 - tp: 350995.0000 - fp: 3159.0000 - tn: 1059960.0000 - fn: 3378.0000 - accuracy: 0.7478 - precision: 0.9911 - recall: 0.9905 - auc: 0.9995 - val_loss: 0.0466 - val_tp: 103016.0000 - val_fp: 1240.0000 - val_tn: 311555.0000 - val_fn: 1249.0000 - val_accuracy: 0.9940 - val_precision: 0.9881 - val_recall: 0.9880 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 42s 392ms/step - loss: 0.0238 - tp: 350849.0000 - fp: 2245.0000 - tn: 1060874.0000 - fn: 3524.0000 - accuracy: 0.7485 - precision: 0.9936 - recall: 0.9901 - auc: 0.9998 - val_loss: 0.0403 - val_tp: 102997.0000 - val_fp: 1196.0000 - val_tn: 311599.0000 - val_fn: 1268.0000 - val_accuracy: 0.9941 - val_precision: 0.9885 - val_recall: 0.9878 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 42s 400ms/step - loss: 0.0141 - tp: 351443.0000 - fp: 1126.0000 - tn: 1061993.0000 - fn: 2930.0000 - accuracy: 0.7498 - precision: 0.9968 - recall: 0.9917 - auc: 0.9999 - val_loss: 0.0413 - val_tp: 103019.0000 - val_fp: 1149.0000 - val_tn: 311646.0000 - val_fn: 1246.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9880 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0092 - tp: 352642.0000 - fp: 766.0000 - tn: 1062353.0000 - fn: 1731.0000 - accuracy: 0.7508 - precision: 0.9978 - recall: 0.9951 - auc: 0.9999 - val_loss: 0.0488 - val_tp: 103039.0000 - val_fp: 1159.0000 - val_tn: 311636.0000 - val_fn: 1226.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9882 - val_auc: 0.9986\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0068 - tp: 353192.0000 - fp: 655.0000 - tn: 1062464.0000 - fn: 1181.0000 - accuracy: 0.7512 - precision: 0.9981 - recall: 0.9967 - auc: 1.0000 - val_loss: 0.0487 - val_tp: 103057.0000 - val_fp: 1157.0000 - val_tn: 311638.0000 - val_fn: 1208.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9884 - val_auc: 0.9986\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0050 - tp: 353552.0000 - fp: 531.0000 - tn: 1062588.0000 - fn: 821.0000 - accuracy: 0.7515 - precision: 0.9985 - recall: 0.9977 - auc: 1.0000 - val_loss: 0.0596 - val_tp: 103051.0000 - val_fp: 1171.0000 - val_tn: 311624.0000 - val_fn: 1214.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9884 - val_auc: 0.9971\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 0.0039 - tp: 353746.0000 - fp: 438.0000 - tn: 1062681.0000 - fn: 627.0000 - accuracy: 0.7517 - precision: 0.9988 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0580 - val_tp: 103042.0000 - val_fp: 1159.0000 - val_tn: 311636.0000 - val_fn: 1223.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9883 - val_auc: 0.9974\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 42s 400ms/step - loss: 0.0032 - tp: 353835.0000 - fp: 381.0000 - tn: 1062738.0000 - fn: 538.0000 - accuracy: 0.7517 - precision: 0.9989 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0595 - val_tp: 103054.0000 - val_fp: 1161.0000 - val_tn: 311634.0000 - val_fn: 1211.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9884 - val_auc: 0.9972\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0025 - tp: 353995.0000 - fp: 291.0000 - tn: 1062828.0000 - fn: 378.0000 - accuracy: 0.7519 - precision: 0.9992 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0644 - val_tp: 103051.0000 - val_fp: 1170.0000 - val_tn: 311625.0000 - val_fn: 1214.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9884 - val_auc: 0.9965\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0019 - tp: 354067.0000 - fp: 229.0000 - tn: 1062890.0000 - fn: 306.0000 - accuracy: 0.7519 - precision: 0.9994 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0710 - val_tp: 103042.0000 - val_fp: 1177.0000 - val_tn: 311618.0000 - val_fn: 1223.0000 - val_accuracy: 0.9942 - val_precision: 0.9887 - val_recall: 0.9883 - val_auc: 0.9959\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0017 - tp: 354121.0000 - fp: 195.0000 - tn: 1062924.0000 - fn: 252.0000 - accuracy: 0.7520 - precision: 0.9994 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0753 - val_tp: 103070.0000 - val_fp: 1170.0000 - val_tn: 311625.0000 - val_fn: 1195.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9955\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0013 - tp: 354182.0000 - fp: 152.0000 - tn: 1062967.0000 - fn: 191.0000 - accuracy: 0.7520 - precision: 0.9996 - recall: 0.9995 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 42s 398ms/step - loss: 0.0013 - tp: 354182.0000 - fp: 152.0000 - tn: 1062967.0000 - fn: 191.0000 - accuracy: 0.7520 - precision: 0.9996 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0762 - val_tp: 103067.0000 - val_fp: 1169.0000 - val_tn: 311626.0000 - val_fn: 1198.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9955\n",
            "Epoch 00013: early stopping\n",
            "Model saved at BiLSTM/BiLSTM_downweight_0.5.h5\n",
            "\n",
            "Down-weighting: 0.6\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 52s 417ms/step - loss: 0.1247 - tp: 424981.0000 - fp: 13683.0000 - tn: 1362231.0000 - fn: 33657.0000 - accuracy: 0.7987 - precision: 0.9688 - recall: 0.9266 - auc: 0.9954 - val_loss: 0.0576 - val_tp: 103025.0000 - val_fp: 1240.0000 - val_tn: 311555.0000 - val_fn: 1240.0000 - val_accuracy: 0.9941 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9991\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 42s 392ms/step - loss: 0.0379 - tp: 351031.0000 - fp: 3199.0000 - tn: 1059920.0000 - fn: 3342.0000 - accuracy: 0.7477 - precision: 0.9910 - recall: 0.9906 - auc: 0.9994 - val_loss: 0.0473 - val_tp: 103020.0000 - val_fp: 1241.0000 - val_tn: 311554.0000 - val_fn: 1245.0000 - val_accuracy: 0.9940 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0258 - tp: 350953.0000 - fp: 2459.0000 - tn: 1060660.0000 - fn: 3420.0000 - accuracy: 0.7483 - precision: 0.9930 - recall: 0.9903 - auc: 0.9998 - val_loss: 0.0413 - val_tp: 103006.0000 - val_fp: 1211.0000 - val_tn: 311584.0000 - val_fn: 1259.0000 - val_accuracy: 0.9941 - val_precision: 0.9884 - val_recall: 0.9879 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0155 - tp: 351373.0000 - fp: 1203.0000 - tn: 1061916.0000 - fn: 3000.0000 - accuracy: 0.7496 - precision: 0.9966 - recall: 0.9915 - auc: 0.9999 - val_loss: 0.0421 - val_tp: 103020.0000 - val_fp: 1146.0000 - val_tn: 311649.0000 - val_fn: 1245.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9881 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0101 - tp: 352546.0000 - fp: 771.0000 - tn: 1062348.0000 - fn: 1827.0000 - accuracy: 0.7507 - precision: 0.9978 - recall: 0.9948 - auc: 0.9999 - val_loss: 0.0496 - val_tp: 103043.0000 - val_fp: 1166.0000 - val_tn: 311629.0000 - val_fn: 1222.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9883 - val_auc: 0.9985\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0075 - tp: 353147.0000 - fp: 650.0000 - tn: 1062469.0000 - fn: 1226.0000 - accuracy: 0.7511 - precision: 0.9982 - recall: 0.9965 - auc: 1.0000 - val_loss: 0.0503 - val_tp: 103055.0000 - val_fp: 1166.0000 - val_tn: 311629.0000 - val_fn: 1210.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9884 - val_auc: 0.9984\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0055 - tp: 353510.0000 - fp: 530.0000 - tn: 1062589.0000 - fn: 863.0000 - accuracy: 0.7514 - precision: 0.9985 - recall: 0.9976 - auc: 1.0000 - val_loss: 0.0613 - val_tp: 103050.0000 - val_fp: 1174.0000 - val_tn: 311621.0000 - val_fn: 1215.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9883 - val_auc: 0.9968\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0043 - tp: 353730.0000 - fp: 429.0000 - tn: 1062690.0000 - fn: 643.0000 - accuracy: 0.7516 - precision: 0.9988 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0570 - val_tp: 103037.0000 - val_fp: 1170.0000 - val_tn: 311625.0000 - val_fn: 1228.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9882 - val_auc: 0.9976\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0036 - tp: 353833.0000 - fp: 383.0000 - tn: 1062736.0000 - fn: 540.0000 - accuracy: 0.7517 - precision: 0.9989 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0611 - val_tp: 103059.0000 - val_fp: 1165.0000 - val_tn: 311630.0000 - val_fn: 1206.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9884 - val_auc: 0.9969\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0028 - tp: 353978.0000 - fp: 297.0000 - tn: 1062822.0000 - fn: 395.0000 - accuracy: 0.7518 - precision: 0.9992 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0664 - val_tp: 103056.0000 - val_fp: 1167.0000 - val_tn: 311628.0000 - val_fn: 1209.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9884 - val_auc: 0.9963\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 42s 398ms/step - loss: 0.0023 - tp: 354043.0000 - fp: 244.0000 - tn: 1062875.0000 - fn: 330.0000 - accuracy: 0.7519 - precision: 0.9993 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0693 - val_tp: 103049.0000 - val_fp: 1166.0000 - val_tn: 311629.0000 - val_fn: 1216.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9883 - val_auc: 0.9961\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0019 - tp: 354123.0000 - fp: 193.0000 - tn: 1062926.0000 - fn: 250.0000 - accuracy: 0.7520 - precision: 0.9995 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0779 - val_tp: 103071.0000 - val_fp: 1172.0000 - val_tn: 311623.0000 - val_fn: 1194.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9953\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0015 - tp: 354172.0000 - fp: 161.0000 - tn: 1062958.0000 - fn: 201.0000 - accuracy: 0.7520 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 42s 398ms/step - loss: 0.0015 - tp: 354172.0000 - fp: 161.0000 - tn: 1062958.0000 - fn: 201.0000 - accuracy: 0.7520 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0744 - val_tp: 103065.0000 - val_fp: 1167.0000 - val_tn: 311628.0000 - val_fn: 1200.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9955\n",
            "Epoch 00013: early stopping\n",
            "Model saved at BiLSTM/BiLSTM_downweight_0.6.h5\n",
            "\n",
            "Down-weighting: 0.7\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 53s 413ms/step - loss: 0.1408 - tp: 425175.0000 - fp: 13792.0000 - tn: 1362122.0000 - fn: 33463.0000 - accuracy: 0.7987 - precision: 0.9686 - recall: 0.9270 - auc: 0.9954 - val_loss: 0.0583 - val_tp: 103025.0000 - val_fp: 1240.0000 - val_tn: 311555.0000 - val_fn: 1240.0000 - val_accuracy: 0.9941 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9991\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 42s 398ms/step - loss: 0.0402 - tp: 351038.0000 - fp: 3212.0000 - tn: 1059907.0000 - fn: 3335.0000 - accuracy: 0.7477 - precision: 0.9909 - recall: 0.9906 - auc: 0.9994 - val_loss: 0.0481 - val_tp: 103022.0000 - val_fp: 1241.0000 - val_tn: 311554.0000 - val_fn: 1243.0000 - val_accuracy: 0.9940 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0277 - tp: 351022.0000 - fp: 2618.0000 - tn: 1060501.0000 - fn: 3351.0000 - accuracy: 0.7482 - precision: 0.9926 - recall: 0.9905 - auc: 0.9998 - val_loss: 0.0423 - val_tp: 103011.0000 - val_fp: 1223.0000 - val_tn: 311572.0000 - val_fn: 1254.0000 - val_accuracy: 0.9941 - val_precision: 0.9883 - val_recall: 0.9880 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0169 - tp: 351354.0000 - fp: 1343.0000 - tn: 1061776.0000 - fn: 3019.0000 - accuracy: 0.7495 - precision: 0.9962 - recall: 0.9915 - auc: 0.9999 - val_loss: 0.0428 - val_tp: 103026.0000 - val_fp: 1157.0000 - val_tn: 311638.0000 - val_fn: 1239.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9881 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 41s 391ms/step - loss: 0.0109 - tp: 352439.0000 - fp: 807.0000 - tn: 1062312.0000 - fn: 1934.0000 - accuracy: 0.7506 - precision: 0.9977 - recall: 0.9945 - auc: 0.9999 - val_loss: 0.0506 - val_tp: 103043.0000 - val_fp: 1177.0000 - val_tn: 311618.0000 - val_fn: 1222.0000 - val_accuracy: 0.9942 - val_precision: 0.9887 - val_recall: 0.9883 - val_auc: 0.9984\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0081 - tp: 353089.0000 - fp: 671.0000 - tn: 1062448.0000 - fn: 1284.0000 - accuracy: 0.7511 - precision: 0.9981 - recall: 0.9964 - auc: 1.0000 - val_loss: 0.0511 - val_tp: 103062.0000 - val_fp: 1168.0000 - val_tn: 311627.0000 - val_fn: 1203.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9984\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0060 - tp: 353458.0000 - fp: 533.0000 - tn: 1062586.0000 - fn: 915.0000 - accuracy: 0.7514 - precision: 0.9985 - recall: 0.9974 - auc: 1.0000 - val_loss: 0.0615 - val_tp: 103053.0000 - val_fp: 1175.0000 - val_tn: 311620.0000 - val_fn: 1212.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9884 - val_auc: 0.9967\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0046 - tp: 353709.0000 - fp: 434.0000 - tn: 1062685.0000 - fn: 664.0000 - accuracy: 0.7516 - precision: 0.9988 - recall: 0.9981 - auc: 1.0000 - val_loss: 0.0586 - val_tp: 103044.0000 - val_fp: 1176.0000 - val_tn: 311619.0000 - val_fn: 1221.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9883 - val_auc: 0.9974\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 42s 400ms/step - loss: 0.0039 - tp: 353832.0000 - fp: 383.0000 - tn: 1062736.0000 - fn: 541.0000 - accuracy: 0.7517 - precision: 0.9989 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0641 - val_tp: 103065.0000 - val_fp: 1169.0000 - val_tn: 311626.0000 - val_fn: 1200.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9965\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0031 - tp: 353969.0000 - fp: 281.0000 - tn: 1062838.0000 - fn: 404.0000 - accuracy: 0.7518 - precision: 0.9992 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0666 - val_tp: 103044.0000 - val_fp: 1175.0000 - val_tn: 311620.0000 - val_fn: 1221.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9883 - val_auc: 0.9962\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0024 - tp: 354026.0000 - fp: 246.0000 - tn: 1062873.0000 - fn: 347.0000 - accuracy: 0.7519 - precision: 0.9993 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0701 - val_tp: 103045.0000 - val_fp: 1178.0000 - val_tn: 311617.0000 - val_fn: 1220.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9883 - val_auc: 0.9959\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0021 - tp: 354104.0000 - fp: 197.0000 - tn: 1062922.0000 - fn: 269.0000 - accuracy: 0.7520 - precision: 0.9994 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0770 - val_tp: 103069.0000 - val_fp: 1178.0000 - val_tn: 311617.0000 - val_fn: 1196.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9885 - val_auc: 0.9952\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0016 - tp: 354152.0000 - fp: 169.0000 - tn: 1062950.0000 - fn: 221.0000 - accuracy: 0.7520 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 43s 407ms/step - loss: 0.0016 - tp: 354152.0000 - fp: 169.0000 - tn: 1062950.0000 - fn: 221.0000 - accuracy: 0.7520 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0792 - val_tp: 103066.0000 - val_fp: 1174.0000 - val_tn: 311621.0000 - val_fn: 1199.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9885 - val_auc: 0.9951\n",
            "Epoch 00013: early stopping\n",
            "Model saved at BiLSTM/BiLSTM_downweight_0.7.h5\n",
            "\n",
            "Down-weighting: 0.8\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 53s 421ms/step - loss: 0.1568 - tp: 425267.0000 - fp: 13925.0000 - tn: 1361989.0000 - fn: 33371.0000 - accuracy: 0.7986 - precision: 0.9683 - recall: 0.9272 - auc: 0.9952 - val_loss: 0.0590 - val_tp: 103025.0000 - val_fp: 1240.0000 - val_tn: 311555.0000 - val_fn: 1240.0000 - val_accuracy: 0.9941 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9990\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 42s 399ms/step - loss: 0.0422 - tp: 351043.0000 - fp: 3221.0000 - tn: 1059898.0000 - fn: 3330.0000 - accuracy: 0.7477 - precision: 0.9909 - recall: 0.9906 - auc: 0.9993 - val_loss: 0.0489 - val_tp: 103021.0000 - val_fp: 1241.0000 - val_tn: 311554.0000 - val_fn: 1244.0000 - val_accuracy: 0.9940 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0295 - tp: 351073.0000 - fp: 2764.0000 - tn: 1060355.0000 - fn: 3300.0000 - accuracy: 0.7481 - precision: 0.9922 - recall: 0.9907 - auc: 0.9998 - val_loss: 0.0433 - val_tp: 103021.0000 - val_fp: 1230.0000 - val_tn: 311565.0000 - val_fn: 1244.0000 - val_accuracy: 0.9941 - val_precision: 0.9882 - val_recall: 0.9881 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0181 - tp: 351332.0000 - fp: 1485.0000 - tn: 1061634.0000 - fn: 3041.0000 - accuracy: 0.7493 - precision: 0.9958 - recall: 0.9914 - auc: 0.9999 - val_loss: 0.0436 - val_tp: 103023.0000 - val_fp: 1161.0000 - val_tn: 311634.0000 - val_fn: 1242.0000 - val_accuracy: 0.9942 - val_precision: 0.9889 - val_recall: 0.9881 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 0.0118 - tp: 352345.0000 - fp: 837.0000 - tn: 1062282.0000 - fn: 2028.0000 - accuracy: 0.7504 - precision: 0.9976 - recall: 0.9943 - auc: 0.9999 - val_loss: 0.0510 - val_tp: 103034.0000 - val_fp: 1184.0000 - val_tn: 311611.0000 - val_fn: 1231.0000 - val_accuracy: 0.9942 - val_precision: 0.9886 - val_recall: 0.9882 - val_auc: 0.9984\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 42s 400ms/step - loss: 0.0088 - tp: 352997.0000 - fp: 709.0000 - tn: 1062410.0000 - fn: 1376.0000 - accuracy: 0.7509 - precision: 0.9980 - recall: 0.9961 - auc: 1.0000 - val_loss: 0.0515 - val_tp: 103062.0000 - val_fp: 1170.0000 - val_tn: 311625.0000 - val_fn: 1203.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9983\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0065 - tp: 353405.0000 - fp: 545.0000 - tn: 1062574.0000 - fn: 968.0000 - accuracy: 0.7513 - precision: 0.9985 - recall: 0.9973 - auc: 1.0000 - val_loss: 0.0609 - val_tp: 103053.0000 - val_fp: 1176.0000 - val_tn: 311619.0000 - val_fn: 1212.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9884 - val_auc: 0.9968\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 42s 394ms/step - loss: 0.0051 - tp: 353668.0000 - fp: 457.0000 - tn: 1062662.0000 - fn: 705.0000 - accuracy: 0.7515 - precision: 0.9987 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0585 - val_tp: 103070.0000 - val_fp: 1162.0000 - val_tn: 311633.0000 - val_fn: 1195.0000 - val_accuracy: 0.9943 - val_precision: 0.9889 - val_recall: 0.9885 - val_auc: 0.9973\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 42s 395ms/step - loss: 0.0043 - tp: 353804.0000 - fp: 397.0000 - tn: 1062722.0000 - fn: 569.0000 - accuracy: 0.7517 - precision: 0.9989 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0645 - val_tp: 103068.0000 - val_fp: 1169.0000 - val_tn: 311626.0000 - val_fn: 1197.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9964\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0033 - tp: 353957.0000 - fp: 292.0000 - tn: 1062827.0000 - fn: 416.0000 - accuracy: 0.7518 - precision: 0.9992 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0660 - val_tp: 103064.0000 - val_fp: 1169.0000 - val_tn: 311626.0000 - val_fn: 1201.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9963\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0026 - tp: 354031.0000 - fp: 237.0000 - tn: 1062882.0000 - fn: 342.0000 - accuracy: 0.7519 - precision: 0.9993 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0685 - val_tp: 103044.0000 - val_fp: 1181.0000 - val_tn: 311614.0000 - val_fn: 1221.0000 - val_accuracy: 0.9942 - val_precision: 0.9887 - val_recall: 0.9883 - val_auc: 0.9961\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 42s 399ms/step - loss: 0.0023 - tp: 354099.0000 - fp: 202.0000 - tn: 1062917.0000 - fn: 274.0000 - accuracy: 0.7519 - precision: 0.9994 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0760 - val_tp: 103067.0000 - val_fp: 1176.0000 - val_tn: 311619.0000 - val_fn: 1198.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9885 - val_auc: 0.9952\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0018 - tp: 354152.0000 - fp: 163.0000 - tn: 1062956.0000 - fn: 221.0000 - accuracy: 0.7520 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 42s 393ms/step - loss: 0.0018 - tp: 354152.0000 - fp: 163.0000 - tn: 1062956.0000 - fn: 221.0000 - accuracy: 0.7520 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0774 - val_tp: 103061.0000 - val_fp: 1180.0000 - val_tn: 311615.0000 - val_fn: 1204.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9885 - val_auc: 0.9952\n",
            "Epoch 00013: early stopping\n",
            "Model saved at BiLSTM/BiLSTM_downweight_0.8.h5\n",
            "\n",
            "Down-weighting: 0.9\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 53s 417ms/step - loss: 0.1727 - tp: 425355.0000 - fp: 14001.0000 - tn: 1361913.0000 - fn: 33283.0000 - accuracy: 0.7985 - precision: 0.9681 - recall: 0.9274 - auc: 0.9952 - val_loss: 0.0598 - val_tp: 103025.0000 - val_fp: 1240.0000 - val_tn: 311555.0000 - val_fn: 1240.0000 - val_accuracy: 0.9941 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9990\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 42s 399ms/step - loss: 0.0441 - tp: 351050.0000 - fp: 3233.0000 - tn: 1059886.0000 - fn: 3323.0000 - accuracy: 0.7477 - precision: 0.9909 - recall: 0.9906 - auc: 0.9992 - val_loss: 0.0497 - val_tp: 103022.0000 - val_fp: 1241.0000 - val_tn: 311554.0000 - val_fn: 1243.0000 - val_accuracy: 0.9940 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9994\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 42s 398ms/step - loss: 0.0311 - tp: 351105.0000 - fp: 2834.0000 - tn: 1060285.0000 - fn: 3268.0000 - accuracy: 0.7480 - precision: 0.9920 - recall: 0.9908 - auc: 0.9998 - val_loss: 0.0442 - val_tp: 103023.0000 - val_fp: 1235.0000 - val_tn: 311560.0000 - val_fn: 1242.0000 - val_accuracy: 0.9941 - val_precision: 0.9882 - val_recall: 0.9881 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 42s 400ms/step - loss: 0.0193 - tp: 351304.0000 - fp: 1630.0000 - tn: 1061489.0000 - fn: 3069.0000 - accuracy: 0.7491 - precision: 0.9954 - recall: 0.9913 - auc: 0.9999 - val_loss: 0.0442 - val_tp: 103027.0000 - val_fp: 1173.0000 - val_tn: 311622.0000 - val_fn: 1238.0000 - val_accuracy: 0.9942 - val_precision: 0.9887 - val_recall: 0.9881 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0125 - tp: 352247.0000 - fp: 896.0000 - tn: 1062223.0000 - fn: 2126.0000 - accuracy: 0.7503 - precision: 0.9975 - recall: 0.9940 - auc: 0.9999 - val_loss: 0.0516 - val_tp: 103039.0000 - val_fp: 1189.0000 - val_tn: 311606.0000 - val_fn: 1226.0000 - val_accuracy: 0.9942 - val_precision: 0.9886 - val_recall: 0.9882 - val_auc: 0.9983\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0094 - tp: 352904.0000 - fp: 729.0000 - tn: 1062390.0000 - fn: 1469.0000 - accuracy: 0.7508 - precision: 0.9979 - recall: 0.9959 - auc: 0.9999 - val_loss: 0.0521 - val_tp: 103058.0000 - val_fp: 1175.0000 - val_tn: 311620.0000 - val_fn: 1207.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9884 - val_auc: 0.9983\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 42s 401ms/step - loss: 0.0070 - tp: 353354.0000 - fp: 569.0000 - tn: 1062550.0000 - fn: 1019.0000 - accuracy: 0.7512 - precision: 0.9984 - recall: 0.9971 - auc: 1.0000 - val_loss: 0.0609 - val_tp: 103057.0000 - val_fp: 1177.0000 - val_tn: 311618.0000 - val_fn: 1208.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9884 - val_auc: 0.9968\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0055 - tp: 353639.0000 - fp: 455.0000 - tn: 1062664.0000 - fn: 734.0000 - accuracy: 0.7515 - precision: 0.9987 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0590 - val_tp: 103061.0000 - val_fp: 1167.0000 - val_tn: 311628.0000 - val_fn: 1204.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9972\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0046 - tp: 353797.0000 - fp: 395.0000 - tn: 1062724.0000 - fn: 576.0000 - accuracy: 0.7516 - precision: 0.9989 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0630 - val_tp: 103066.0000 - val_fp: 1170.0000 - val_tn: 311625.0000 - val_fn: 1199.0000 - val_accuracy: 0.9943 - val_precision: 0.9888 - val_recall: 0.9885 - val_auc: 0.9966\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0036 - tp: 353928.0000 - fp: 298.0000 - tn: 1062821.0000 - fn: 445.0000 - accuracy: 0.7518 - precision: 0.9992 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0655 - val_tp: 103066.0000 - val_fp: 1173.0000 - val_tn: 311622.0000 - val_fn: 1199.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9885 - val_auc: 0.9963\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 42s 401ms/step - loss: 0.0029 - tp: 354002.0000 - fp: 258.0000 - tn: 1062861.0000 - fn: 371.0000 - accuracy: 0.7519 - precision: 0.9993 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0677 - val_tp: 103029.0000 - val_fp: 1196.0000 - val_tn: 311599.0000 - val_fn: 1236.0000 - val_accuracy: 0.9942 - val_precision: 0.9885 - val_recall: 0.9881 - val_auc: 0.9962\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 42s 397ms/step - loss: 0.0025 - tp: 354064.0000 - fp: 220.0000 - tn: 1062899.0000 - fn: 309.0000 - accuracy: 0.7519 - precision: 0.9994 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0767 - val_tp: 103069.0000 - val_fp: 1174.0000 - val_tn: 311621.0000 - val_fn: 1196.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9885 - val_auc: 0.9951\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0021 - tp: 354117.0000 - fp: 192.0000 - tn: 1062927.0000 - fn: 256.0000 - accuracy: 0.7520 - precision: 0.9995 - recall: 0.9993 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 42s 396ms/step - loss: 0.0021 - tp: 354117.0000 - fp: 192.0000 - tn: 1062927.0000 - fn: 256.0000 - accuracy: 0.7520 - precision: 0.9995 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0777 - val_tp: 103067.0000 - val_fp: 1180.0000 - val_tn: 311615.0000 - val_fn: 1198.0000 - val_accuracy: 0.9943 - val_precision: 0.9887 - val_recall: 0.9885 - val_auc: 0.9950\n",
            "Epoch 00013: early stopping\n",
            "Model saved at BiLSTM/BiLSTM_downweight_0.9.h5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoF17aAFY5l5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc99b62-dbbf-49c5-f1ab-d63373cce0d3"
      },
      "source": [
        "downweight_models[1.0] = model\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n",
        "    preds = np.argmax(downweight_models[weight].predict(dev_seqs_padded), axis=-1)\n",
        "    flat_preds = [p for pred in preds for p in pred]\n",
        "    print(f'Weight = {weight}:', Counter(flat_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 0.1: Counter({3: 88887, 2: 14937, 0: 416, 1: 25})\n",
            "Weight = 0.2: Counter({3: 88892, 2: 15339, 0: 34})\n",
            "Weight = 0.3: Counter({3: 88889, 2: 15371, 0: 5})\n",
            "Weight = 0.4: Counter({3: 88889, 2: 15375, 0: 1})\n",
            "Weight = 0.5: Counter({3: 88891, 2: 15346, 0: 28})\n",
            "Weight = 0.6: Counter({3: 88890, 2: 15363, 0: 12})\n",
            "Weight = 0.7: Counter({3: 88888, 2: 15372, 0: 5})\n",
            "Weight = 0.8: Counter({3: 88888, 2: 15376, 0: 1})\n",
            "Weight = 0.9: Counter({3: 88888, 2: 15376, 0: 1})\n",
            "Weight = 1.0: Counter({3: 88887, 2: 15377, 0: 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgjKM0ZT_ebY",
        "outputId": "2c29158b-8970-4c7e-9f6d-e8f5d40a6bbe"
      },
      "source": [
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n",
        "    preds = np.argmax(downweight_models[weight].predict(dev_seqs_padded), axis=-1)\n",
        "\n",
        "    dev_seqs['prediction'] = ''\n",
        "    for i in dev_seqs.index:\n",
        "        this_seq_length = len(dev_seqs['token'][i])\n",
        "        dev_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    dev_long = dev_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = [reverse_bio(b) for b in dev_long['bio_only']]\n",
        "    dev_long['bio_only'] = bio_labs\n",
        "    pred_labs = [reverse_bio(b) for b in dev_long['prediction']]\n",
        "    dev_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(dev_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 0.1:\n",
            "Sum of TP and FP = 416\n",
            "Sum of TP and FN = 826\n",
            "True positives = 69, False positives = 347, False negatives = 757\n",
            "Precision = 0.166, Recall = 0.084, F1 = 0.111\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 34\n",
            "Sum of TP and FN = 826\n",
            "True positives = 1, False positives = 33, False negatives = 825\n",
            "Precision = 0.029, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 5\n",
            "Sum of TP and FN = 826\n",
            "True positives = 1, False positives = 4, False negatives = 825\n",
            "Precision = 0.200, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 1\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 1, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 28\n",
            "Sum of TP and FN = 826\n",
            "True positives = 6, False positives = 22, False negatives = 820\n",
            "Precision = 0.214, Recall = 0.007, F1 = 0.014\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 12\n",
            "Sum of TP and FN = 826\n",
            "True positives = 1, False positives = 11, False negatives = 825\n",
            "Precision = 0.083, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 5\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 5, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 1\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 1, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 1\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 1, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 1.0:\n",
            "Sum of TP and FP = 1\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 1, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCiMJUG0t15b"
      },
      "source": [
        "### 2. Down-sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MSQ4YZdbHFTh",
        "outputId": "3b42b9be-8bab-49ee-efc9-0456f85b2162"
      },
      "source": [
        "mask = train_seqs.bio_only.apply(lambda labels: 0.0 in labels)\n",
        "train_seqs_downsampled = train_seqs[mask]\n",
        "train_seqs_downsampled.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[@paulwalk, It, 's, the, view, from, where, I,...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[From, Green, Newsfeed, :, AHFA, extends, dead...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Pxleyes, Top, 50, Photography, Contest, Pictu...</td>\n",
              "      <td>[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[4Dbling, 's, place, til, monday, ,, party, pa...</td>\n",
              "      <td>[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>[watching, the, VMA, pre-show, again, lol, it,...</td>\n",
              "      <td>[2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[65.0, 3.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                      token_indices\n",
              "0             0  ...  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...\n",
              "1             1  ...  [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
              "2             2  ...  [39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....\n",
              "4             4  ...  [57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...\n",
              "5             5  ...  [65.0, 3.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73lVCYhcL0vz",
        "outputId": "59e06c64-54ea-4f70-ea65-fdd55d1300cf"
      },
      "source": [
        "train_seqs_downsampled_padded = pad_sequences(train_seqs_downsampled['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                              dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "train_labs_downsampled_padded = pad_sequences(train_seqs_downsampled['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                              dtype='int32', padding='post', truncating='post', value=padlab)\n",
        "train_labs_downsampled_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_downsampled_padded]\n",
        "\n",
        "# follow the print outputs below to see how the labels are transformed\n",
        "print('Example of padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(train_seqs_downsampled.loc[1])\n",
        "print('Length of input sequence: %i' % len(train_seqs_downsampled_padded[1]))\n",
        "print('Length of label sequence: %i' % len(train_labs_downsampled_onehot[1]))\n",
        "print(train_labs_downsampled_padded[1][:11])\n",
        "print(train_labs_downsampled_onehot[1][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     1\n",
            "token            [From, Green, Newsfeed, :, AHFA, extends, dead...\n",
            "bio_only         [2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, ...\n",
            "token_indices    [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
            "Name: 1, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of label sequence: 105\n",
            "[2 2 2 2 0 2 2 2 2 2 2]\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q54iE55qV7Ue",
        "outputId": "e478563a-c753-4f15-ef6d-14c744035003"
      },
      "source": [
        "# figure out the label distribution in our downsampled fixed-length texts\n",
        "all_labs = [l for lab in train_labs_downsampled_padded for l in lab]\n",
        "label_count = Counter(all_labs)\n",
        "total_labs = len(all_labs)\n",
        "print(label_count)\n",
        "print(total_labs)\n",
        "\n",
        "# use this to define an initial model bias\n",
        "initial_bias=[(label_count[0]/total_labs), (label_count[1]/total_labs),\n",
        "              (label_count[2]/total_labs), (label_count[3]/total_labs)]\n",
        "print('Initial bias:')\n",
        "print(initial_bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({3: 103017, 2: 22152, 0: 1964, 1: 1177})\n",
            "128310\n",
            "Initial bias:\n",
            "[0.015306679136466371, 0.00917309640713896, 0.17264437689969606, 0.8028758475566986]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuY5f7vQZuv_",
        "outputId": "e5420255-7a98-4ad3-9853-945e19a920dc"
      },
      "source": [
        "# prepare sequences and labels as numpy arrays, check dimensions\n",
        "X = np.array(train_seqs_downsampled_padded)\n",
        "y = np.array(train_labs_downsampled_onehot)\n",
        "print('Input sequence dimensions (n.docs, seq.length):')\n",
        "print(X.shape)\n",
        "print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence dimensions (n.docs, seq.length):\n",
            "(1222, 105)\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):\n",
            "(1222, 105, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVrVoUDTmeR7",
        "outputId": "ab62ab7f-e0d3-47ad-af20-8b7ac6d125ca"
      },
      "source": [
        "# re-initiate model with bias\n",
        "model = make_model(output_bias=initial_bias)\n",
        "\n",
        "# and fit...\n",
        "model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(dev_X, dev_y))\n",
        "\n",
        "# save the model\n",
        "model.save('BiLSTM_downsampled/BiLSTM_downsampled.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 475ms/step - loss: 0.4386 - tp: 198668.0000 - fp: 10985.0000 - tn: 686734.0000 - fn: 33905.0000 - accuracy: 0.9517 - precision: 0.9476 - recall: 0.8542 - auc: 0.9895 - val_loss: 0.1109 - val_tp: 101140.0000 - val_fp: 2704.0000 - val_tn: 310091.0000 - val_fn: 3125.0000 - val_accuracy: 0.9860 - val_precision: 0.9740 - val_recall: 0.9700 - val_auc: 0.9981\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.1315 - tp: 124283.0000 - fp: 3662.0000 - tn: 381262.0000 - fn: 4025.0000 - accuracy: 0.9850 - precision: 0.9714 - recall: 0.9686 - auc: 0.9962 - val_loss: 0.0638 - val_tp: 102962.0000 - val_fp: 1250.0000 - val_tn: 311545.0000 - val_fn: 1303.0000 - val_accuracy: 0.9939 - val_precision: 0.9880 - val_recall: 0.9875 - val_auc: 0.9990\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.1032 - tp: 124966.0000 - fp: 3201.0000 - tn: 381723.0000 - fn: 3342.0000 - accuracy: 0.9873 - precision: 0.9750 - recall: 0.9740 - auc: 0.9976 - val_loss: 0.0555 - val_tp: 103008.0000 - val_fp: 1236.0000 - val_tn: 311559.0000 - val_fn: 1257.0000 - val_accuracy: 0.9940 - val_precision: 0.9881 - val_recall: 0.9879 - val_auc: 0.9993\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0903 - tp: 125072.0000 - fp: 3149.0000 - tn: 381775.0000 - fn: 3236.0000 - accuracy: 0.9876 - precision: 0.9754 - recall: 0.9748 - auc: 0.9984 - val_loss: 0.0502 - val_tp: 103012.0000 - val_fp: 1245.0000 - val_tn: 311550.0000 - val_fn: 1253.0000 - val_accuracy: 0.9940 - val_precision: 0.9881 - val_recall: 0.9880 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 415ms/step - loss: 0.0756 - tp: 125082.0000 - fp: 2864.0000 - tn: 382060.0000 - fn: 3226.0000 - accuracy: 0.9881 - precision: 0.9776 - recall: 0.9749 - auc: 0.9991 - val_loss: 0.0449 - val_tp: 103004.0000 - val_fp: 1244.0000 - val_tn: 311551.0000 - val_fn: 1261.0000 - val_accuracy: 0.9940 - val_precision: 0.9881 - val_recall: 0.9879 - val_auc: 0.9996\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 0.0593 - tp: 125053.0000 - fp: 2091.0000 - tn: 382833.0000 - fn: 3255.0000 - accuracy: 0.9896 - precision: 0.9836 - recall: 0.9746 - auc: 0.9994 - val_loss: 0.0411 - val_tp: 102957.0000 - val_fp: 1208.0000 - val_tn: 311587.0000 - val_fn: 1308.0000 - val_accuracy: 0.9940 - val_precision: 0.9884 - val_recall: 0.9875 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 413ms/step - loss: 0.0433 - tp: 125559.0000 - fp: 1080.0000 - tn: 383844.0000 - fn: 2749.0000 - accuracy: 0.9925 - precision: 0.9915 - recall: 0.9786 - auc: 0.9996 - val_loss: 0.0395 - val_tp: 102945.0000 - val_fp: 1164.0000 - val_tn: 311631.0000 - val_fn: 1320.0000 - val_accuracy: 0.9940 - val_precision: 0.9888 - val_recall: 0.9873 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0312 - tp: 126326.0000 - fp: 675.0000 - tn: 384249.0000 - fn: 1982.0000 - accuracy: 0.9948 - precision: 0.9947 - recall: 0.9846 - auc: 0.9998 - val_loss: 0.0400 - val_tp: 102971.0000 - val_fp: 1161.0000 - val_tn: 311634.0000 - val_fn: 1294.0000 - val_accuracy: 0.9941 - val_precision: 0.9889 - val_recall: 0.9876 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 420ms/step - loss: 0.0247 - tp: 126859.0000 - fp: 591.0000 - tn: 384333.0000 - fn: 1449.0000 - accuracy: 0.9960 - precision: 0.9954 - recall: 0.9887 - auc: 0.9998 - val_loss: 0.0406 - val_tp: 102977.0000 - val_fp: 1157.0000 - val_tn: 311638.0000 - val_fn: 1288.0000 - val_accuracy: 0.9941 - val_precision: 0.9889 - val_recall: 0.9876 - val_auc: 0.9991\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.0202 - tp: 127214.0000 - fp: 548.0000 - tn: 384376.0000 - fn: 1094.0000 - accuracy: 0.9968 - precision: 0.9957 - recall: 0.9915 - auc: 0.9998 - val_loss: 0.0421 - val_tp: 102971.0000 - val_fp: 1169.0000 - val_tn: 311626.0000 - val_fn: 1294.0000 - val_accuracy: 0.9941 - val_precision: 0.9888 - val_recall: 0.9876 - val_auc: 0.9990\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0171 - tp: 127442.0000 - fp: 446.0000 - tn: 384478.0000 - fn: 866.0000 - accuracy: 0.9974 - precision: 0.9965 - recall: 0.9933 - auc: 0.9999 - val_loss: 0.0427 - val_tp: 102972.0000 - val_fp: 1180.0000 - val_tn: 311615.0000 - val_fn: 1293.0000 - val_accuracy: 0.9941 - val_precision: 0.9887 - val_recall: 0.9876 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 412ms/step - loss: 0.0148 - tp: 127591.0000 - fp: 403.0000 - tn: 384521.0000 - fn: 717.0000 - accuracy: 0.9978 - precision: 0.9969 - recall: 0.9944 - auc: 0.9999 - val_loss: 0.0451 - val_tp: 102968.0000 - val_fp: 1182.0000 - val_tn: 311613.0000 - val_fn: 1297.0000 - val_accuracy: 0.9941 - val_precision: 0.9887 - val_recall: 0.9876 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0128 - tp: 127724.0000 - fp: 359.0000 - tn: 384565.0000 - fn: 584.0000 - accuracy: 0.9982 - precision: 0.9972 - recall: 0.9954 - auc: 0.9999 - val_loss: 0.0474 - val_tp: 103019.0000 - val_fp: 1162.0000 - val_tn: 311633.0000 - val_fn: 1246.0000 - val_accuracy: 0.9942 - val_precision: 0.9888 - val_recall: 0.9880 - val_auc: 0.9987\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 0.0110 - tp: 127807.0000 - fp: 312.0000 - tn: 384612.0000 - fn: 501.0000 - accuracy: 0.9984 - precision: 0.9976 - recall: 0.9961 - auc: 0.9999 - val_loss: 0.0468 - val_tp: 102989.0000 - val_fp: 1178.0000 - val_tn: 311617.0000 - val_fn: 1276.0000 - val_accuracy: 0.9941 - val_precision: 0.9887 - val_recall: 0.9878 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0098 - tp: 127861.0000 - fp: 291.0000 - tn: 384633.0000 - fn: 447.0000 - accuracy: 0.9986 - precision: 0.9977 - recall: 0.9965 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 412ms/step - loss: 0.0098 - tp: 127861.0000 - fp: 291.0000 - tn: 384633.0000 - fn: 447.0000 - accuracy: 0.9986 - precision: 0.9977 - recall: 0.9965 - auc: 1.0000 - val_loss: 0.0508 - val_tp: 103027.0000 - val_fp: 1168.0000 - val_tn: 311627.0000 - val_fn: 1238.0000 - val_accuracy: 0.9942 - val_precision: 0.9888 - val_recall: 0.9881 - val_auc: 0.9985\n",
            "Epoch 00015: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEjpH41XoL1G",
        "outputId": "adc0c12f-08b1-4831-de9c-d60528b8684a"
      },
      "source": [
        "# now try the weighted one-hot encoding\n",
        "downweight_models = {}\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9]:\n",
        "    train_weights_onehot = down_weight(train_labs_downsampled_onehot, weight)\n",
        "    y = np.array(train_weights_onehot)\n",
        "    print('Down-weighting:', weight)\n",
        "    print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):', np.shape(y))\n",
        "\n",
        "    downweight_model = make_model(output_bias=initial_bias)\n",
        "    downweight_model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(dev_X, dev_y))\n",
        "\n",
        "    downweight_models[weight] = downweight_model\n",
        "    downweight_model.save(f'BiLSTM_downsampled/BiLSTM_downsampled_downweight_{weight}.h5')\n",
        "    print(f'Model saved at BiLSTM_downsampled/BiLSTM_downsampled_downweight_{weight}.h5')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Down-weighting: 0.1\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 26s 464ms/step - loss: 0.0854 - tp: 193831.0000 - fp: 5735.0000 - tn: 691984.0000 - fn: 38742.0000 - accuracy: 0.8546 - precision: 0.9713 - recall: 0.8334 - auc: 0.9901 - val_loss: 0.1978 - val_tp: 87224.0000 - val_fp: 15.0000 - val_tn: 312780.0000 - val_fn: 17041.0000 - val_accuracy: 0.9591 - val_precision: 0.9998 - val_recall: 0.8366 - val_auc: 0.9970\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 410ms/step - loss: 0.0499 - tp: 105065.0000 - fp: 915.0000 - tn: 384009.0000 - fn: 23243.0000 - accuracy: 0.7483 - precision: 0.9914 - recall: 0.8188 - auc: 0.9939 - val_loss: 0.1464 - val_tp: 89527.0000 - val_fp: 31.0000 - val_tn: 312764.0000 - val_fn: 14738.0000 - val_accuracy: 0.9646 - val_precision: 0.9997 - val_recall: 0.8586 - val_auc: 0.9978\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0448 - tp: 108957.0000 - fp: 1026.0000 - tn: 383898.0000 - fn: 19351.0000 - accuracy: 0.7482 - precision: 0.9907 - recall: 0.8492 - auc: 0.9952 - val_loss: 0.1250 - val_tp: 94060.0000 - val_fp: 52.0000 - val_tn: 312743.0000 - val_fn: 10205.0000 - val_accuracy: 0.9754 - val_precision: 0.9994 - val_recall: 0.9021 - val_auc: 0.9989\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 424ms/step - loss: 0.0387 - tp: 115052.0000 - fp: 1188.0000 - tn: 383736.0000 - fn: 13256.0000 - accuracy: 0.7484 - precision: 0.9898 - recall: 0.8967 - auc: 0.9972 - val_loss: 0.0909 - val_tp: 98625.0000 - val_fp: 221.0000 - val_tn: 312574.0000 - val_fn: 5640.0000 - val_accuracy: 0.9859 - val_precision: 0.9978 - val_recall: 0.9459 - val_auc: 0.9992\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0294 - tp: 120912.0000 - fp: 1677.0000 - tn: 383247.0000 - fn: 7396.0000 - accuracy: 0.7494 - precision: 0.9863 - recall: 0.9424 - auc: 0.9985 - val_loss: 0.0658 - val_tp: 100330.0000 - val_fp: 908.0000 - val_tn: 311887.0000 - val_fn: 3935.0000 - val_accuracy: 0.9884 - val_precision: 0.9910 - val_recall: 0.9623 - val_auc: 0.9994\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 419ms/step - loss: 0.0187 - tp: 124722.0000 - fp: 1539.0000 - tn: 383385.0000 - fn: 3586.0000 - accuracy: 0.7515 - precision: 0.9878 - recall: 0.9721 - auc: 0.9994 - val_loss: 0.0511 - val_tp: 101375.0000 - val_fp: 1274.0000 - val_tn: 311521.0000 - val_fn: 2890.0000 - val_accuracy: 0.9900 - val_precision: 0.9876 - val_recall: 0.9723 - val_auc: 0.9995\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0112 - tp: 126496.0000 - fp: 1096.0000 - tn: 383828.0000 - fn: 1812.0000 - accuracy: 0.7532 - precision: 0.9914 - recall: 0.9859 - auc: 0.9998 - val_loss: 0.0468 - val_tp: 101773.0000 - val_fp: 1438.0000 - val_tn: 311357.0000 - val_fn: 2492.0000 - val_accuracy: 0.9906 - val_precision: 0.9861 - val_recall: 0.9761 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0069 - tp: 127181.0000 - fp: 794.0000 - tn: 384130.0000 - fn: 1127.0000 - accuracy: 0.7542 - precision: 0.9938 - recall: 0.9912 - auc: 0.9998 - val_loss: 0.0443 - val_tp: 102059.0000 - val_fp: 1506.0000 - val_tn: 311289.0000 - val_fn: 2206.0000 - val_accuracy: 0.9911 - val_precision: 0.9855 - val_recall: 0.9788 - val_auc: 0.9994\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0046 - tp: 127538.0000 - fp: 584.0000 - tn: 384340.0000 - fn: 770.0000 - accuracy: 0.7548 - precision: 0.9954 - recall: 0.9940 - auc: 0.9999 - val_loss: 0.0419 - val_tp: 102416.0000 - val_fp: 1391.0000 - val_tn: 311404.0000 - val_fn: 1849.0000 - val_accuracy: 0.9922 - val_precision: 0.9866 - val_recall: 0.9823 - val_auc: 0.9993\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0035 - tp: 127702.0000 - fp: 476.0000 - tn: 384448.0000 - fn: 606.0000 - accuracy: 0.7551 - precision: 0.9963 - recall: 0.9953 - auc: 0.9999 - val_loss: 0.0421 - val_tp: 102550.0000 - val_fp: 1396.0000 - val_tn: 311399.0000 - val_fn: 1715.0000 - val_accuracy: 0.9925 - val_precision: 0.9866 - val_recall: 0.9836 - val_auc: 0.9991\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0029 - tp: 127785.0000 - fp: 435.0000 - tn: 384489.0000 - fn: 523.0000 - accuracy: 0.7552 - precision: 0.9966 - recall: 0.9959 - auc: 0.9999 - val_loss: 0.0412 - val_tp: 102627.0000 - val_fp: 1335.0000 - val_tn: 311460.0000 - val_fn: 1638.0000 - val_accuracy: 0.9929 - val_precision: 0.9872 - val_recall: 0.9843 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 420ms/step - loss: 0.0023 - tp: 127885.0000 - fp: 354.0000 - tn: 384570.0000 - fn: 423.0000 - accuracy: 0.7554 - precision: 0.9972 - recall: 0.9967 - auc: 0.9999 - val_loss: 0.0427 - val_tp: 102693.0000 - val_fp: 1332.0000 - val_tn: 311463.0000 - val_fn: 1572.0000 - val_accuracy: 0.9930 - val_precision: 0.9872 - val_recall: 0.9849 - val_auc: 0.9988\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0019 - tp: 127960.0000 - fp: 300.0000 - tn: 384624.0000 - fn: 348.0000 - accuracy: 0.7555 - precision: 0.9977 - recall: 0.9973 - auc: 0.9999 - val_loss: 0.0431 - val_tp: 102737.0000 - val_fp: 1295.0000 - val_tn: 311500.0000 - val_fn: 1528.0000 - val_accuracy: 0.9932 - val_precision: 0.9876 - val_recall: 0.9853 - val_auc: 0.9988\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 414ms/step - loss: 0.0016 - tp: 128032.0000 - fp: 246.0000 - tn: 384678.0000 - fn: 276.0000 - accuracy: 0.7556 - precision: 0.9981 - recall: 0.9978 - auc: 0.9999 - val_loss: 0.0443 - val_tp: 102668.0000 - val_fp: 1400.0000 - val_tn: 311395.0000 - val_fn: 1597.0000 - val_accuracy: 0.9928 - val_precision: 0.9865 - val_recall: 0.9847 - val_auc: 0.9987\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - 16s 419ms/step - loss: 0.0014 - tp: 128017.0000 - fp: 255.0000 - tn: 384669.0000 - fn: 291.0000 - accuracy: 0.7556 - precision: 0.9980 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0457 - val_tp: 102886.0000 - val_fp: 1242.0000 - val_tn: 311553.0000 - val_fn: 1379.0000 - val_accuracy: 0.9937 - val_precision: 0.9881 - val_recall: 0.9868 - val_auc: 0.9985\n",
            "Epoch 16/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0013 - tp: 128076.0000 - fp: 209.0000 - tn: 384715.0000 - fn: 232.0000 - accuracy: 0.7557 - precision: 0.9984 - recall: 0.9982 - auc: 0.9999Restoring model weights from the end of the best epoch: 6.\n",
            "39/39 [==============================] - 16s 415ms/step - loss: 0.0013 - tp: 128076.0000 - fp: 209.0000 - tn: 384715.0000 - fn: 232.0000 - accuracy: 0.7557 - precision: 0.9984 - recall: 0.9982 - auc: 0.9999 - val_loss: 0.0456 - val_tp: 102756.0000 - val_fp: 1344.0000 - val_tn: 311451.0000 - val_fn: 1509.0000 - val_accuracy: 0.9932 - val_precision: 0.9871 - val_recall: 0.9855 - val_auc: 0.9986\n",
            "Epoch 00016: early stopping\n",
            "Model saved at BiLSTM_downsampled/BiLSTM_downsampled_downweight_0.1.h5\n",
            "\n",
            "Down-weighting: 0.2\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 26s 466ms/step - loss: 0.1287 - tp: 194063.0000 - fp: 7761.0000 - tn: 689958.0000 - fn: 38510.0000 - accuracy: 0.8521 - precision: 0.9615 - recall: 0.8344 - auc: 0.9905 - val_loss: 0.1713 - val_tp: 89547.0000 - val_fp: 105.0000 - val_tn: 312690.0000 - val_fn: 14718.0000 - val_accuracy: 0.9645 - val_precision: 0.9988 - val_recall: 0.8588 - val_auc: 0.9982\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 415ms/step - loss: 0.0659 - tp: 116894.0000 - fp: 1862.0000 - tn: 383062.0000 - fn: 11414.0000 - accuracy: 0.7464 - precision: 0.9843 - recall: 0.9110 - auc: 0.9973 - val_loss: 0.1066 - val_tp: 100332.0000 - val_fp: 786.0000 - val_tn: 312009.0000 - val_fn: 3933.0000 - val_accuracy: 0.9887 - val_precision: 0.9922 - val_recall: 0.9623 - val_auc: 0.9992\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 0.0567 - tp: 120384.0000 - fp: 1816.0000 - tn: 383108.0000 - fn: 7924.0000 - accuracy: 0.7465 - precision: 0.9851 - recall: 0.9382 - auc: 0.9982 - val_loss: 0.0842 - val_tp: 101851.0000 - val_fp: 900.0000 - val_tn: 311895.0000 - val_fn: 2414.0000 - val_accuracy: 0.9921 - val_precision: 0.9912 - val_recall: 0.9768 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 0.0477 - tp: 121812.0000 - fp: 1214.0000 - tn: 383710.0000 - fn: 6496.0000 - accuracy: 0.7478 - precision: 0.9901 - recall: 0.9494 - auc: 0.9989 - val_loss: 0.0629 - val_tp: 101858.0000 - val_fp: 825.0000 - val_tn: 311970.0000 - val_fn: 2407.0000 - val_accuracy: 0.9923 - val_precision: 0.9920 - val_recall: 0.9769 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 413ms/step - loss: 0.0361 - tp: 123476.0000 - fp: 968.0000 - tn: 383956.0000 - fn: 4832.0000 - accuracy: 0.7496 - precision: 0.9922 - recall: 0.9623 - auc: 0.9994 - val_loss: 0.0490 - val_tp: 101946.0000 - val_fp: 798.0000 - val_tn: 311997.0000 - val_fn: 2319.0000 - val_accuracy: 0.9925 - val_precision: 0.9922 - val_recall: 0.9778 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 417ms/step - loss: 0.0239 - tp: 125635.0000 - fp: 923.0000 - tn: 384001.0000 - fn: 2673.0000 - accuracy: 0.7519 - precision: 0.9927 - recall: 0.9792 - auc: 0.9997 - val_loss: 0.0423 - val_tp: 102172.0000 - val_fp: 1007.0000 - val_tn: 311788.0000 - val_fn: 2093.0000 - val_accuracy: 0.9926 - val_precision: 0.9902 - val_recall: 0.9799 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 415ms/step - loss: 0.0156 - tp: 126815.0000 - fp: 773.0000 - tn: 384151.0000 - fn: 1493.0000 - accuracy: 0.7533 - precision: 0.9939 - recall: 0.9884 - auc: 0.9998 - val_loss: 0.0399 - val_tp: 102422.0000 - val_fp: 1133.0000 - val_tn: 311662.0000 - val_fn: 1843.0000 - val_accuracy: 0.9929 - val_precision: 0.9891 - val_recall: 0.9823 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 415ms/step - loss: 0.0101 - tp: 127366.0000 - fp: 596.0000 - tn: 384328.0000 - fn: 942.0000 - accuracy: 0.7543 - precision: 0.9953 - recall: 0.9927 - auc: 0.9999 - val_loss: 0.0390 - val_tp: 102678.0000 - val_fp: 1185.0000 - val_tn: 311610.0000 - val_fn: 1587.0000 - val_accuracy: 0.9934 - val_precision: 0.9886 - val_recall: 0.9848 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 419ms/step - loss: 0.0072 - tp: 127671.0000 - fp: 469.0000 - tn: 384455.0000 - fn: 637.0000 - accuracy: 0.7548 - precision: 0.9963 - recall: 0.9950 - auc: 0.9999 - val_loss: 0.0395 - val_tp: 102785.0000 - val_fp: 1186.0000 - val_tn: 311609.0000 - val_fn: 1480.0000 - val_accuracy: 0.9936 - val_precision: 0.9886 - val_recall: 0.9858 - val_auc: 0.9991\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0055 - tp: 127838.0000 - fp: 345.0000 - tn: 384579.0000 - fn: 470.0000 - accuracy: 0.7552 - precision: 0.9973 - recall: 0.9963 - auc: 0.9999 - val_loss: 0.0399 - val_tp: 102796.0000 - val_fp: 1216.0000 - val_tn: 311579.0000 - val_fn: 1469.0000 - val_accuracy: 0.9936 - val_precision: 0.9883 - val_recall: 0.9859 - val_auc: 0.9990\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 412ms/step - loss: 0.0044 - tp: 127898.0000 - fp: 318.0000 - tn: 384606.0000 - fn: 410.0000 - accuracy: 0.7553 - precision: 0.9975 - recall: 0.9968 - auc: 1.0000 - val_loss: 0.0408 - val_tp: 102811.0000 - val_fp: 1246.0000 - val_tn: 311549.0000 - val_fn: 1454.0000 - val_accuracy: 0.9935 - val_precision: 0.9880 - val_recall: 0.9861 - val_auc: 0.9989\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0035 - tp: 127984.0000 - fp: 256.0000 - tn: 384668.0000 - fn: 324.0000 - accuracy: 0.7555 - precision: 0.9980 - recall: 0.9975 - auc: 1.0000 - val_loss: 0.0425 - val_tp: 102915.0000 - val_fp: 1196.0000 - val_tn: 311599.0000 - val_fn: 1350.0000 - val_accuracy: 0.9939 - val_precision: 0.9885 - val_recall: 0.9871 - val_auc: 0.9987\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0029 - tp: 128053.0000 - fp: 208.0000 - tn: 384716.0000 - fn: 255.0000 - accuracy: 0.7556 - precision: 0.9984 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0441 - val_tp: 102968.0000 - val_fp: 1183.0000 - val_tn: 311612.0000 - val_fn: 1297.0000 - val_accuracy: 0.9941 - val_precision: 0.9886 - val_recall: 0.9876 - val_auc: 0.9986\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 415ms/step - loss: 0.0023 - tp: 128111.0000 - fp: 164.0000 - tn: 384760.0000 - fn: 197.0000 - accuracy: 0.7557 - precision: 0.9987 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0435 - val_tp: 102821.0000 - val_fp: 1267.0000 - val_tn: 311528.0000 - val_fn: 1444.0000 - val_accuracy: 0.9935 - val_precision: 0.9878 - val_recall: 0.9862 - val_auc: 0.9987\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0021 - tp: 128107.0000 - fp: 171.0000 - tn: 384753.0000 - fn: 201.0000 - accuracy: 0.7557 - precision: 0.9987 - recall: 0.9984 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0021 - tp: 128107.0000 - fp: 171.0000 - tn: 384753.0000 - fn: 201.0000 - accuracy: 0.7557 - precision: 0.9987 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0463 - val_tp: 102962.0000 - val_fp: 1205.0000 - val_tn: 311590.0000 - val_fn: 1303.0000 - val_accuracy: 0.9940 - val_precision: 0.9884 - val_recall: 0.9875 - val_auc: 0.9984\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_downsampled/BiLSTM_downsampled_downweight_0.2.h5\n",
            "\n",
            "Down-weighting: 0.3\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 33s 534ms/step - loss: 0.1699 - tp: 194925.0000 - fp: 8539.0000 - tn: 689180.0000 - fn: 37648.0000 - accuracy: 0.8515 - precision: 0.9580 - recall: 0.8381 - auc: 0.9904 - val_loss: 0.1537 - val_tp: 93623.0000 - val_fp: 738.0000 - val_tn: 312057.0000 - val_fn: 10642.0000 - val_accuracy: 0.9727 - val_precision: 0.9922 - val_recall: 0.8979 - val_auc: 0.9981\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0779 - tp: 121294.0000 - fp: 2759.0000 - tn: 382165.0000 - fn: 7014.0000 - accuracy: 0.7446 - precision: 0.9778 - recall: 0.9453 - auc: 0.9972 - val_loss: 0.0897 - val_tp: 102498.0000 - val_fp: 1155.0000 - val_tn: 311640.0000 - val_fn: 1767.0000 - val_accuracy: 0.9930 - val_precision: 0.9889 - val_recall: 0.9831 - val_auc: 0.9992\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0655 - tp: 123392.0000 - fp: 2626.0000 - tn: 382298.0000 - fn: 4916.0000 - accuracy: 0.7449 - precision: 0.9792 - recall: 0.9617 - auc: 0.9982 - val_loss: 0.0712 - val_tp: 102802.0000 - val_fp: 1194.0000 - val_tn: 311601.0000 - val_fn: 1463.0000 - val_accuracy: 0.9936 - val_precision: 0.9885 - val_recall: 0.9860 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 422ms/step - loss: 0.0553 - tp: 123637.0000 - fp: 1871.0000 - tn: 383053.0000 - fn: 4671.0000 - accuracy: 0.7464 - precision: 0.9851 - recall: 0.9636 - auc: 0.9990 - val_loss: 0.0560 - val_tp: 102605.0000 - val_fp: 1079.0000 - val_tn: 311716.0000 - val_fn: 1660.0000 - val_accuracy: 0.9934 - val_precision: 0.9896 - val_recall: 0.9841 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0424 - tp: 124106.0000 - fp: 1041.0000 - tn: 383883.0000 - fn: 4202.0000 - accuracy: 0.7487 - precision: 0.9917 - recall: 0.9673 - auc: 0.9994 - val_loss: 0.0456 - val_tp: 102467.0000 - val_fp: 968.0000 - val_tn: 311827.0000 - val_fn: 1798.0000 - val_accuracy: 0.9934 - val_precision: 0.9906 - val_recall: 0.9828 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 17s 424ms/step - loss: 0.0289 - tp: 125583.0000 - fp: 795.0000 - tn: 384129.0000 - fn: 2725.0000 - accuracy: 0.7512 - precision: 0.9937 - recall: 0.9788 - auc: 0.9997 - val_loss: 0.0403 - val_tp: 102480.0000 - val_fp: 994.0000 - val_tn: 311801.0000 - val_fn: 1785.0000 - val_accuracy: 0.9933 - val_precision: 0.9904 - val_recall: 0.9829 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 17s 431ms/step - loss: 0.0196 - tp: 126727.0000 - fp: 681.0000 - tn: 384243.0000 - fn: 1581.0000 - accuracy: 0.7530 - precision: 0.9947 - recall: 0.9877 - auc: 0.9999 - val_loss: 0.0385 - val_tp: 102607.0000 - val_fp: 1057.0000 - val_tn: 311738.0000 - val_fn: 1658.0000 - val_accuracy: 0.9935 - val_precision: 0.9898 - val_recall: 0.9841 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0132 - tp: 127324.0000 - fp: 558.0000 - tn: 384366.0000 - fn: 984.0000 - accuracy: 0.7540 - precision: 0.9956 - recall: 0.9923 - auc: 0.9999 - val_loss: 0.0381 - val_tp: 102885.0000 - val_fp: 1120.0000 - val_tn: 311675.0000 - val_fn: 1380.0000 - val_accuracy: 0.9940 - val_precision: 0.9892 - val_recall: 0.9868 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 422ms/step - loss: 0.0095 - tp: 127622.0000 - fp: 442.0000 - tn: 384482.0000 - fn: 686.0000 - accuracy: 0.7546 - precision: 0.9965 - recall: 0.9947 - auc: 0.9999 - val_loss: 0.0386 - val_tp: 102873.0000 - val_fp: 1130.0000 - val_tn: 311665.0000 - val_fn: 1392.0000 - val_accuracy: 0.9940 - val_precision: 0.9891 - val_recall: 0.9866 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0073 - tp: 127813.0000 - fp: 328.0000 - tn: 384596.0000 - fn: 495.0000 - accuracy: 0.7551 - precision: 0.9974 - recall: 0.9961 - auc: 1.0000 - val_loss: 0.0394 - val_tp: 102884.0000 - val_fp: 1169.0000 - val_tn: 311626.0000 - val_fn: 1381.0000 - val_accuracy: 0.9939 - val_precision: 0.9888 - val_recall: 0.9868 - val_auc: 0.9990\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 434ms/step - loss: 0.0059 - tp: 127873.0000 - fp: 305.0000 - tn: 384619.0000 - fn: 435.0000 - accuracy: 0.7552 - precision: 0.9976 - recall: 0.9966 - auc: 1.0000 - val_loss: 0.0401 - val_tp: 102885.0000 - val_fp: 1187.0000 - val_tn: 311608.0000 - val_fn: 1380.0000 - val_accuracy: 0.9938 - val_precision: 0.9886 - val_recall: 0.9868 - val_auc: 0.9989\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 435ms/step - loss: 0.0048 - tp: 127995.0000 - fp: 243.0000 - tn: 384681.0000 - fn: 313.0000 - accuracy: 0.7554 - precision: 0.9981 - recall: 0.9976 - auc: 1.0000 - val_loss: 0.0422 - val_tp: 102986.0000 - val_fp: 1172.0000 - val_tn: 311623.0000 - val_fn: 1279.0000 - val_accuracy: 0.9941 - val_precision: 0.9887 - val_recall: 0.9877 - val_auc: 0.9987\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 434ms/step - loss: 0.0039 - tp: 128064.0000 - fp: 192.0000 - tn: 384732.0000 - fn: 244.0000 - accuracy: 0.7556 - precision: 0.9985 - recall: 0.9981 - auc: 1.0000 - val_loss: 0.0442 - val_tp: 103004.0000 - val_fp: 1154.0000 - val_tn: 311641.0000 - val_fn: 1261.0000 - val_accuracy: 0.9942 - val_precision: 0.9889 - val_recall: 0.9879 - val_auc: 0.9986\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0032 - tp: 128112.0000 - fp: 149.0000 - tn: 384775.0000 - fn: 196.0000 - accuracy: 0.7557 - precision: 0.9988 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0430 - val_tp: 102899.0000 - val_fp: 1205.0000 - val_tn: 311590.0000 - val_fn: 1366.0000 - val_accuracy: 0.9938 - val_precision: 0.9884 - val_recall: 0.9869 - val_auc: 0.9987\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0028 - tp: 128140.0000 - fp: 139.0000 - tn: 384785.0000 - fn: 168.0000 - accuracy: 0.7558 - precision: 0.9989 - recall: 0.9987 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0028 - tp: 128140.0000 - fp: 139.0000 - tn: 384785.0000 - fn: 168.0000 - accuracy: 0.7558 - precision: 0.9989 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0466 - val_tp: 103009.0000 - val_fp: 1157.0000 - val_tn: 311638.0000 - val_fn: 1256.0000 - val_accuracy: 0.9942 - val_precision: 0.9889 - val_recall: 0.9880 - val_auc: 0.9984\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_downsampled/BiLSTM_downsampled_downweight_0.3.h5\n",
            "\n",
            "Down-weighting: 0.4\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 28s 505ms/step - loss: 0.2101 - tp: 195723.0000 - fp: 9151.0000 - tn: 688568.0000 - fn: 36850.0000 - accuracy: 0.8509 - precision: 0.9553 - recall: 0.8416 - auc: 0.9903 - val_loss: 0.1391 - val_tp: 97084.0000 - val_fp: 1452.0000 - val_tn: 311343.0000 - val_fn: 7181.0000 - val_accuracy: 0.9793 - val_precision: 0.9853 - val_recall: 0.9311 - val_auc: 0.9981\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 433ms/step - loss: 0.0877 - tp: 122951.0000 - fp: 3188.0000 - tn: 381736.0000 - fn: 5357.0000 - accuracy: 0.7438 - precision: 0.9747 - recall: 0.9582 - auc: 0.9970 - val_loss: 0.0801 - val_tp: 102733.0000 - val_fp: 1215.0000 - val_tn: 311580.0000 - val_fn: 1532.0000 - val_accuracy: 0.9934 - val_precision: 0.9883 - val_recall: 0.9853 - val_auc: 0.9992\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 432ms/step - loss: 0.0728 - tp: 124342.0000 - fp: 2979.0000 - tn: 381945.0000 - fn: 3966.0000 - accuracy: 0.7442 - precision: 0.9766 - recall: 0.9691 - auc: 0.9981 - val_loss: 0.0650 - val_tp: 102929.0000 - val_fp: 1226.0000 - val_tn: 311569.0000 - val_fn: 1336.0000 - val_accuracy: 0.9939 - val_precision: 0.9882 - val_recall: 0.9872 - val_auc: 0.9994\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0619 - tp: 124377.0000 - fp: 2381.0000 - tn: 382543.0000 - fn: 3931.0000 - accuracy: 0.7454 - precision: 0.9812 - recall: 0.9694 - auc: 0.9989 - val_loss: 0.0528 - val_tp: 102844.0000 - val_fp: 1192.0000 - val_tn: 311603.0000 - val_fn: 1421.0000 - val_accuracy: 0.9937 - val_precision: 0.9885 - val_recall: 0.9864 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 17s 432ms/step - loss: 0.0485 - tp: 124400.0000 - fp: 1358.0000 - tn: 383566.0000 - fn: 3908.0000 - accuracy: 0.7476 - precision: 0.9892 - recall: 0.9695 - auc: 0.9994 - val_loss: 0.0443 - val_tp: 102709.0000 - val_fp: 1093.0000 - val_tn: 311702.0000 - val_fn: 1556.0000 - val_accuracy: 0.9936 - val_precision: 0.9895 - val_recall: 0.9851 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0341 - tp: 125374.0000 - fp: 803.0000 - tn: 384121.0000 - fn: 2934.0000 - accuracy: 0.7504 - precision: 0.9936 - recall: 0.9771 - auc: 0.9997 - val_loss: 0.0397 - val_tp: 102653.0000 - val_fp: 1024.0000 - val_tn: 311771.0000 - val_fn: 1612.0000 - val_accuracy: 0.9937 - val_precision: 0.9901 - val_recall: 0.9845 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0234 - tp: 126539.0000 - fp: 650.0000 - tn: 384274.0000 - fn: 1769.0000 - accuracy: 0.7525 - precision: 0.9949 - recall: 0.9862 - auc: 0.9998 - val_loss: 0.0380 - val_tp: 102742.0000 - val_fp: 1057.0000 - val_tn: 311738.0000 - val_fn: 1523.0000 - val_accuracy: 0.9938 - val_precision: 0.9898 - val_recall: 0.9854 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0161 - tp: 127243.0000 - fp: 546.0000 - tn: 384378.0000 - fn: 1065.0000 - accuracy: 0.7537 - precision: 0.9957 - recall: 0.9917 - auc: 0.9999 - val_loss: 0.0379 - val_tp: 102926.0000 - val_fp: 1102.0000 - val_tn: 311693.0000 - val_fn: 1339.0000 - val_accuracy: 0.9941 - val_precision: 0.9894 - val_recall: 0.9872 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0118 - tp: 127563.0000 - fp: 425.0000 - tn: 384499.0000 - fn: 745.0000 - accuracy: 0.7544 - precision: 0.9967 - recall: 0.9942 - auc: 1.0000 - val_loss: 0.0384 - val_tp: 102920.0000 - val_fp: 1103.0000 - val_tn: 311692.0000 - val_fn: 1345.0000 - val_accuracy: 0.9941 - val_precision: 0.9894 - val_recall: 0.9871 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 433ms/step - loss: 0.0092 - tp: 127756.0000 - fp: 363.0000 - tn: 384561.0000 - fn: 552.0000 - accuracy: 0.7548 - precision: 0.9972 - recall: 0.9957 - auc: 0.9999 - val_loss: 0.0395 - val_tp: 102935.0000 - val_fp: 1121.0000 - val_tn: 311674.0000 - val_fn: 1330.0000 - val_accuracy: 0.9941 - val_precision: 0.9892 - val_recall: 0.9872 - val_auc: 0.9990\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0075 - tp: 127851.0000 - fp: 296.0000 - tn: 384628.0000 - fn: 457.0000 - accuracy: 0.7551 - precision: 0.9977 - recall: 0.9964 - auc: 1.0000 - val_loss: 0.0399 - val_tp: 102936.0000 - val_fp: 1156.0000 - val_tn: 311639.0000 - val_fn: 1329.0000 - val_accuracy: 0.9940 - val_precision: 0.9889 - val_recall: 0.9873 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 439ms/step - loss: 0.0060 - tp: 127976.0000 - fp: 244.0000 - tn: 384680.0000 - fn: 332.0000 - accuracy: 0.7553 - precision: 0.9981 - recall: 0.9974 - auc: 1.0000 - val_loss: 0.0415 - val_tp: 102969.0000 - val_fp: 1161.0000 - val_tn: 311634.0000 - val_fn: 1296.0000 - val_accuracy: 0.9941 - val_precision: 0.9889 - val_recall: 0.9876 - val_auc: 0.9988\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 432ms/step - loss: 0.0050 - tp: 128037.0000 - fp: 195.0000 - tn: 384729.0000 - fn: 271.0000 - accuracy: 0.7555 - precision: 0.9985 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0438 - val_tp: 103013.0000 - val_fp: 1146.0000 - val_tn: 311649.0000 - val_fn: 1252.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9880 - val_auc: 0.9987\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0041 - tp: 128098.0000 - fp: 149.0000 - tn: 384775.0000 - fn: 210.0000 - accuracy: 0.7557 - precision: 0.9988 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0430 - val_tp: 102952.0000 - val_fp: 1168.0000 - val_tn: 311627.0000 - val_fn: 1313.0000 - val_accuracy: 0.9941 - val_precision: 0.9888 - val_recall: 0.9874 - val_auc: 0.9987\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0036 - tp: 128120.0000 - fp: 146.0000 - tn: 384778.0000 - fn: 188.0000 - accuracy: 0.7557 - precision: 0.9989 - recall: 0.9985 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0036 - tp: 128120.0000 - fp: 146.0000 - tn: 384778.0000 - fn: 188.0000 - accuracy: 0.7557 - precision: 0.9989 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0468 - val_tp: 103020.0000 - val_fp: 1154.0000 - val_tn: 311641.0000 - val_fn: 1245.0000 - val_accuracy: 0.9942 - val_precision: 0.9889 - val_recall: 0.9881 - val_auc: 0.9984\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_downsampled/BiLSTM_downsampled_downweight_0.4.h5\n",
            "\n",
            "Down-weighting: 0.5\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 485ms/step - loss: 0.2493 - tp: 196534.0000 - fp: 9712.0000 - tn: 688007.0000 - fn: 36039.0000 - accuracy: 0.8503 - precision: 0.9529 - recall: 0.8450 - auc: 0.9902 - val_loss: 0.1272 - val_tp: 99580.0000 - val_fp: 2106.0000 - val_tn: 310689.0000 - val_fn: 4685.0000 - val_accuracy: 0.9837 - val_precision: 0.9793 - val_recall: 0.9551 - val_auc: 0.9981\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0963 - tp: 123719.0000 - fp: 3401.0000 - tn: 381523.0000 - fn: 4589.0000 - accuracy: 0.7434 - precision: 0.9732 - recall: 0.9642 - auc: 0.9968 - val_loss: 0.0742 - val_tp: 102807.0000 - val_fp: 1231.0000 - val_tn: 311564.0000 - val_fn: 1458.0000 - val_accuracy: 0.9936 - val_precision: 0.9882 - val_recall: 0.9860 - val_auc: 0.9992\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0792 - tp: 124649.0000 - fp: 3096.0000 - tn: 381828.0000 - fn: 3659.0000 - accuracy: 0.7440 - precision: 0.9758 - recall: 0.9715 - auc: 0.9980 - val_loss: 0.0617 - val_tp: 102957.0000 - val_fp: 1227.0000 - val_tn: 311568.0000 - val_fn: 1308.0000 - val_accuracy: 0.9939 - val_precision: 0.9882 - val_recall: 0.9875 - val_auc: 0.9994\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 443ms/step - loss: 0.0679 - tp: 124718.0000 - fp: 2726.0000 - tn: 382198.0000 - fn: 3590.0000 - accuracy: 0.7447 - precision: 0.9786 - recall: 0.9720 - auc: 0.9988 - val_loss: 0.0514 - val_tp: 102918.0000 - val_fp: 1227.0000 - val_tn: 311568.0000 - val_fn: 1347.0000 - val_accuracy: 0.9938 - val_precision: 0.9882 - val_recall: 0.9871 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0542 - tp: 124637.0000 - fp: 1761.0000 - tn: 383163.0000 - fn: 3671.0000 - accuracy: 0.7467 - precision: 0.9861 - recall: 0.9714 - auc: 0.9993 - val_loss: 0.0438 - val_tp: 102872.0000 - val_fp: 1172.0000 - val_tn: 311623.0000 - val_fn: 1393.0000 - val_accuracy: 0.9938 - val_precision: 0.9887 - val_recall: 0.9866 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0391 - tp: 125171.0000 - fp: 892.0000 - tn: 384032.0000 - fn: 3137.0000 - accuracy: 0.7495 - precision: 0.9929 - recall: 0.9756 - auc: 0.9997 - val_loss: 0.0395 - val_tp: 102775.0000 - val_fp: 1084.0000 - val_tn: 311711.0000 - val_fn: 1490.0000 - val_accuracy: 0.9938 - val_precision: 0.9896 - val_recall: 0.9857 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0272 - tp: 126306.0000 - fp: 676.0000 - tn: 384248.0000 - fn: 2002.0000 - accuracy: 0.7518 - precision: 0.9947 - recall: 0.9844 - auc: 0.9998 - val_loss: 0.0382 - val_tp: 102784.0000 - val_fp: 1066.0000 - val_tn: 311729.0000 - val_fn: 1481.0000 - val_accuracy: 0.9939 - val_precision: 0.9897 - val_recall: 0.9858 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0192 - tp: 127079.0000 - fp: 543.0000 - tn: 384381.0000 - fn: 1229.0000 - accuracy: 0.7534 - precision: 0.9957 - recall: 0.9904 - auc: 0.9999 - val_loss: 0.0383 - val_tp: 102963.0000 - val_fp: 1112.0000 - val_tn: 311683.0000 - val_fn: 1302.0000 - val_accuracy: 0.9942 - val_precision: 0.9893 - val_recall: 0.9875 - val_auc: 0.9992\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 438ms/step - loss: 0.0143 - tp: 127453.0000 - fp: 454.0000 - tn: 384470.0000 - fn: 855.0000 - accuracy: 0.7541 - precision: 0.9965 - recall: 0.9933 - auc: 0.9999 - val_loss: 0.0391 - val_tp: 102974.0000 - val_fp: 1126.0000 - val_tn: 311669.0000 - val_fn: 1291.0000 - val_accuracy: 0.9942 - val_precision: 0.9892 - val_recall: 0.9876 - val_auc: 0.9991\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0111 - tp: 127691.0000 - fp: 394.0000 - tn: 384530.0000 - fn: 617.0000 - accuracy: 0.7546 - precision: 0.9969 - recall: 0.9952 - auc: 0.9999 - val_loss: 0.0399 - val_tp: 102969.0000 - val_fp: 1130.0000 - val_tn: 311665.0000 - val_fn: 1296.0000 - val_accuracy: 0.9942 - val_precision: 0.9891 - val_recall: 0.9876 - val_auc: 0.9990\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0091 - tp: 127791.0000 - fp: 321.0000 - tn: 384603.0000 - fn: 517.0000 - accuracy: 0.7549 - precision: 0.9975 - recall: 0.9960 - auc: 1.0000 - val_loss: 0.0405 - val_tp: 102998.0000 - val_fp: 1134.0000 - val_tn: 311661.0000 - val_fn: 1267.0000 - val_accuracy: 0.9942 - val_precision: 0.9891 - val_recall: 0.9878 - val_auc: 0.9989\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0074 - tp: 127931.0000 - fp: 250.0000 - tn: 384674.0000 - fn: 377.0000 - accuracy: 0.7552 - precision: 0.9980 - recall: 0.9971 - auc: 1.0000 - val_loss: 0.0417 - val_tp: 102982.0000 - val_fp: 1148.0000 - val_tn: 311647.0000 - val_fn: 1283.0000 - val_accuracy: 0.9942 - val_precision: 0.9890 - val_recall: 0.9877 - val_auc: 0.9988\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0063 - tp: 127995.0000 - fp: 210.0000 - tn: 384714.0000 - fn: 313.0000 - accuracy: 0.7554 - precision: 0.9984 - recall: 0.9976 - auc: 1.0000 - val_loss: 0.0446 - val_tp: 103028.0000 - val_fp: 1147.0000 - val_tn: 311648.0000 - val_fn: 1237.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9881 - val_auc: 0.9987\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 435ms/step - loss: 0.0052 - tp: 128058.0000 - fp: 174.0000 - tn: 384750.0000 - fn: 250.0000 - accuracy: 0.7555 - precision: 0.9986 - recall: 0.9981 - auc: 1.0000 - val_loss: 0.0434 - val_tp: 103008.0000 - val_fp: 1151.0000 - val_tn: 311644.0000 - val_fn: 1257.0000 - val_accuracy: 0.9942 - val_precision: 0.9889 - val_recall: 0.9879 - val_auc: 0.9987\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0044 - tp: 128087.0000 - fp: 158.0000 - tn: 384766.0000 - fn: 221.0000 - accuracy: 0.7556 - precision: 0.9988 - recall: 0.9983 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0044 - tp: 128087.0000 - fp: 158.0000 - tn: 384766.0000 - fn: 221.0000 - accuracy: 0.7556 - precision: 0.9988 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0470 - val_tp: 103042.0000 - val_fp: 1149.0000 - val_tn: 311646.0000 - val_fn: 1223.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9883 - val_auc: 0.9985\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_downsampled/BiLSTM_downsampled_downweight_0.5.h5\n",
            "\n",
            "Down-weighting: 0.6\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 28s 510ms/step - loss: 0.2879 - tp: 197169.0000 - fp: 10174.0000 - tn: 687545.0000 - fn: 35404.0000 - accuracy: 0.8498 - precision: 0.9509 - recall: 0.8478 - auc: 0.9901 - val_loss: 0.1199 - val_tp: 100656.0000 - val_fp: 2387.0000 - val_tn: 310408.0000 - val_fn: 3609.0000 - val_accuracy: 0.9856 - val_precision: 0.9768 - val_recall: 0.9654 - val_auc: 0.9981\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 439ms/step - loss: 0.1041 - tp: 124027.0000 - fp: 3484.0000 - tn: 381440.0000 - fn: 4281.0000 - accuracy: 0.7432 - precision: 0.9727 - recall: 0.9666 - auc: 0.9966 - val_loss: 0.0701 - val_tp: 102856.0000 - val_fp: 1241.0000 - val_tn: 311554.0000 - val_fn: 1409.0000 - val_accuracy: 0.9936 - val_precision: 0.9881 - val_recall: 0.9865 - val_auc: 0.9991\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0850 - tp: 124801.0000 - fp: 3151.0000 - tn: 381773.0000 - fn: 3507.0000 - accuracy: 0.7439 - precision: 0.9754 - recall: 0.9727 - auc: 0.9979 - val_loss: 0.0595 - val_tp: 102981.0000 - val_fp: 1230.0000 - val_tn: 311565.0000 - val_fn: 1284.0000 - val_accuracy: 0.9940 - val_precision: 0.9882 - val_recall: 0.9877 - val_auc: 0.9994\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 437ms/step - loss: 0.0733 - tp: 124902.0000 - fp: 2925.0000 - tn: 381999.0000 - fn: 3406.0000 - accuracy: 0.7443 - precision: 0.9771 - recall: 0.9735 - auc: 0.9987 - val_loss: 0.0509 - val_tp: 102954.0000 - val_fp: 1238.0000 - val_tn: 311557.0000 - val_fn: 1311.0000 - val_accuracy: 0.9939 - val_precision: 0.9881 - val_recall: 0.9874 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0594 - tp: 124800.0000 - fp: 2126.0000 - tn: 382798.0000 - fn: 3508.0000 - accuracy: 0.7459 - precision: 0.9833 - recall: 0.9727 - auc: 0.9993 - val_loss: 0.0436 - val_tp: 102926.0000 - val_fp: 1204.0000 - val_tn: 311591.0000 - val_fn: 1339.0000 - val_accuracy: 0.9939 - val_precision: 0.9884 - val_recall: 0.9872 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 422ms/step - loss: 0.0438 - tp: 125072.0000 - fp: 1077.0000 - tn: 383847.0000 - fn: 3236.0000 - accuracy: 0.7487 - precision: 0.9915 - recall: 0.9748 - auc: 0.9996 - val_loss: 0.0394 - val_tp: 102859.0000 - val_fp: 1136.0000 - val_tn: 311659.0000 - val_fn: 1406.0000 - val_accuracy: 0.9939 - val_precision: 0.9891 - val_recall: 0.9865 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0308 - tp: 126132.0000 - fp: 694.0000 - tn: 384230.0000 - fn: 2176.0000 - accuracy: 0.7513 - precision: 0.9945 - recall: 0.9830 - auc: 0.9998 - val_loss: 0.0382 - val_tp: 102867.0000 - val_fp: 1103.0000 - val_tn: 311692.0000 - val_fn: 1398.0000 - val_accuracy: 0.9940 - val_precision: 0.9894 - val_recall: 0.9866 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 432ms/step - loss: 0.0221 - tp: 126916.0000 - fp: 549.0000 - tn: 384375.0000 - fn: 1392.0000 - accuracy: 0.7530 - precision: 0.9957 - recall: 0.9892 - auc: 0.9998 - val_loss: 0.0385 - val_tp: 102949.0000 - val_fp: 1118.0000 - val_tn: 311677.0000 - val_fn: 1316.0000 - val_accuracy: 0.9942 - val_precision: 0.9893 - val_recall: 0.9874 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 432ms/step - loss: 0.0168 - tp: 127335.0000 - fp: 478.0000 - tn: 384446.0000 - fn: 973.0000 - accuracy: 0.7538 - precision: 0.9963 - recall: 0.9924 - auc: 0.9999 - val_loss: 0.0393 - val_tp: 102966.0000 - val_fp: 1125.0000 - val_tn: 311670.0000 - val_fn: 1299.0000 - val_accuracy: 0.9942 - val_precision: 0.9892 - val_recall: 0.9875 - val_auc: 0.9991\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 434ms/step - loss: 0.0133 - tp: 127593.0000 - fp: 415.0000 - tn: 384509.0000 - fn: 715.0000 - accuracy: 0.7543 - precision: 0.9968 - recall: 0.9944 - auc: 0.9999 - val_loss: 0.0403 - val_tp: 102976.0000 - val_fp: 1128.0000 - val_tn: 311667.0000 - val_fn: 1289.0000 - val_accuracy: 0.9942 - val_precision: 0.9892 - val_recall: 0.9876 - val_auc: 0.9990\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 424ms/step - loss: 0.0109 - tp: 127721.0000 - fp: 354.0000 - tn: 384570.0000 - fn: 587.0000 - accuracy: 0.7547 - precision: 0.9972 - recall: 0.9954 - auc: 1.0000 - val_loss: 0.0411 - val_tp: 103007.0000 - val_fp: 1139.0000 - val_tn: 311656.0000 - val_fn: 1258.0000 - val_accuracy: 0.9943 - val_precision: 0.9891 - val_recall: 0.9879 - val_auc: 0.9989\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 432ms/step - loss: 0.0090 - tp: 127844.0000 - fp: 295.0000 - tn: 384629.0000 - fn: 464.0000 - accuracy: 0.7550 - precision: 0.9977 - recall: 0.9964 - auc: 1.0000 - val_loss: 0.0419 - val_tp: 103008.0000 - val_fp: 1148.0000 - val_tn: 311647.0000 - val_fn: 1257.0000 - val_accuracy: 0.9942 - val_precision: 0.9890 - val_recall: 0.9879 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 431ms/step - loss: 0.0078 - tp: 127921.0000 - fp: 249.0000 - tn: 384675.0000 - fn: 387.0000 - accuracy: 0.7552 - precision: 0.9981 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0445 - val_tp: 103035.0000 - val_fp: 1143.0000 - val_tn: 311652.0000 - val_fn: 1230.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9882 - val_auc: 0.9987\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0063 - tp: 128009.0000 - fp: 204.0000 - tn: 384720.0000 - fn: 299.0000 - accuracy: 0.7554 - precision: 0.9984 - recall: 0.9977 - auc: 1.0000 - val_loss: 0.0432 - val_tp: 103016.0000 - val_fp: 1160.0000 - val_tn: 311635.0000 - val_fn: 1249.0000 - val_accuracy: 0.9942 - val_precision: 0.9889 - val_recall: 0.9880 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0055 - tp: 128038.0000 - fp: 183.0000 - tn: 384741.0000 - fn: 270.0000 - accuracy: 0.7555 - precision: 0.9986 - recall: 0.9979 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0055 - tp: 128038.0000 - fp: 183.0000 - tn: 384741.0000 - fn: 270.0000 - accuracy: 0.7555 - precision: 0.9986 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0476 - val_tp: 103050.0000 - val_fp: 1151.0000 - val_tn: 311644.0000 - val_fn: 1215.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9883 - val_auc: 0.9985\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_downsampled/BiLSTM_downsampled_downweight_0.6.h5\n",
            "\n",
            "Down-weighting: 0.7\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 476ms/step - loss: 0.3257 - tp: 197729.0000 - fp: 10401.0000 - tn: 687318.0000 - fn: 34844.0000 - accuracy: 0.8496 - precision: 0.9500 - recall: 0.8502 - auc: 0.9901 - val_loss: 0.1153 - val_tp: 101051.0000 - val_fp: 2533.0000 - val_tn: 310262.0000 - val_fn: 3214.0000 - val_accuracy: 0.9862 - val_precision: 0.9755 - val_recall: 0.9692 - val_auc: 0.9981\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.1116 - tp: 124164.0000 - fp: 3542.0000 - tn: 381382.0000 - fn: 4144.0000 - accuracy: 0.7431 - precision: 0.9723 - recall: 0.9677 - auc: 0.9965 - val_loss: 0.0674 - val_tp: 102892.0000 - val_fp: 1247.0000 - val_tn: 311548.0000 - val_fn: 1373.0000 - val_accuracy: 0.9937 - val_precision: 0.9880 - val_recall: 0.9868 - val_auc: 0.9991\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 433ms/step - loss: 0.0902 - tp: 124872.0000 - fp: 3174.0000 - tn: 381750.0000 - fn: 3436.0000 - accuracy: 0.7438 - precision: 0.9752 - recall: 0.9732 - auc: 0.9978 - val_loss: 0.0580 - val_tp: 102993.0000 - val_fp: 1233.0000 - val_tn: 311562.0000 - val_fn: 1272.0000 - val_accuracy: 0.9940 - val_precision: 0.9882 - val_recall: 0.9878 - val_auc: 0.9994\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0782 - tp: 124991.0000 - fp: 3029.0000 - tn: 381895.0000 - fn: 3317.0000 - accuracy: 0.7441 - precision: 0.9763 - recall: 0.9741 - auc: 0.9986 - val_loss: 0.0507 - val_tp: 102979.0000 - val_fp: 1240.0000 - val_tn: 311555.0000 - val_fn: 1286.0000 - val_accuracy: 0.9939 - val_precision: 0.9881 - val_recall: 0.9877 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 17s 439ms/step - loss: 0.0641 - tp: 124926.0000 - fp: 2401.0000 - tn: 382523.0000 - fn: 3382.0000 - accuracy: 0.7453 - precision: 0.9811 - recall: 0.9736 - auc: 0.9992 - val_loss: 0.0439 - val_tp: 102952.0000 - val_fp: 1229.0000 - val_tn: 311566.0000 - val_fn: 1313.0000 - val_accuracy: 0.9939 - val_precision: 0.9882 - val_recall: 0.9874 - val_auc: 0.9996\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 17s 436ms/step - loss: 0.0481 - tp: 125033.0000 - fp: 1339.0000 - tn: 383585.0000 - fn: 3275.0000 - accuracy: 0.7479 - precision: 0.9894 - recall: 0.9745 - auc: 0.9996 - val_loss: 0.0398 - val_tp: 102894.0000 - val_fp: 1157.0000 - val_tn: 311638.0000 - val_fn: 1371.0000 - val_accuracy: 0.9939 - val_precision: 0.9889 - val_recall: 0.9869 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 17s 433ms/step - loss: 0.0342 - tp: 125941.0000 - fp: 741.0000 - tn: 384183.0000 - fn: 2367.0000 - accuracy: 0.7508 - precision: 0.9942 - recall: 0.9816 - auc: 0.9997 - val_loss: 0.0385 - val_tp: 102894.0000 - val_fp: 1129.0000 - val_tn: 311666.0000 - val_fn: 1371.0000 - val_accuracy: 0.9940 - val_precision: 0.9891 - val_recall: 0.9869 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 437ms/step - loss: 0.0247 - tp: 126746.0000 - fp: 579.0000 - tn: 384345.0000 - fn: 1562.0000 - accuracy: 0.7525 - precision: 0.9955 - recall: 0.9878 - auc: 0.9998 - val_loss: 0.0389 - val_tp: 102945.0000 - val_fp: 1126.0000 - val_tn: 311669.0000 - val_fn: 1320.0000 - val_accuracy: 0.9941 - val_precision: 0.9892 - val_recall: 0.9873 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0193 - tp: 127165.0000 - fp: 499.0000 - tn: 384425.0000 - fn: 1143.0000 - accuracy: 0.7534 - precision: 0.9961 - recall: 0.9911 - auc: 0.9999 - val_loss: 0.0396 - val_tp: 102954.0000 - val_fp: 1141.0000 - val_tn: 311654.0000 - val_fn: 1311.0000 - val_accuracy: 0.9941 - val_precision: 0.9890 - val_recall: 0.9874 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0156 - tp: 127456.0000 - fp: 471.0000 - tn: 384453.0000 - fn: 852.0000 - accuracy: 0.7540 - precision: 0.9963 - recall: 0.9934 - auc: 0.9999 - val_loss: 0.0411 - val_tp: 102963.0000 - val_fp: 1145.0000 - val_tn: 311650.0000 - val_fn: 1302.0000 - val_accuracy: 0.9941 - val_precision: 0.9890 - val_recall: 0.9875 - val_auc: 0.9990\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0131 - tp: 127623.0000 - fp: 388.0000 - tn: 384536.0000 - fn: 685.0000 - accuracy: 0.7544 - precision: 0.9970 - recall: 0.9947 - auc: 0.9999 - val_loss: 0.0410 - val_tp: 102984.0000 - val_fp: 1134.0000 - val_tn: 311661.0000 - val_fn: 1281.0000 - val_accuracy: 0.9942 - val_precision: 0.9891 - val_recall: 0.9877 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 439ms/step - loss: 0.0111 - tp: 127734.0000 - fp: 354.0000 - tn: 384570.0000 - fn: 574.0000 - accuracy: 0.7547 - precision: 0.9972 - recall: 0.9955 - auc: 0.9999 - val_loss: 0.0430 - val_tp: 102990.0000 - val_fp: 1142.0000 - val_tn: 311653.0000 - val_fn: 1275.0000 - val_accuracy: 0.9942 - val_precision: 0.9890 - val_recall: 0.9878 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 433ms/step - loss: 0.0096 - tp: 127837.0000 - fp: 305.0000 - tn: 384619.0000 - fn: 471.0000 - accuracy: 0.7550 - precision: 0.9976 - recall: 0.9963 - auc: 0.9999 - val_loss: 0.0449 - val_tp: 103023.0000 - val_fp: 1143.0000 - val_tn: 311652.0000 - val_fn: 1242.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9881 - val_auc: 0.9988\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0080 - tp: 127923.0000 - fp: 255.0000 - tn: 384669.0000 - fn: 385.0000 - accuracy: 0.7552 - precision: 0.9980 - recall: 0.9970 - auc: 1.0000 - val_loss: 0.0442 - val_tp: 103022.0000 - val_fp: 1157.0000 - val_tn: 311638.0000 - val_fn: 1243.0000 - val_accuracy: 0.9942 - val_precision: 0.9889 - val_recall: 0.9881 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0070 - tp: 127966.0000 - fp: 232.0000 - tn: 384692.0000 - fn: 342.0000 - accuracy: 0.7553 - precision: 0.9982 - recall: 0.9973 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0070 - tp: 127966.0000 - fp: 232.0000 - tn: 384692.0000 - fn: 342.0000 - accuracy: 0.7553 - precision: 0.9982 - recall: 0.9973 - auc: 1.0000 - val_loss: 0.0485 - val_tp: 103034.0000 - val_fp: 1149.0000 - val_tn: 311646.0000 - val_fn: 1231.0000 - val_accuracy: 0.9943 - val_precision: 0.9890 - val_recall: 0.9882 - val_auc: 0.9986\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_downsampled/BiLSTM_downsampled_downweight_0.7.h5\n",
            "\n",
            "Down-weighting: 0.8\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 488ms/step - loss: 0.3636 - tp: 198067.0000 - fp: 10592.0000 - tn: 687127.0000 - fn: 34506.0000 - accuracy: 0.8494 - precision: 0.9492 - recall: 0.8516 - auc: 0.9900 - val_loss: 0.1131 - val_tp: 101148.0000 - val_fp: 2594.0000 - val_tn: 310201.0000 - val_fn: 3117.0000 - val_accuracy: 0.9863 - val_precision: 0.9750 - val_recall: 0.9701 - val_auc: 0.9981\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 431ms/step - loss: 0.1186 - tp: 124235.0000 - fp: 3583.0000 - tn: 381341.0000 - fn: 4073.0000 - accuracy: 0.7430 - precision: 0.9720 - recall: 0.9683 - auc: 0.9964 - val_loss: 0.0657 - val_tp: 102920.0000 - val_fp: 1248.0000 - val_tn: 311547.0000 - val_fn: 1345.0000 - val_accuracy: 0.9938 - val_precision: 0.9880 - val_recall: 0.9871 - val_auc: 0.9991\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 434ms/step - loss: 0.0949 - tp: 124916.0000 - fp: 3190.0000 - tn: 381734.0000 - fn: 3392.0000 - accuracy: 0.7438 - precision: 0.9751 - recall: 0.9736 - auc: 0.9977 - val_loss: 0.0568 - val_tp: 103001.0000 - val_fp: 1234.0000 - val_tn: 311561.0000 - val_fn: 1264.0000 - val_accuracy: 0.9940 - val_precision: 0.9882 - val_recall: 0.9879 - val_auc: 0.9993\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 435ms/step - loss: 0.0826 - tp: 125037.0000 - fp: 3096.0000 - tn: 381828.0000 - fn: 3271.0000 - accuracy: 0.7440 - precision: 0.9758 - recall: 0.9745 - auc: 0.9985 - val_loss: 0.0505 - val_tp: 102991.0000 - val_fp: 1241.0000 - val_tn: 311554.0000 - val_fn: 1274.0000 - val_accuracy: 0.9940 - val_precision: 0.9881 - val_recall: 0.9878 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0682 - tp: 124990.0000 - fp: 2615.0000 - tn: 382309.0000 - fn: 3318.0000 - accuracy: 0.7449 - precision: 0.9795 - recall: 0.9741 - auc: 0.9992 - val_loss: 0.0442 - val_tp: 102977.0000 - val_fp: 1240.0000 - val_tn: 311555.0000 - val_fn: 1288.0000 - val_accuracy: 0.9939 - val_precision: 0.9881 - val_recall: 0.9876 - val_auc: 0.9996\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0520 - tp: 125023.0000 - fp: 1610.0000 - tn: 383314.0000 - fn: 3285.0000 - accuracy: 0.7472 - precision: 0.9873 - recall: 0.9744 - auc: 0.9995 - val_loss: 0.0402 - val_tp: 102922.0000 - val_fp: 1177.0000 - val_tn: 311618.0000 - val_fn: 1343.0000 - val_accuracy: 0.9940 - val_precision: 0.9887 - val_recall: 0.9871 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 17s 431ms/step - loss: 0.0373 - tp: 125804.0000 - fp: 831.0000 - tn: 384093.0000 - fn: 2504.0000 - accuracy: 0.7502 - precision: 0.9934 - recall: 0.9805 - auc: 0.9997 - val_loss: 0.0388 - val_tp: 102921.0000 - val_fp: 1148.0000 - val_tn: 311647.0000 - val_fn: 1344.0000 - val_accuracy: 0.9940 - val_precision: 0.9890 - val_recall: 0.9871 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0270 - tp: 126581.0000 - fp: 594.0000 - tn: 384330.0000 - fn: 1727.0000 - accuracy: 0.7521 - precision: 0.9953 - recall: 0.9865 - auc: 0.9998 - val_loss: 0.0393 - val_tp: 102963.0000 - val_fp: 1136.0000 - val_tn: 311659.0000 - val_fn: 1302.0000 - val_accuracy: 0.9942 - val_precision: 0.9891 - val_recall: 0.9875 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 435ms/step - loss: 0.0212 - tp: 127062.0000 - fp: 529.0000 - tn: 384395.0000 - fn: 1246.0000 - accuracy: 0.7531 - precision: 0.9959 - recall: 0.9903 - auc: 0.9998 - val_loss: 0.0400 - val_tp: 102960.0000 - val_fp: 1148.0000 - val_tn: 311647.0000 - val_fn: 1305.0000 - val_accuracy: 0.9941 - val_precision: 0.9890 - val_recall: 0.9875 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 436ms/step - loss: 0.0173 - tp: 127379.0000 - fp: 484.0000 - tn: 384440.0000 - fn: 929.0000 - accuracy: 0.7538 - precision: 0.9962 - recall: 0.9928 - auc: 0.9999 - val_loss: 0.0415 - val_tp: 102962.0000 - val_fp: 1159.0000 - val_tn: 311636.0000 - val_fn: 1303.0000 - val_accuracy: 0.9941 - val_precision: 0.9889 - val_recall: 0.9875 - val_auc: 0.9990\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 434ms/step - loss: 0.0146 - tp: 127559.0000 - fp: 425.0000 - tn: 384499.0000 - fn: 749.0000 - accuracy: 0.7542 - precision: 0.9967 - recall: 0.9942 - auc: 0.9999 - val_loss: 0.0419 - val_tp: 102980.0000 - val_fp: 1151.0000 - val_tn: 311644.0000 - val_fn: 1285.0000 - val_accuracy: 0.9942 - val_precision: 0.9889 - val_recall: 0.9877 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0125 - tp: 127655.0000 - fp: 374.0000 - tn: 384550.0000 - fn: 653.0000 - accuracy: 0.7545 - precision: 0.9971 - recall: 0.9949 - auc: 0.9999 - val_loss: 0.0448 - val_tp: 102976.0000 - val_fp: 1173.0000 - val_tn: 311622.0000 - val_fn: 1289.0000 - val_accuracy: 0.9941 - val_precision: 0.9887 - val_recall: 0.9876 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 431ms/step - loss: 0.0110 - tp: 127776.0000 - fp: 339.0000 - tn: 384585.0000 - fn: 532.0000 - accuracy: 0.7547 - precision: 0.9974 - recall: 0.9959 - auc: 0.9999 - val_loss: 0.0460 - val_tp: 103002.0000 - val_fp: 1162.0000 - val_tn: 311633.0000 - val_fn: 1263.0000 - val_accuracy: 0.9942 - val_precision: 0.9888 - val_recall: 0.9879 - val_auc: 0.9988\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 431ms/step - loss: 0.0094 - tp: 127863.0000 - fp: 291.0000 - tn: 384633.0000 - fn: 445.0000 - accuracy: 0.7550 - precision: 0.9977 - recall: 0.9965 - auc: 0.9999 - val_loss: 0.0456 - val_tp: 103001.0000 - val_fp: 1164.0000 - val_tn: 311631.0000 - val_fn: 1264.0000 - val_accuracy: 0.9942 - val_precision: 0.9888 - val_recall: 0.9879 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0084 - tp: 127912.0000 - fp: 259.0000 - tn: 384665.0000 - fn: 396.0000 - accuracy: 0.7551 - precision: 0.9980 - recall: 0.9969 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 19s 484ms/step - loss: 0.0084 - tp: 127912.0000 - fp: 259.0000 - tn: 384665.0000 - fn: 396.0000 - accuracy: 0.7551 - precision: 0.9980 - recall: 0.9969 - auc: 1.0000 - val_loss: 0.0492 - val_tp: 103013.0000 - val_fp: 1167.0000 - val_tn: 311628.0000 - val_fn: 1252.0000 - val_accuracy: 0.9942 - val_precision: 0.9888 - val_recall: 0.9880 - val_auc: 0.9986\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_downsampled/BiLSTM_downsampled_downweight_0.8.h5\n",
            "\n",
            "Down-weighting: 0.9\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 28s 488ms/step - loss: 0.4012 - tp: 198364.0000 - fp: 10790.0000 - tn: 686929.0000 - fn: 34209.0000 - accuracy: 0.8491 - precision: 0.9484 - recall: 0.8529 - auc: 0.9900 - val_loss: 0.1118 - val_tp: 101148.0000 - val_fp: 2659.0000 - val_tn: 310136.0000 - val_fn: 3117.0000 - val_accuracy: 0.9862 - val_precision: 0.9744 - val_recall: 0.9701 - val_auc: 0.9981\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 441ms/step - loss: 0.1252 - tp: 124271.0000 - fp: 3630.0000 - tn: 381294.0000 - fn: 4037.0000 - accuracy: 0.7429 - precision: 0.9716 - recall: 0.9685 - auc: 0.9963 - val_loss: 0.0645 - val_tp: 102946.0000 - val_fp: 1253.0000 - val_tn: 311542.0000 - val_fn: 1319.0000 - val_accuracy: 0.9938 - val_precision: 0.9880 - val_recall: 0.9873 - val_auc: 0.9991\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 443ms/step - loss: 0.0992 - tp: 124936.0000 - fp: 3196.0000 - tn: 381728.0000 - fn: 3372.0000 - accuracy: 0.7438 - precision: 0.9751 - recall: 0.9737 - auc: 0.9976 - val_loss: 0.0560 - val_tp: 103005.0000 - val_fp: 1234.0000 - val_tn: 311561.0000 - val_fn: 1260.0000 - val_accuracy: 0.9940 - val_precision: 0.9882 - val_recall: 0.9879 - val_auc: 0.9993\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 438ms/step - loss: 0.0867 - tp: 125058.0000 - fp: 3137.0000 - tn: 381787.0000 - fn: 3250.0000 - accuracy: 0.7439 - precision: 0.9755 - recall: 0.9747 - auc: 0.9984 - val_loss: 0.0503 - val_tp: 103007.0000 - val_fp: 1247.0000 - val_tn: 311548.0000 - val_fn: 1258.0000 - val_accuracy: 0.9940 - val_precision: 0.9880 - val_recall: 0.9879 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 17s 437ms/step - loss: 0.0721 - tp: 125039.0000 - fp: 2763.0000 - tn: 382161.0000 - fn: 3269.0000 - accuracy: 0.7446 - precision: 0.9784 - recall: 0.9745 - auc: 0.9991 - val_loss: 0.0446 - val_tp: 102991.0000 - val_fp: 1246.0000 - val_tn: 311549.0000 - val_fn: 1274.0000 - val_accuracy: 0.9940 - val_precision: 0.9880 - val_recall: 0.9878 - val_auc: 0.9996\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 17s 438ms/step - loss: 0.0559 - tp: 125034.0000 - fp: 1875.0000 - tn: 383049.0000 - fn: 3274.0000 - accuracy: 0.7465 - precision: 0.9852 - recall: 0.9745 - auc: 0.9995 - val_loss: 0.0407 - val_tp: 102941.0000 - val_fp: 1194.0000 - val_tn: 311601.0000 - val_fn: 1324.0000 - val_accuracy: 0.9940 - val_precision: 0.9885 - val_recall: 0.9873 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 17s 444ms/step - loss: 0.0404 - tp: 125642.0000 - fp: 934.0000 - tn: 383990.0000 - fn: 2666.0000 - accuracy: 0.7496 - precision: 0.9926 - recall: 0.9792 - auc: 0.9997 - val_loss: 0.0391 - val_tp: 102928.0000 - val_fp: 1151.0000 - val_tn: 311644.0000 - val_fn: 1337.0000 - val_accuracy: 0.9940 - val_precision: 0.9889 - val_recall: 0.9872 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 435ms/step - loss: 0.0292 - tp: 126471.0000 - fp: 627.0000 - tn: 384297.0000 - fn: 1837.0000 - accuracy: 0.7518 - precision: 0.9951 - recall: 0.9857 - auc: 0.9998 - val_loss: 0.0396 - val_tp: 102969.0000 - val_fp: 1151.0000 - val_tn: 311644.0000 - val_fn: 1296.0000 - val_accuracy: 0.9941 - val_precision: 0.9889 - val_recall: 0.9876 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 435ms/step - loss: 0.0231 - tp: 126956.0000 - fp: 549.0000 - tn: 384375.0000 - fn: 1352.0000 - accuracy: 0.7528 - precision: 0.9957 - recall: 0.9895 - auc: 0.9998 - val_loss: 0.0402 - val_tp: 102964.0000 - val_fp: 1154.0000 - val_tn: 311641.0000 - val_fn: 1301.0000 - val_accuracy: 0.9941 - val_precision: 0.9889 - val_recall: 0.9875 - val_auc: 0.9991\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 431ms/step - loss: 0.0188 - tp: 127283.0000 - fp: 513.0000 - tn: 384411.0000 - fn: 1025.0000 - accuracy: 0.7535 - precision: 0.9960 - recall: 0.9920 - auc: 0.9999 - val_loss: 0.0418 - val_tp: 102968.0000 - val_fp: 1165.0000 - val_tn: 311630.0000 - val_fn: 1297.0000 - val_accuracy: 0.9941 - val_precision: 0.9888 - val_recall: 0.9876 - val_auc: 0.9990\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 445ms/step - loss: 0.0159 - tp: 127495.0000 - fp: 426.0000 - tn: 384498.0000 - fn: 813.0000 - accuracy: 0.7540 - precision: 0.9967 - recall: 0.9937 - auc: 0.9999 - val_loss: 0.0423 - val_tp: 102967.0000 - val_fp: 1176.0000 - val_tn: 311619.0000 - val_fn: 1298.0000 - val_accuracy: 0.9941 - val_precision: 0.9887 - val_recall: 0.9876 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 437ms/step - loss: 0.0136 - tp: 127631.0000 - fp: 380.0000 - tn: 384544.0000 - fn: 677.0000 - accuracy: 0.7544 - precision: 0.9970 - recall: 0.9947 - auc: 0.9999 - val_loss: 0.0449 - val_tp: 102964.0000 - val_fp: 1178.0000 - val_tn: 311617.0000 - val_fn: 1301.0000 - val_accuracy: 0.9941 - val_precision: 0.9887 - val_recall: 0.9875 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 435ms/step - loss: 0.0120 - tp: 127754.0000 - fp: 342.0000 - tn: 384582.0000 - fn: 554.0000 - accuracy: 0.7547 - precision: 0.9973 - recall: 0.9957 - auc: 0.9999 - val_loss: 0.0470 - val_tp: 103015.0000 - val_fp: 1161.0000 - val_tn: 311634.0000 - val_fn: 1250.0000 - val_accuracy: 0.9942 - val_precision: 0.9889 - val_recall: 0.9880 - val_auc: 0.9988\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 444ms/step - loss: 0.0102 - tp: 127839.0000 - fp: 302.0000 - tn: 384622.0000 - fn: 469.0000 - accuracy: 0.7549 - precision: 0.9976 - recall: 0.9963 - auc: 0.9999 - val_loss: 0.0463 - val_tp: 102979.0000 - val_fp: 1186.0000 - val_tn: 311609.0000 - val_fn: 1286.0000 - val_accuracy: 0.9941 - val_precision: 0.9886 - val_recall: 0.9877 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0092 - tp: 127893.0000 - fp: 281.0000 - tn: 384643.0000 - fn: 415.0000 - accuracy: 0.7550 - precision: 0.9978 - recall: 0.9968 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 436ms/step - loss: 0.0092 - tp: 127893.0000 - fp: 281.0000 - tn: 384643.0000 - fn: 415.0000 - accuracy: 0.7550 - precision: 0.9978 - recall: 0.9968 - auc: 1.0000 - val_loss: 0.0501 - val_tp: 103021.0000 - val_fp: 1169.0000 - val_tn: 311626.0000 - val_fn: 1244.0000 - val_accuracy: 0.9942 - val_precision: 0.9888 - val_recall: 0.9881 - val_auc: 0.9986\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_downsampled/BiLSTM_downsampled_downweight_0.9.h5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiV3AQPezk-E",
        "outputId": "6b6548db-00cb-4743-ea6f-e0ef1597120f"
      },
      "source": [
        "downweight_models[1.0] = model\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n",
        "    preds = np.argmax(downweight_models[weight].predict(dev_seqs_padded), axis=-1)\n",
        "\n",
        "    dev_seqs['prediction'] = ''\n",
        "    for i in dev_seqs.index:\n",
        "        this_seq_length = len(dev_seqs['token'][i])\n",
        "        dev_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    dev_long = dev_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = [reverse_bio(b) for b in dev_long['bio_only']]\n",
        "    dev_long['bio_only'] = bio_labs\n",
        "    pred_labs = [reverse_bio(b) for b in dev_long['prediction']]\n",
        "    dev_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(dev_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 0.1:\n",
            "Sum of TP and FP = 1780\n",
            "Sum of TP and FN = 826\n",
            "True positives = 283, False positives = 1497, False negatives = 543\n",
            "Precision = 0.159, Recall = 0.343, F1 = 0.217\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 672\n",
            "Sum of TP and FN = 826\n",
            "True positives = 73, False positives = 599, False negatives = 753\n",
            "Precision = 0.109, Recall = 0.088, F1 = 0.097\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 246\n",
            "Sum of TP and FN = 826\n",
            "True positives = 36, False positives = 210, False negatives = 790\n",
            "Precision = 0.146, Recall = 0.044, F1 = 0.067\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 111\n",
            "Sum of TP and FN = 826\n",
            "True positives = 17, False positives = 94, False negatives = 809\n",
            "Precision = 0.153, Recall = 0.021, F1 = 0.036\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 46\n",
            "Sum of TP and FN = 826\n",
            "True positives = 3, False positives = 43, False negatives = 823\n",
            "Precision = 0.065, Recall = 0.004, F1 = 0.007\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 14\n",
            "Sum of TP and FN = 826\n",
            "True positives = 1, False positives = 13, False negatives = 825\n",
            "Precision = 0.071, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 4\n",
            "Sum of TP and FN = 826\n",
            "True positives = 1, False positives = 3, False negatives = 825\n",
            "Precision = 0.250, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 1\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 1, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 0, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 1.0:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 0, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY_dgC3cEqed"
      },
      "source": [
        "### 3. Merging B and I labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr0MqLrW1reH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "outputId": "494de8bf-4fc5-4ccc-e5b1-60d2451f94d0"
      },
      "source": [
        "# training labels: convert BIO to integers, this time merging B and I\n",
        "def bo_index(bio):\n",
        "    ind = bio\n",
        "    if not pd.isnull(bio):  # deal with empty lines\n",
        "        if bio == 'B' or bio =='I':\n",
        "            ind = 0\n",
        "        elif bio == 'O':\n",
        "            ind = 1\n",
        "    return ind\n",
        "\n",
        "# pass a data frame through the new feature extractor\n",
        "def extract_features_bo(txt, istest=False):\n",
        "    txt_copy = txt.copy()\n",
        "    tokinds = [token_index(u) for u in txt_copy['token']]\n",
        "    txt_copy['token_indices'] = tokinds\n",
        "    if not istest:  # can't do this with the test set\n",
        "        boints = [bo_index(b) for b in txt_copy['bio_only']]\n",
        "        txt_copy['bio_only'] = boints\n",
        "    return txt_copy\n",
        "\n",
        "train_copy_bo = extract_features_bo(train)\n",
        "train_copy_bo.head(n=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "      <th>token_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@paulwalk</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PRON</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'s</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>AUX</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>DET</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>view</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>where</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PRON</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>'m</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>X</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>living</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>for</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>two</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NUM</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>weeks</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Empire</td>\n",
              "      <td>B-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>State</td>\n",
              "      <td>I-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Building</td>\n",
              "      <td>I-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>=</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>SYM</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ESB</td>\n",
              "      <td>B-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Pretty</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>bad</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>storm</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>here</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>last</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>evening</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>From</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Green</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token       label  bio_only   upos  token_indices\n",
              "0   @paulwalk           O       1.0   NOUN            0.0\n",
              "1          It           O       1.0   PRON            1.0\n",
              "2          's           O       1.0    AUX            2.0\n",
              "3         the           O       1.0    DET            3.0\n",
              "4        view           O       1.0   NOUN            4.0\n",
              "5        from           O       1.0    ADP            5.0\n",
              "6       where           O       1.0    ADV            6.0\n",
              "7           I           O       1.0   PRON            7.0\n",
              "8          'm           O       1.0      X            8.0\n",
              "9      living           O       1.0   NOUN            9.0\n",
              "10        for           O       1.0    ADP           10.0\n",
              "11        two           O       1.0    NUM           11.0\n",
              "12      weeks           O       1.0   NOUN           12.0\n",
              "13          .           O       1.0  PUNCT           13.0\n",
              "14     Empire  B-location       0.0  PROPN           14.0\n",
              "15      State  I-location       0.0  PROPN           15.0\n",
              "16   Building  I-location       0.0  PROPN           16.0\n",
              "17          =           O       1.0    SYM           17.0\n",
              "18        ESB  B-location       0.0  PROPN           18.0\n",
              "19          .           O       1.0  PUNCT           13.0\n",
              "20     Pretty           O       1.0    ADV           19.0\n",
              "21        bad           O       1.0    ADJ           20.0\n",
              "22      storm           O       1.0   NOUN           21.0\n",
              "23       here           O       1.0    ADV           22.0\n",
              "24       last           O       1.0    ADJ           23.0\n",
              "25    evening           O       1.0   NOUN           24.0\n",
              "26          .           O       1.0  PUNCT           13.0\n",
              "27        NaN         NaN       NaN    NaN            NaN\n",
              "28       From           O       1.0    ADP           26.0\n",
              "29      Green           O       1.0  PROPN           27.0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "e92MaD_SGdvf",
        "outputId": "5b3ee382-fa63-4ee7-a380-7b50a3fa654c"
      },
      "source": [
        "print(\"This cell takes a little while to run: be patient :)\")\n",
        "train_seqs_bo = tokens2sequences(train_copy_bo)\n",
        "train_seqs_bo.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This cell takes a little while to run: be patient :)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[@paulwalk, It, 's, the, view, from, where, I,...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[From, Green, Newsfeed, :, AHFA, extends, dead...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Pxleyes, Top, 50, Photography, Contest, Pictu...</td>\n",
              "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[today, is, my, last, day, at, the, office, .]</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
              "      <td>[51.0, 52.0, 53.0, 23.0, 54.0, 55.0, 3.0, 56.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[4Dbling, 's, place, til, monday, ,, party, pa...</td>\n",
              "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                      token_indices\n",
              "0             0  ...  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...\n",
              "1             1  ...  [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
              "2             2  ...  [39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....\n",
              "3             3  ...  [51.0, 52.0, 53.0, 23.0, 54.0, 55.0, 3.0, 56.0...\n",
              "4             4  ...  [57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "s2JoAkBuGtvp",
        "outputId": "c4bf8815-2174-43a8-ead5-2222b595dd5e"
      },
      "source": [
        "# process the dev set\n",
        "wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'\n",
        "dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "dev_copy_bo = extract_features_bo(dev)\n",
        "dev_seqs_bo = tokens2sequences(dev_copy_bo)\n",
        "dev_seqs_bo.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[Stabilized, approach, or, not, ?, That, , s,...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[14801.0, 10361.0, 414.0, 556.0, 131.0, 1740.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[You, should, ', ve, stayed, on, Redondo, Beac...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[151.0, 1018.0, 573.0, 12927.0, 9346.0, 137.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[All, I, ', ve, been, doing, is, BINGE, watchi...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[405.0, 7.0, 573.0, 12927.0, 90.0, 848.0, 52.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[wow, emma, and, kaite, is, so, very, cute, an...</td>\n",
              "      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[4777.0, 14801.0, 113.0, 14801.0, 52.0, 79.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[THIS, IS, SO, GOOD]</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0]</td>\n",
              "      <td>[2239.0, 1567.0, 1089.0, 9176.0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                      token_indices\n",
              "0             0  ...  [14801.0, 10361.0, 414.0, 556.0, 131.0, 1740.0...\n",
              "1             1  ...  [151.0, 1018.0, 573.0, 12927.0, 9346.0, 137.0,...\n",
              "2             2  ...  [405.0, 7.0, 573.0, 12927.0, 90.0, 848.0, 52.0...\n",
              "3             3  ...  [4777.0, 14801.0, 113.0, 14801.0, 52.0, 79.0, ...\n",
              "4             4  ...                   [2239.0, 1567.0, 1089.0, 9176.0]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQUOoL5EI_wq",
        "outputId": "1acfcf62-b845-4c3f-8bcb-56482f0b129e"
      },
      "source": [
        "# process the 2-class training set: padding the tokens & labels, and one-hot encoding the labels\n",
        "train_seqs_bo_padded = pad_sequences(train_seqs_bo['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                     dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "padlab_bo = 2\n",
        "train_labs_bo_padded = pad_sequences(train_seqs_bo['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                     dtype='int32', padding='post', truncating='post', value=padlab_bo)\n",
        "n_labs_bo = 3\n",
        "train_labs_bo_onehot = [to_categorical(i, num_classes=n_labs_bo) for i in train_labs_bo_padded]\n",
        "\n",
        "print('Example of padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(train_seqs_bo.loc[1])\n",
        "print('Length of input sequence: %i' % len(train_seqs_bo_padded[1]))\n",
        "print('Length of label sequence: %i' % len(train_labs_bo_onehot[1]))\n",
        "print(train_labs_bo_padded[1][:11])\n",
        "print(train_labs_bo_onehot[1][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     1\n",
            "token            [From, Green, Newsfeed, :, AHFA, extends, dead...\n",
            "bio_only         [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...\n",
            "token_indices    [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
            "Name: 1, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of label sequence: 105\n",
            "[1 1 1 1 0 1 1 1 1 1 1]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyfJtP9rKjHn",
        "outputId": "a0ce5eae-76a3-4b1a-b994-8d57c10d1317"
      },
      "source": [
        "# now process the 2-class dev set in the same way: padding the tokens & labels, and one-hot encoding the labels\n",
        "dev_seqs_bo_padded = pad_sequences(dev_seqs_bo['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                   dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "dev_labs_bo_padded = pad_sequences(dev_seqs_bo['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                   dtype='int32', padding='post', truncating='post', value=padlab_bo)\n",
        "dev_labs_bo_onehot = [to_categorical(i, num_classes=n_labs_bo) for i in dev_labs_bo_padded]\n",
        "\n",
        "print('Dev set padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(dev_seqs_bo.loc[2])\n",
        "print('Length of input sequence: %i' % len(dev_seqs_bo_padded[1]))\n",
        "print('Length of label sequence: %i' % len(dev_labs_bo_onehot[1]))\n",
        "print(dev_labs_bo_padded[2][:11])\n",
        "print(dev_labs_bo_onehot[2][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev set padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     2\n",
            "token            [All, I, ', ve, been, doing, is, BINGE, watchi...\n",
            "bio_only         [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
            "token_indices    [405.0, 7.0, 573.0, 12927.0, 90.0, 848.0, 52.0...\n",
            "Name: 2, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of label sequence: 105\n",
            "[1 1 1 1 1 1 1 1 1 0 0]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_xbaUJ4LDZJ",
        "outputId": "f3801dec-5feb-422b-c2ca-254a25920563"
      },
      "source": [
        "all_labs_bo = [l for lab in train_labs_bo_padded for l in lab]\n",
        "label_count = Counter(all_labs_bo)\n",
        "total_labs = len(all_labs_bo)\n",
        "print(label_count)\n",
        "print(total_labs)\n",
        "\n",
        "# use this to define an initial model bias\n",
        "initial_bias = [(label_count[0]/total_labs), (label_count[1]/total_labs), (label_count[2]/total_labs)]\n",
        "print('Initial bias:')\n",
        "print(initial_bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({2: 292139, 1: 59095, 0: 3141})\n",
            "354375\n",
            "Initial bias:\n",
            "[0.008863492063492063, 0.1667583774250441, 0.8243781305114638]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6BIptYHK0fo",
        "outputId": "337bb22c-fa11-4a1b-d66c-9a9d2302944d"
      },
      "source": [
        "# prepare sequences and labels as numpy arrays, check dimensions\n",
        "X = np.array(train_seqs_bo_padded)\n",
        "y = np.array(train_labs_bo_onehot)\n",
        "print('Input sequence dimensions (n.docs, seq.length):')\n",
        "print(X.shape)\n",
        "print('Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels):')\n",
        "print(y.shape)\n",
        "\n",
        "# use the same model, with the only difference in the output dimensions\n",
        "def make_model_bo(metrics=METRICS, output_bias=None, seed=42):\n",
        "    init_random_seed(seed)\n",
        "    if output_bias is not None:\n",
        "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_length, mask_zero=True, trainable=True),\n",
        "        keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),  # 2 directions, 50 units each, concatenated (can change this)\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(n_labs_bo, activation='softmax', bias_initializer=output_bias)),\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)\n",
        "    return model\n",
        "\n",
        "print('**Defining a neural network**')\n",
        "# pass the bias to the model and re-evaluate\n",
        "model = make_model_bo(output_bias=initial_bias)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence dimensions (n.docs, seq.length):\n",
            "(3375, 105)\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels):\n",
            "(3375, 105, 3)\n",
            "**Defining a neural network**\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 105, 128)          1894784   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 105, 100)         71600     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 105, 100)          0         \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 105, 3)           303       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,966,687\n",
            "Trainable params: 1,966,687\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nlvi2qGsE-3"
      },
      "source": [
        "# prepare the dev sequences and labels as numpy arrays\n",
        "dev_X = np.array(dev_seqs_bo_padded)\n",
        "dev_y = np.array(dev_labs_bo_onehot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhUasVbrMe8-",
        "outputId": "ab5ce3ce-22b7-42f7-bf45-5784b18eb75d"
      },
      "source": [
        "# re-initiate model with bias\n",
        "model = make_model_bo(output_bias=initial_bias)\n",
        "\n",
        "# and fit...\n",
        "model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(dev_X, dev_y))\n",
        "\n",
        "# save the model\n",
        "model.save('BiLSTM_2class/BiLSTM_2class.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "106/106 [==============================] - 54s 428ms/step - loss: 0.1487 - tp: 436835.0000 - fp: 16284.0000 - tn: 1005257.0000 - fn: 21803.0000 - accuracy: 0.9728 - precision: 0.9641 - recall: 0.9525 - auc: 0.9964 - val_loss: 0.0499 - val_tp: 103023.0000 - val_fp: 1240.0000 - val_tn: 207290.0000 - val_fn: 1242.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9991\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0374 - tp: 351061.0000 - fp: 3250.0000 - tn: 705496.0000 - fn: 3312.0000 - accuracy: 0.9938 - precision: 0.9908 - recall: 0.9907 - auc: 0.9994 - val_loss: 0.0409 - val_tp: 103025.0000 - val_fp: 1239.0000 - val_tn: 207291.0000 - val_fn: 1240.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 44s 412ms/step - loss: 0.0246 - tp: 351345.0000 - fp: 2925.0000 - tn: 705821.0000 - fn: 3028.0000 - accuracy: 0.9944 - precision: 0.9917 - recall: 0.9915 - auc: 0.9998 - val_loss: 0.0360 - val_tp: 103028.0000 - val_fp: 1234.0000 - val_tn: 207296.0000 - val_fn: 1237.0000 - val_accuracy: 0.9921 - val_precision: 0.9882 - val_recall: 0.9881 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0131 - tp: 352657.0000 - fp: 1605.0000 - tn: 707141.0000 - fn: 1716.0000 - accuracy: 0.9969 - precision: 0.9955 - recall: 0.9952 - auc: 0.9999 - val_loss: 0.0374 - val_tp: 103081.0000 - val_fp: 1178.0000 - val_tn: 207352.0000 - val_fn: 1184.0000 - val_accuracy: 0.9924 - val_precision: 0.9887 - val_recall: 0.9886 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 44s 412ms/step - loss: 0.0078 - tp: 353545.0000 - fp: 774.0000 - tn: 707972.0000 - fn: 828.0000 - accuracy: 0.9985 - precision: 0.9978 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0450 - val_tp: 103077.0000 - val_fp: 1187.0000 - val_tn: 207343.0000 - val_fn: 1188.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9886 - val_auc: 0.9984\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0057 - tp: 353766.0000 - fp: 576.0000 - tn: 708170.0000 - fn: 607.0000 - accuracy: 0.9989 - precision: 0.9984 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0471 - val_tp: 103075.0000 - val_fp: 1188.0000 - val_tn: 207342.0000 - val_fn: 1190.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9886 - val_auc: 0.9981\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 44s 412ms/step - loss: 0.0046 - tp: 353852.0000 - fp: 489.0000 - tn: 708257.0000 - fn: 521.0000 - accuracy: 0.9991 - precision: 0.9986 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0504 - val_tp: 103078.0000 - val_fp: 1186.0000 - val_tn: 207344.0000 - val_fn: 1187.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9886 - val_auc: 0.9975\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 44s 414ms/step - loss: 0.0038 - tp: 353943.0000 - fp: 410.0000 - tn: 708336.0000 - fn: 430.0000 - accuracy: 0.9992 - precision: 0.9988 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0536 - val_tp: 103079.0000 - val_fp: 1185.0000 - val_tn: 207345.0000 - val_fn: 1186.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9886 - val_auc: 0.9971\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0030 - tp: 354053.0000 - fp: 298.0000 - tn: 708448.0000 - fn: 320.0000 - accuracy: 0.9994 - precision: 0.9992 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0551 - val_tp: 103071.0000 - val_fp: 1190.0000 - val_tn: 207340.0000 - val_fn: 1194.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9885 - val_auc: 0.9969\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0025 - tp: 354094.0000 - fp: 268.0000 - tn: 708478.0000 - fn: 279.0000 - accuracy: 0.9995 - precision: 0.9992 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0571 - val_tp: 103060.0000 - val_fp: 1204.0000 - val_tn: 207326.0000 - val_fn: 1205.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9884 - val_auc: 0.9967\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0021 - tp: 354137.0000 - fp: 231.0000 - tn: 708515.0000 - fn: 236.0000 - accuracy: 0.9996 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0615 - val_tp: 103044.0000 - val_fp: 1218.0000 - val_tn: 207312.0000 - val_fn: 1221.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9883 - val_auc: 0.9963\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 44s 416ms/step - loss: 0.0018 - tp: 354185.0000 - fp: 183.0000 - tn: 708563.0000 - fn: 188.0000 - accuracy: 0.9997 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0698 - val_tp: 103082.0000 - val_fp: 1181.0000 - val_tn: 207349.0000 - val_fn: 1183.0000 - val_accuracy: 0.9924 - val_precision: 0.9887 - val_recall: 0.9887 - val_auc: 0.9953\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0015 - tp: 354222.0000 - fp: 144.0000 - tn: 708602.0000 - fn: 151.0000 - accuracy: 0.9997 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0015 - tp: 354222.0000 - fp: 144.0000 - tn: 708602.0000 - fn: 151.0000 - accuracy: 0.9997 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0680 - val_tp: 103072.0000 - val_fp: 1192.0000 - val_tn: 207338.0000 - val_fn: 1193.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9886 - val_auc: 0.9956\n",
            "Epoch 00013: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bthhDXINO3xk"
      },
      "source": [
        "# now try the weighted one-hot encoding\n",
        "def down_weight_bo(labs_onehot, weight=1):\n",
        "    weights_onehot = copy.deepcopy(labs_onehot)\n",
        "\n",
        "    # our first-pass class weights: normal for named entities (0), down-weighted for non named entities (1 and 2)\n",
        "    class_wts = [1, weight, weight]\n",
        "\n",
        "    # apply our weights to the label lists\n",
        "    for i, labs in enumerate(weights_onehot):\n",
        "        for j, lablist in enumerate(labs):\n",
        "            lablistaslist = lablist.tolist()\n",
        "            whichismax = lablistaslist.index(max(lablistaslist))\n",
        "            weights_onehot[i][j][whichismax] = class_wts[whichismax]\n",
        "    \n",
        "    return weights_onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlUA_eLzOUlK",
        "outputId": "493534f8-8987-4ae7-fb7e-a6687ac96a7d"
      },
      "source": [
        "downweight_models = {}\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9]:\n",
        "    train_weights_onehot = down_weight_bo(train_labs_bo_onehot, weight)\n",
        "    y = np.array(train_weights_onehot)\n",
        "    print('Down-weighting:', weight)\n",
        "    print('Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels):', np.shape(y))\n",
        "\n",
        "    downweight_model = make_model_bo(output_bias=initial_bias)\n",
        "    downweight_model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(dev_X, dev_y))\n",
        "\n",
        "    downweight_models[weight] = downweight_model\n",
        "    downweight_model.save(f'BiLSTM_2class/BiLSTM_2class_downweight_{weight}.h5')\n",
        "    print(f'Model saved at BiLSTM_2class/BiLSTM_2class_downweight_{weight}.h5')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Down-weighting: 0.1\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 60s 429ms/step - loss: 0.0285 - tp: 429815.0000 - fp: 16382.0000 - tn: 900894.0000 - fn: 28823.0000 - accuracy: 0.7298 - precision: 0.9633 - recall: 0.9372 - auc: 0.9946 - val_loss: 0.0646 - val_tp: 102924.0000 - val_fp: 1228.0000 - val_tn: 207302.0000 - val_fn: 1341.0000 - val_accuracy: 0.9918 - val_precision: 0.9882 - val_recall: 0.9871 - val_auc: 0.9995\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0121 - tp: 347126.0000 - fp: 6300.0000 - tn: 702446.0000 - fn: 7247.0000 - accuracy: 0.6623 - precision: 0.9822 - recall: 0.9795 - auc: 0.9993 - val_loss: 0.0460 - val_tp: 102213.0000 - val_fp: 1958.0000 - val_tn: 206572.0000 - val_fn: 2052.0000 - val_accuracy: 0.9872 - val_precision: 0.9812 - val_recall: 0.9803 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 44s 412ms/step - loss: 0.0056 - tp: 350636.0000 - fp: 3563.0000 - tn: 705183.0000 - fn: 3737.0000 - accuracy: 0.6659 - precision: 0.9899 - recall: 0.9895 - auc: 0.9998 - val_loss: 0.0311 - val_tp: 102752.0000 - val_fp: 1458.0000 - val_tn: 207072.0000 - val_fn: 1513.0000 - val_accuracy: 0.9905 - val_precision: 0.9860 - val_recall: 0.9855 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 43s 407ms/step - loss: 0.0026 - tp: 352664.0000 - fp: 1660.0000 - tn: 707086.0000 - fn: 1709.0000 - accuracy: 0.6679 - precision: 0.9953 - recall: 0.9952 - auc: 0.9999 - val_loss: 0.0320 - val_tp: 102841.0000 - val_fp: 1372.0000 - val_tn: 207158.0000 - val_fn: 1424.0000 - val_accuracy: 0.9911 - val_precision: 0.9868 - val_recall: 0.9863 - val_auc: 0.9994\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 43s 405ms/step - loss: 0.0017 - tp: 353175.0000 - fp: 1177.0000 - tn: 707569.0000 - fn: 1198.0000 - accuracy: 0.6684 - precision: 0.9967 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0349 - val_tp: 102940.0000 - val_fp: 1305.0000 - val_tn: 207225.0000 - val_fn: 1325.0000 - val_accuracy: 0.9916 - val_precision: 0.9875 - val_recall: 0.9873 - val_auc: 0.9990\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 44s 414ms/step - loss: 0.0012 - tp: 353514.0000 - fp: 846.0000 - tn: 707900.0000 - fn: 859.0000 - accuracy: 0.6688 - precision: 0.9976 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0369 - val_tp: 103007.0000 - val_fp: 1249.0000 - val_tn: 207281.0000 - val_fn: 1258.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9879 - val_auc: 0.9987\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 44s 416ms/step - loss: 9.4217e-04 - tp: 353704.0000 - fp: 664.0000 - tn: 708082.0000 - fn: 669.0000 - accuracy: 0.6690 - precision: 0.9981 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0412 - val_tp: 102894.0000 - val_fp: 1358.0000 - val_tn: 207172.0000 - val_fn: 1371.0000 - val_accuracy: 0.9913 - val_precision: 0.9870 - val_recall: 0.9869 - val_auc: 0.9985\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 7.8309e-04 - tp: 353815.0000 - fp: 552.0000 - tn: 708194.0000 - fn: 558.0000 - accuracy: 0.6691 - precision: 0.9984 - recall: 0.9984 - auc: 0.9999 - val_loss: 0.0421 - val_tp: 102899.0000 - val_fp: 1363.0000 - val_tn: 207167.0000 - val_fn: 1366.0000 - val_accuracy: 0.9913 - val_precision: 0.9869 - val_recall: 0.9869 - val_auc: 0.9984\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 44s 412ms/step - loss: 6.6173e-04 - tp: 353907.0000 - fp: 463.0000 - tn: 708283.0000 - fn: 466.0000 - accuracy: 0.6692 - precision: 0.9987 - recall: 0.9987 - auc: 0.9999 - val_loss: 0.0438 - val_tp: 102828.0000 - val_fp: 1427.0000 - val_tn: 207103.0000 - val_fn: 1437.0000 - val_accuracy: 0.9908 - val_precision: 0.9863 - val_recall: 0.9862 - val_auc: 0.9984\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 5.5397e-04 - tp: 353981.0000 - fp: 387.0000 - tn: 708359.0000 - fn: 392.0000 - accuracy: 0.6692 - precision: 0.9989 - recall: 0.9989 - auc: 0.9999 - val_loss: 0.0463 - val_tp: 102997.0000 - val_fp: 1263.0000 - val_tn: 207267.0000 - val_fn: 1268.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9878 - val_auc: 0.9979\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 4.3909e-04 - tp: 354073.0000 - fp: 299.0000 - tn: 708447.0000 - fn: 300.0000 - accuracy: 0.6693 - precision: 0.9992 - recall: 0.9992 - auc: 0.9999 - val_loss: 0.0486 - val_tp: 102926.0000 - val_fp: 1332.0000 - val_tn: 207198.0000 - val_fn: 1339.0000 - val_accuracy: 0.9915 - val_precision: 0.9872 - val_recall: 0.9872 - val_auc: 0.9978\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 3.7332e-04 - tp: 354119.0000 - fp: 251.0000 - tn: 708495.0000 - fn: 254.0000 - accuracy: 0.6694 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0527 - val_tp: 103006.0000 - val_fp: 1256.0000 - val_tn: 207274.0000 - val_fn: 1259.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9879 - val_auc: 0.9973\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 3.5869e-04 - tp: 354133.0000 - fp: 238.0000 - tn: 708508.0000 - fn: 240.0000 - accuracy: 0.6694 - precision: 0.9993 - recall: 0.9993 - auc: 0.9999Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 3.5869e-04 - tp: 354133.0000 - fp: 238.0000 - tn: 708508.0000 - fn: 240.0000 - accuracy: 0.6694 - precision: 0.9993 - recall: 0.9993 - auc: 0.9999 - val_loss: 0.0506 - val_tp: 102827.0000 - val_fp: 1433.0000 - val_tn: 207097.0000 - val_fn: 1438.0000 - val_accuracy: 0.9908 - val_precision: 0.9863 - val_recall: 0.9862 - val_auc: 0.9978\n",
            "Epoch 00013: early stopping\n",
            "Model saved at BiLSTM_2class/BiLSTM_2class_downweight_0.1.h5\n",
            "\n",
            "Down-weighting: 0.2\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 54s 427ms/step - loss: 0.0443 - tp: 434062.0000 - fp: 15871.0000 - tn: 901405.0000 - fn: 24576.0000 - accuracy: 0.7299 - precision: 0.9647 - recall: 0.9464 - auc: 0.9959 - val_loss: 0.0508 - val_tp: 102990.0000 - val_fp: 1231.0000 - val_tn: 207299.0000 - val_fn: 1275.0000 - val_accuracy: 0.9920 - val_precision: 0.9882 - val_recall: 0.9878 - val_auc: 0.9994\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0172 - tp: 350530.0000 - fp: 3462.0000 - tn: 705284.0000 - fn: 3843.0000 - accuracy: 0.6641 - precision: 0.9902 - recall: 0.9892 - auc: 0.9997 - val_loss: 0.0382 - val_tp: 102810.0000 - val_fp: 1439.0000 - val_tn: 207091.0000 - val_fn: 1455.0000 - val_accuracy: 0.9907 - val_precision: 0.9862 - val_recall: 0.9860 - val_auc: 0.9997\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0084 - tp: 351853.0000 - fp: 2409.0000 - tn: 706337.0000 - fn: 2520.0000 - accuracy: 0.6667 - precision: 0.9932 - recall: 0.9929 - auc: 0.9999 - val_loss: 0.0297 - val_tp: 102989.0000 - val_fp: 1264.0000 - val_tn: 207266.0000 - val_fn: 1276.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9878 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 43s 406ms/step - loss: 0.0040 - tp: 353042.0000 - fp: 1281.0000 - tn: 707465.0000 - fn: 1331.0000 - accuracy: 0.6682 - precision: 0.9964 - recall: 0.9962 - auc: 0.9999 - val_loss: 0.0319 - val_tp: 102980.0000 - val_fp: 1263.0000 - val_tn: 207267.0000 - val_fn: 1285.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9877 - val_auc: 0.9993\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0027 - tp: 353455.0000 - fp: 892.0000 - tn: 707854.0000 - fn: 918.0000 - accuracy: 0.6686 - precision: 0.9975 - recall: 0.9974 - auc: 1.0000 - val_loss: 0.0349 - val_tp: 103001.0000 - val_fp: 1259.0000 - val_tn: 207271.0000 - val_fn: 1264.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9879 - val_auc: 0.9990\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 43s 406ms/step - loss: 0.0020 - tp: 353680.0000 - fp: 680.0000 - tn: 708066.0000 - fn: 693.0000 - accuracy: 0.6689 - precision: 0.9981 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0371 - val_tp: 103014.0000 - val_fp: 1246.0000 - val_tn: 207284.0000 - val_fn: 1251.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9880 - val_auc: 0.9988\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 43s 408ms/step - loss: 0.0017 - tp: 353778.0000 - fp: 583.0000 - tn: 708163.0000 - fn: 595.0000 - accuracy: 0.6690 - precision: 0.9984 - recall: 0.9983 - auc: 0.9999 - val_loss: 0.0405 - val_tp: 102971.0000 - val_fp: 1280.0000 - val_tn: 207250.0000 - val_fn: 1294.0000 - val_accuracy: 0.9918 - val_precision: 0.9877 - val_recall: 0.9876 - val_auc: 0.9986\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 43s 406ms/step - loss: 0.0013 - tp: 353888.0000 - fp: 477.0000 - tn: 708269.0000 - fn: 485.0000 - accuracy: 0.6691 - precision: 0.9987 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0432 - val_tp: 103011.0000 - val_fp: 1251.0000 - val_tn: 207279.0000 - val_fn: 1254.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9880 - val_auc: 0.9982\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 43s 403ms/step - loss: 0.0012 - tp: 353945.0000 - fp: 419.0000 - tn: 708327.0000 - fn: 428.0000 - accuracy: 0.6692 - precision: 0.9988 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0427 - val_tp: 102925.0000 - val_fp: 1336.0000 - val_tn: 207194.0000 - val_fn: 1340.0000 - val_accuracy: 0.9914 - val_precision: 0.9872 - val_recall: 0.9871 - val_auc: 0.9983\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 43s 408ms/step - loss: 9.0509e-04 - tp: 354032.0000 - fp: 338.0000 - tn: 708408.0000 - fn: 341.0000 - accuracy: 0.6693 - precision: 0.9990 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0478 - val_tp: 103023.0000 - val_fp: 1239.0000 - val_tn: 207291.0000 - val_fn: 1242.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9977\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 43s 408ms/step - loss: 7.5209e-04 - tp: 354125.0000 - fp: 243.0000 - tn: 708503.0000 - fn: 248.0000 - accuracy: 0.6694 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0499 - val_tp: 103005.0000 - val_fp: 1254.0000 - val_tn: 207276.0000 - val_fn: 1260.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9879 - val_auc: 0.9975\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 6.1700e-04 - tp: 354170.0000 - fp: 202.0000 - tn: 708544.0000 - fn: 203.0000 - accuracy: 0.6694 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 6.1700e-04 - tp: 354170.0000 - fp: 202.0000 - tn: 708544.0000 - fn: 203.0000 - accuracy: 0.6694 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0548 - val_tp: 103027.0000 - val_fp: 1236.0000 - val_tn: 207294.0000 - val_fn: 1238.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9970\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_2class/BiLSTM_2class_downweight_0.2.h5\n",
            "\n",
            "Down-weighting: 0.3\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 53s 425ms/step - loss: 0.0588 - tp: 435356.0000 - fp: 15898.0000 - tn: 901378.0000 - fn: 23282.0000 - accuracy: 0.7300 - precision: 0.9648 - recall: 0.9492 - auc: 0.9959 - val_loss: 0.0475 - val_tp: 102999.0000 - val_fp: 1233.0000 - val_tn: 207297.0000 - val_fn: 1266.0000 - val_accuracy: 0.9920 - val_precision: 0.9882 - val_recall: 0.9879 - val_auc: 0.9993\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0213 - tp: 350969.0000 - fp: 3146.0000 - tn: 705600.0000 - fn: 3404.0000 - accuracy: 0.6640 - precision: 0.9911 - recall: 0.9904 - auc: 0.9997 - val_loss: 0.0363 - val_tp: 102952.0000 - val_fp: 1298.0000 - val_tn: 207232.0000 - val_fn: 1313.0000 - val_accuracy: 0.9917 - val_precision: 0.9875 - val_recall: 0.9874 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0110 - tp: 352073.0000 - fp: 2176.0000 - tn: 706570.0000 - fn: 2300.0000 - accuracy: 0.6665 - precision: 0.9939 - recall: 0.9935 - auc: 0.9999 - val_loss: 0.0300 - val_tp: 103007.0000 - val_fp: 1248.0000 - val_tn: 207282.0000 - val_fn: 1258.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9879 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0053 - tp: 353181.0000 - fp: 1136.0000 - tn: 707610.0000 - fn: 1192.0000 - accuracy: 0.6683 - precision: 0.9968 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0327 - val_tp: 103044.0000 - val_fp: 1207.0000 - val_tn: 207323.0000 - val_fn: 1221.0000 - val_accuracy: 0.9922 - val_precision: 0.9884 - val_recall: 0.9883 - val_auc: 0.9993\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0035 - tp: 353580.0000 - fp: 757.0000 - tn: 707989.0000 - fn: 793.0000 - accuracy: 0.6687 - precision: 0.9979 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0362 - val_tp: 103045.0000 - val_fp: 1218.0000 - val_tn: 207312.0000 - val_fn: 1220.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9883 - val_auc: 0.9990\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0026 - tp: 353753.0000 - fp: 598.0000 - tn: 708148.0000 - fn: 620.0000 - accuracy: 0.6689 - precision: 0.9983 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0388 - val_tp: 103041.0000 - val_fp: 1223.0000 - val_tn: 207307.0000 - val_fn: 1224.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9883 - val_auc: 0.9988\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0022 - tp: 353820.0000 - fp: 541.0000 - tn: 708205.0000 - fn: 553.0000 - accuracy: 0.6690 - precision: 0.9985 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0420 - val_tp: 102994.0000 - val_fp: 1266.0000 - val_tn: 207264.0000 - val_fn: 1271.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9878 - val_auc: 0.9984\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 43s 408ms/step - loss: 0.0018 - tp: 353906.0000 - fp: 460.0000 - tn: 708286.0000 - fn: 467.0000 - accuracy: 0.6691 - precision: 0.9987 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0442 - val_tp: 103013.0000 - val_fp: 1251.0000 - val_tn: 207279.0000 - val_fn: 1252.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9880 - val_auc: 0.9981\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0015 - tp: 354003.0000 - fp: 363.0000 - tn: 708383.0000 - fn: 370.0000 - accuracy: 0.6692 - precision: 0.9990 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0441 - val_tp: 103006.0000 - val_fp: 1254.0000 - val_tn: 207276.0000 - val_fn: 1259.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9879 - val_auc: 0.9981\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0012 - tp: 354077.0000 - fp: 291.0000 - tn: 708455.0000 - fn: 296.0000 - accuracy: 0.6693 - precision: 0.9992 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0496 - val_tp: 103052.0000 - val_fp: 1212.0000 - val_tn: 207318.0000 - val_fn: 1213.0000 - val_accuracy: 0.9922 - val_precision: 0.9884 - val_recall: 0.9884 - val_auc: 0.9976\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 9.9650e-04 - tp: 354130.0000 - fp: 240.0000 - tn: 708506.0000 - fn: 243.0000 - accuracy: 0.6694 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0554 - val_tp: 103044.0000 - val_fp: 1220.0000 - val_tn: 207310.0000 - val_fn: 1221.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9883 - val_auc: 0.9970\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 8.2772e-04 - tp: 354180.0000 - fp: 191.0000 - tn: 708555.0000 - fn: 193.0000 - accuracy: 0.6694 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 8.2772e-04 - tp: 354180.0000 - fp: 191.0000 - tn: 708555.0000 - fn: 193.0000 - accuracy: 0.6694 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0596 - val_tp: 103058.0000 - val_fp: 1205.0000 - val_tn: 207325.0000 - val_fn: 1207.0000 - val_accuracy: 0.9923 - val_precision: 0.9884 - val_recall: 0.9884 - val_auc: 0.9966\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_2class/BiLSTM_2class_downweight_0.3.h5\n",
            "\n",
            "Down-weighting: 0.4\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 54s 426ms/step - loss: 0.0725 - tp: 435984.0000 - fp: 16026.0000 - tn: 901250.0000 - fn: 22654.0000 - accuracy: 0.7299 - precision: 0.9645 - recall: 0.9506 - auc: 0.9958 - val_loss: 0.0468 - val_tp: 103011.0000 - val_fp: 1238.0000 - val_tn: 207292.0000 - val_fn: 1254.0000 - val_accuracy: 0.9920 - val_precision: 0.9881 - val_recall: 0.9880 - val_auc: 0.9992\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0247 - tp: 351011.0000 - fp: 3166.0000 - tn: 705580.0000 - fn: 3362.0000 - accuracy: 0.6638 - precision: 0.9911 - recall: 0.9905 - auc: 0.9996 - val_loss: 0.0365 - val_tp: 103004.0000 - val_fp: 1250.0000 - val_tn: 207280.0000 - val_fn: 1261.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9879 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0134 - tp: 352054.0000 - fp: 2177.0000 - tn: 706569.0000 - fn: 2319.0000 - accuracy: 0.6661 - precision: 0.9939 - recall: 0.9935 - auc: 0.9999 - val_loss: 0.0309 - val_tp: 103022.0000 - val_fp: 1234.0000 - val_tn: 207296.0000 - val_fn: 1243.0000 - val_accuracy: 0.9921 - val_precision: 0.9882 - val_recall: 0.9881 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 44s 415ms/step - loss: 0.0064 - tp: 353216.0000 - fp: 1104.0000 - tn: 707642.0000 - fn: 1157.0000 - accuracy: 0.6682 - precision: 0.9969 - recall: 0.9967 - auc: 0.9999 - val_loss: 0.0340 - val_tp: 103051.0000 - val_fp: 1204.0000 - val_tn: 207326.0000 - val_fn: 1214.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9884 - val_auc: 0.9992\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 44s 417ms/step - loss: 0.0042 - tp: 353610.0000 - fp: 737.0000 - tn: 708009.0000 - fn: 763.0000 - accuracy: 0.6687 - precision: 0.9979 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0378 - val_tp: 103053.0000 - val_fp: 1209.0000 - val_tn: 207321.0000 - val_fn: 1212.0000 - val_accuracy: 0.9923 - val_precision: 0.9884 - val_recall: 0.9884 - val_auc: 0.9989\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 44s 415ms/step - loss: 0.0031 - tp: 353783.0000 - fp: 569.0000 - tn: 708177.0000 - fn: 590.0000 - accuracy: 0.6689 - precision: 0.9984 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0407 - val_tp: 103061.0000 - val_fp: 1203.0000 - val_tn: 207327.0000 - val_fn: 1204.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9987\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 44s 414ms/step - loss: 0.0026 - tp: 353873.0000 - fp: 482.0000 - tn: 708264.0000 - fn: 500.0000 - accuracy: 0.6690 - precision: 0.9986 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0437 - val_tp: 103004.0000 - val_fp: 1248.0000 - val_tn: 207282.0000 - val_fn: 1261.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9879 - val_auc: 0.9983\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 43s 408ms/step - loss: 0.0022 - tp: 353963.0000 - fp: 403.0000 - tn: 708343.0000 - fn: 410.0000 - accuracy: 0.6691 - precision: 0.9989 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0466 - val_tp: 103038.0000 - val_fp: 1225.0000 - val_tn: 207305.0000 - val_fn: 1227.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9882 - val_auc: 0.9979\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0017 - tp: 354049.0000 - fp: 318.0000 - tn: 708428.0000 - fn: 324.0000 - accuracy: 0.6692 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0469 - val_tp: 103015.0000 - val_fp: 1246.0000 - val_tn: 207284.0000 - val_fn: 1250.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9880 - val_auc: 0.9979\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0014 - tp: 354097.0000 - fp: 270.0000 - tn: 708476.0000 - fn: 276.0000 - accuracy: 0.6693 - precision: 0.9992 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0524 - val_tp: 103053.0000 - val_fp: 1211.0000 - val_tn: 207319.0000 - val_fn: 1212.0000 - val_accuracy: 0.9923 - val_precision: 0.9884 - val_recall: 0.9884 - val_auc: 0.9973\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 44s 415ms/step - loss: 0.0012 - tp: 354142.0000 - fp: 226.0000 - tn: 708520.0000 - fn: 231.0000 - accuracy: 0.6694 - precision: 0.9994 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0588 - val_tp: 103063.0000 - val_fp: 1200.0000 - val_tn: 207330.0000 - val_fn: 1202.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9966\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 9.6332e-04 - tp: 354201.0000 - fp: 168.0000 - tn: 708578.0000 - fn: 172.0000 - accuracy: 0.6694 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 9.6332e-04 - tp: 354201.0000 - fp: 168.0000 - tn: 708578.0000 - fn: 172.0000 - accuracy: 0.6694 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0615 - val_tp: 103074.0000 - val_fp: 1190.0000 - val_tn: 207340.0000 - val_fn: 1191.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9886 - val_auc: 0.9964\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_2class/BiLSTM_2class_downweight_0.4.h5\n",
            "\n",
            "Down-weighting: 0.5\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 53s 427ms/step - loss: 0.0857 - tp: 436327.0000 - fp: 16088.0000 - tn: 901188.0000 - fn: 22311.0000 - accuracy: 0.7299 - precision: 0.9644 - recall: 0.9514 - auc: 0.9957 - val_loss: 0.0469 - val_tp: 103021.0000 - val_fp: 1239.0000 - val_tn: 207291.0000 - val_fn: 1244.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9992\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0275 - tp: 351034.0000 - fp: 3214.0000 - tn: 705532.0000 - fn: 3339.0000 - accuracy: 0.6637 - precision: 0.9909 - recall: 0.9906 - auc: 0.9996 - val_loss: 0.0370 - val_tp: 103020.0000 - val_fp: 1242.0000 - val_tn: 207288.0000 - val_fn: 1245.0000 - val_accuracy: 0.9920 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0157 - tp: 351904.0000 - fp: 2310.0000 - tn: 706436.0000 - fn: 2469.0000 - accuracy: 0.6656 - precision: 0.9935 - recall: 0.9930 - auc: 0.9999 - val_loss: 0.0316 - val_tp: 103052.0000 - val_fp: 1205.0000 - val_tn: 207325.0000 - val_fn: 1213.0000 - val_accuracy: 0.9923 - val_precision: 0.9884 - val_recall: 0.9884 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 43s 408ms/step - loss: 0.0076 - tp: 353188.0000 - fp: 1123.0000 - tn: 707623.0000 - fn: 1185.0000 - accuracy: 0.6680 - precision: 0.9968 - recall: 0.9967 - auc: 0.9999 - val_loss: 0.0348 - val_tp: 103051.0000 - val_fp: 1203.0000 - val_tn: 207327.0000 - val_fn: 1214.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9884 - val_auc: 0.9992\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0049 - tp: 353612.0000 - fp: 726.0000 - tn: 708020.0000 - fn: 761.0000 - accuracy: 0.6687 - precision: 0.9980 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0397 - val_tp: 103061.0000 - val_fp: 1201.0000 - val_tn: 207329.0000 - val_fn: 1204.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9988\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0036 - tp: 353788.0000 - fp: 561.0000 - tn: 708185.0000 - fn: 585.0000 - accuracy: 0.6689 - precision: 0.9984 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0425 - val_tp: 103060.0000 - val_fp: 1205.0000 - val_tn: 207325.0000 - val_fn: 1205.0000 - val_accuracy: 0.9923 - val_precision: 0.9884 - val_recall: 0.9884 - val_auc: 0.9985\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 44s 415ms/step - loss: 0.0030 - tp: 353869.0000 - fp: 480.0000 - tn: 708266.0000 - fn: 504.0000 - accuracy: 0.6690 - precision: 0.9986 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0451 - val_tp: 103008.0000 - val_fp: 1243.0000 - val_tn: 207287.0000 - val_fn: 1257.0000 - val_accuracy: 0.9920 - val_precision: 0.9881 - val_recall: 0.9879 - val_auc: 0.9981\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0024 - tp: 353970.0000 - fp: 392.0000 - tn: 708354.0000 - fn: 403.0000 - accuracy: 0.6691 - precision: 0.9989 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0482 - val_tp: 103032.0000 - val_fp: 1231.0000 - val_tn: 207299.0000 - val_fn: 1233.0000 - val_accuracy: 0.9921 - val_precision: 0.9882 - val_recall: 0.9882 - val_auc: 0.9977\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 44s 417ms/step - loss: 0.0019 - tp: 354045.0000 - fp: 317.0000 - tn: 708429.0000 - fn: 328.0000 - accuracy: 0.6692 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0482 - val_tp: 103014.0000 - val_fp: 1246.0000 - val_tn: 207284.0000 - val_fn: 1251.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9880 - val_auc: 0.9978\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0016 - tp: 354113.0000 - fp: 252.0000 - tn: 708494.0000 - fn: 260.0000 - accuracy: 0.6693 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0532 - val_tp: 103047.0000 - val_fp: 1216.0000 - val_tn: 207314.0000 - val_fn: 1218.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9883 - val_auc: 0.9972\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 44s 412ms/step - loss: 0.0013 - tp: 354148.0000 - fp: 219.0000 - tn: 708527.0000 - fn: 225.0000 - accuracy: 0.6693 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0565 - val_tp: 103036.0000 - val_fp: 1225.0000 - val_tn: 207305.0000 - val_fn: 1229.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9882 - val_auc: 0.9969\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0011 - tp: 354197.0000 - fp: 173.0000 - tn: 708573.0000 - fn: 176.0000 - accuracy: 0.6694 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0011 - tp: 354197.0000 - fp: 173.0000 - tn: 708573.0000 - fn: 176.0000 - accuracy: 0.6694 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0653 - val_tp: 103065.0000 - val_fp: 1197.0000 - val_tn: 207333.0000 - val_fn: 1200.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9960\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_2class/BiLSTM_2class_downweight_0.5.h5\n",
            "\n",
            "Down-weighting: 0.6\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 55s 431ms/step - loss: 0.0986 - tp: 436536.0000 - fp: 16161.0000 - tn: 901115.0000 - fn: 22102.0000 - accuracy: 0.7298 - precision: 0.9643 - recall: 0.9518 - auc: 0.9956 - val_loss: 0.0472 - val_tp: 103021.0000 - val_fp: 1239.0000 - val_tn: 207291.0000 - val_fn: 1244.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9992\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 43s 408ms/step - loss: 0.0299 - tp: 351047.0000 - fp: 3229.0000 - tn: 705517.0000 - fn: 3326.0000 - accuracy: 0.6636 - precision: 0.9909 - recall: 0.9906 - auc: 0.9995 - val_loss: 0.0377 - val_tp: 103023.0000 - val_fp: 1240.0000 - val_tn: 207290.0000 - val_fn: 1242.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0177 - tp: 351758.0000 - fp: 2450.0000 - tn: 706296.0000 - fn: 2615.0000 - accuracy: 0.6652 - precision: 0.9931 - recall: 0.9926 - auc: 0.9999 - val_loss: 0.0325 - val_tp: 103056.0000 - val_fp: 1195.0000 - val_tn: 207335.0000 - val_fn: 1209.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9884 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 43s 406ms/step - loss: 0.0087 - tp: 353096.0000 - fp: 1212.0000 - tn: 707534.0000 - fn: 1277.0000 - accuracy: 0.6678 - precision: 0.9966 - recall: 0.9964 - auc: 0.9999 - val_loss: 0.0354 - val_tp: 103067.0000 - val_fp: 1186.0000 - val_tn: 207344.0000 - val_fn: 1198.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9885 - val_auc: 0.9992\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0056 - tp: 353605.0000 - fp: 729.0000 - tn: 708017.0000 - fn: 768.0000 - accuracy: 0.6686 - precision: 0.9979 - recall: 0.9978 - auc: 0.9999 - val_loss: 0.0412 - val_tp: 103067.0000 - val_fp: 1194.0000 - val_tn: 207336.0000 - val_fn: 1198.0000 - val_accuracy: 0.9924 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9988\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0041 - tp: 353773.0000 - fp: 576.0000 - tn: 708170.0000 - fn: 600.0000 - accuracy: 0.6688 - precision: 0.9984 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0436 - val_tp: 103066.0000 - val_fp: 1198.0000 - val_tn: 207332.0000 - val_fn: 1199.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9984\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 44s 414ms/step - loss: 0.0034 - tp: 353857.0000 - fp: 483.0000 - tn: 708263.0000 - fn: 516.0000 - accuracy: 0.6689 - precision: 0.9986 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0460 - val_tp: 103037.0000 - val_fp: 1224.0000 - val_tn: 207306.0000 - val_fn: 1228.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9882 - val_auc: 0.9980\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 44s 412ms/step - loss: 0.0028 - tp: 353963.0000 - fp: 393.0000 - tn: 708353.0000 - fn: 410.0000 - accuracy: 0.6691 - precision: 0.9989 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0496 - val_tp: 103046.0000 - val_fp: 1216.0000 - val_tn: 207314.0000 - val_fn: 1219.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9883 - val_auc: 0.9975\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 43s 408ms/step - loss: 0.0022 - tp: 354054.0000 - fp: 306.0000 - tn: 708440.0000 - fn: 319.0000 - accuracy: 0.6692 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0493 - val_tp: 103035.0000 - val_fp: 1227.0000 - val_tn: 207303.0000 - val_fn: 1230.0000 - val_accuracy: 0.9921 - val_precision: 0.9882 - val_recall: 0.9882 - val_auc: 0.9976\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0018 - tp: 354103.0000 - fp: 264.0000 - tn: 708482.0000 - fn: 270.0000 - accuracy: 0.6693 - precision: 0.9993 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0533 - val_tp: 103055.0000 - val_fp: 1209.0000 - val_tn: 207321.0000 - val_fn: 1210.0000 - val_accuracy: 0.9923 - val_precision: 0.9884 - val_recall: 0.9884 - val_auc: 0.9971\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0015 - tp: 354127.0000 - fp: 236.0000 - tn: 708510.0000 - fn: 246.0000 - accuracy: 0.6693 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0596 - val_tp: 103043.0000 - val_fp: 1222.0000 - val_tn: 207308.0000 - val_fn: 1222.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9883 - val_auc: 0.9965\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0012 - tp: 354204.0000 - fp: 164.0000 - tn: 708582.0000 - fn: 169.0000 - accuracy: 0.6694 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0669 - val_tp: 103068.0000 - val_fp: 1196.0000 - val_tn: 207334.0000 - val_fn: 1197.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9958\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0010 - tp: 354230.0000 - fp: 140.0000 - tn: 708606.0000 - fn: 143.0000 - accuracy: 0.6694 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0010 - tp: 354230.0000 - fp: 140.0000 - tn: 708606.0000 - fn: 143.0000 - accuracy: 0.6694 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0653 - val_tp: 103054.0000 - val_fp: 1209.0000 - val_tn: 207321.0000 - val_fn: 1211.0000 - val_accuracy: 0.9923 - val_precision: 0.9884 - val_recall: 0.9884 - val_auc: 0.9960\n",
            "Epoch 00013: early stopping\n",
            "Model saved at BiLSTM_2class/BiLSTM_2class_downweight_0.6.h5\n",
            "\n",
            "Down-weighting: 0.7\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 53s 426ms/step - loss: 0.1113 - tp: 436638.0000 - fp: 16213.0000 - tn: 901063.0000 - fn: 22000.0000 - accuracy: 0.7298 - precision: 0.9642 - recall: 0.9520 - auc: 0.9956 - val_loss: 0.0478 - val_tp: 103022.0000 - val_fp: 1239.0000 - val_tn: 207291.0000 - val_fn: 1243.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9991\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 43s 408ms/step - loss: 0.0321 - tp: 351052.0000 - fp: 3237.0000 - tn: 705509.0000 - fn: 3321.0000 - accuracy: 0.6636 - precision: 0.9909 - recall: 0.9906 - auc: 0.9995 - val_loss: 0.0385 - val_tp: 103024.0000 - val_fp: 1240.0000 - val_tn: 207290.0000 - val_fn: 1241.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 46s 432ms/step - loss: 0.0196 - tp: 351602.0000 - fp: 2588.0000 - tn: 706158.0000 - fn: 2771.0000 - accuracy: 0.6648 - precision: 0.9927 - recall: 0.9922 - auc: 0.9999 - val_loss: 0.0333 - val_tp: 103049.0000 - val_fp: 1211.0000 - val_tn: 207319.0000 - val_fn: 1216.0000 - val_accuracy: 0.9922 - val_precision: 0.9884 - val_recall: 0.9883 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0099 - tp: 353011.0000 - fp: 1273.0000 - tn: 707473.0000 - fn: 1362.0000 - accuracy: 0.6676 - precision: 0.9964 - recall: 0.9962 - auc: 0.9999 - val_loss: 0.0358 - val_tp: 103082.0000 - val_fp: 1178.0000 - val_tn: 207352.0000 - val_fn: 1183.0000 - val_accuracy: 0.9925 - val_precision: 0.9887 - val_recall: 0.9887 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 44s 414ms/step - loss: 0.0062 - tp: 353603.0000 - fp: 727.0000 - tn: 708019.0000 - fn: 770.0000 - accuracy: 0.6686 - precision: 0.9979 - recall: 0.9978 - auc: 0.9999 - val_loss: 0.0425 - val_tp: 103074.0000 - val_fp: 1189.0000 - val_tn: 207341.0000 - val_fn: 1191.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9886 - val_auc: 0.9987\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 44s 415ms/step - loss: 0.0045 - tp: 353787.0000 - fp: 562.0000 - tn: 708184.0000 - fn: 586.0000 - accuracy: 0.6688 - precision: 0.9984 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0448 - val_tp: 103070.0000 - val_fp: 1192.0000 - val_tn: 207338.0000 - val_fn: 1195.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9885 - val_auc: 0.9983\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 44s 414ms/step - loss: 0.0037 - tp: 353873.0000 - fp: 478.0000 - tn: 708268.0000 - fn: 500.0000 - accuracy: 0.6689 - precision: 0.9987 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0476 - val_tp: 103048.0000 - val_fp: 1211.0000 - val_tn: 207319.0000 - val_fn: 1217.0000 - val_accuracy: 0.9922 - val_precision: 0.9884 - val_recall: 0.9883 - val_auc: 0.9978\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 44s 412ms/step - loss: 0.0030 - tp: 353957.0000 - fp: 404.0000 - tn: 708342.0000 - fn: 416.0000 - accuracy: 0.6691 - precision: 0.9989 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0506 - val_tp: 103052.0000 - val_fp: 1211.0000 - val_tn: 207319.0000 - val_fn: 1213.0000 - val_accuracy: 0.9923 - val_precision: 0.9884 - val_recall: 0.9884 - val_auc: 0.9974\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0024 - tp: 354067.0000 - fp: 295.0000 - tn: 708451.0000 - fn: 306.0000 - accuracy: 0.6692 - precision: 0.9992 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0510 - val_tp: 103036.0000 - val_fp: 1225.0000 - val_tn: 207305.0000 - val_fn: 1229.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9882 - val_auc: 0.9974\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0020 - tp: 354097.0000 - fp: 262.0000 - tn: 708484.0000 - fn: 276.0000 - accuracy: 0.6693 - precision: 0.9993 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0546 - val_tp: 103059.0000 - val_fp: 1206.0000 - val_tn: 207324.0000 - val_fn: 1206.0000 - val_accuracy: 0.9923 - val_precision: 0.9884 - val_recall: 0.9884 - val_auc: 0.9970\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0017 - tp: 354134.0000 - fp: 235.0000 - tn: 708511.0000 - fn: 239.0000 - accuracy: 0.6693 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0576 - val_tp: 103020.0000 - val_fp: 1237.0000 - val_tn: 207293.0000 - val_fn: 1245.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9968\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 44s 414ms/step - loss: 0.0014 - tp: 354200.0000 - fp: 171.0000 - tn: 708575.0000 - fn: 173.0000 - accuracy: 0.6694 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0675 - val_tp: 103071.0000 - val_fp: 1193.0000 - val_tn: 207337.0000 - val_fn: 1194.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9885 - val_auc: 0.9957\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0012 - tp: 354221.0000 - fp: 148.0000 - tn: 708598.0000 - fn: 152.0000 - accuracy: 0.6694 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 44s 412ms/step - loss: 0.0012 - tp: 354221.0000 - fp: 148.0000 - tn: 708598.0000 - fn: 152.0000 - accuracy: 0.6694 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0628 - val_tp: 103052.0000 - val_fp: 1209.0000 - val_tn: 207321.0000 - val_fn: 1213.0000 - val_accuracy: 0.9923 - val_precision: 0.9884 - val_recall: 0.9884 - val_auc: 0.9964\n",
            "Epoch 00013: early stopping\n",
            "Model saved at BiLSTM_2class/BiLSTM_2class_downweight_0.7.h5\n",
            "\n",
            "Down-weighting: 0.8\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 54s 433ms/step - loss: 0.1239 - tp: 436737.0000 - fp: 16247.0000 - tn: 901029.0000 - fn: 21901.0000 - accuracy: 0.7298 - precision: 0.9641 - recall: 0.9522 - auc: 0.9956 - val_loss: 0.0484 - val_tp: 103021.0000 - val_fp: 1239.0000 - val_tn: 207291.0000 - val_fn: 1244.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9991\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 44s 417ms/step - loss: 0.0340 - tp: 351057.0000 - fp: 3247.0000 - tn: 705499.0000 - fn: 3316.0000 - accuracy: 0.6636 - precision: 0.9908 - recall: 0.9906 - auc: 0.9995 - val_loss: 0.0393 - val_tp: 103025.0000 - val_fp: 1239.0000 - val_tn: 207291.0000 - val_fn: 1240.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 44s 417ms/step - loss: 0.0214 - tp: 351494.0000 - fp: 2723.0000 - tn: 706023.0000 - fn: 2879.0000 - accuracy: 0.6645 - precision: 0.9923 - recall: 0.9919 - auc: 0.9998 - val_loss: 0.0342 - val_tp: 103042.0000 - val_fp: 1218.0000 - val_tn: 207312.0000 - val_fn: 1223.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9883 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 44s 417ms/step - loss: 0.0110 - tp: 352892.0000 - fp: 1393.0000 - tn: 707353.0000 - fn: 1481.0000 - accuracy: 0.6673 - precision: 0.9961 - recall: 0.9958 - auc: 0.9999 - val_loss: 0.0365 - val_tp: 103091.0000 - val_fp: 1169.0000 - val_tn: 207361.0000 - val_fn: 1174.0000 - val_accuracy: 0.9925 - val_precision: 0.9888 - val_recall: 0.9887 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 44s 414ms/step - loss: 0.0067 - tp: 353593.0000 - fp: 735.0000 - tn: 708011.0000 - fn: 780.0000 - accuracy: 0.6685 - precision: 0.9979 - recall: 0.9978 - auc: 0.9999 - val_loss: 0.0434 - val_tp: 103081.0000 - val_fp: 1183.0000 - val_tn: 207347.0000 - val_fn: 1184.0000 - val_accuracy: 0.9924 - val_precision: 0.9887 - val_recall: 0.9886 - val_auc: 0.9986\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 44s 414ms/step - loss: 0.0049 - tp: 353785.0000 - fp: 558.0000 - tn: 708188.0000 - fn: 588.0000 - accuracy: 0.6688 - precision: 0.9984 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0457 - val_tp: 103075.0000 - val_fp: 1188.0000 - val_tn: 207342.0000 - val_fn: 1190.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9886 - val_auc: 0.9983\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0040 - tp: 353875.0000 - fp: 476.0000 - tn: 708270.0000 - fn: 498.0000 - accuracy: 0.6689 - precision: 0.9987 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0486 - val_tp: 103063.0000 - val_fp: 1201.0000 - val_tn: 207329.0000 - val_fn: 1202.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9977\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 44s 414ms/step - loss: 0.0033 - tp: 353951.0000 - fp: 408.0000 - tn: 708338.0000 - fn: 422.0000 - accuracy: 0.6690 - precision: 0.9988 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0533 - val_tp: 103067.0000 - val_fp: 1197.0000 - val_tn: 207333.0000 - val_fn: 1198.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9971\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0026 - tp: 354062.0000 - fp: 300.0000 - tn: 708446.0000 - fn: 311.0000 - accuracy: 0.6692 - precision: 0.9992 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0518 - val_tp: 103050.0000 - val_fp: 1212.0000 - val_tn: 207318.0000 - val_fn: 1215.0000 - val_accuracy: 0.9922 - val_precision: 0.9884 - val_recall: 0.9883 - val_auc: 0.9973\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 44s 418ms/step - loss: 0.0022 - tp: 354105.0000 - fp: 256.0000 - tn: 708490.0000 - fn: 268.0000 - accuracy: 0.6693 - precision: 0.9993 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0558 - val_tp: 103064.0000 - val_fp: 1199.0000 - val_tn: 207331.0000 - val_fn: 1201.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9969\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 44s 416ms/step - loss: 0.0018 - tp: 354146.0000 - fp: 223.0000 - tn: 708523.0000 - fn: 227.0000 - accuracy: 0.6693 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0603 - val_tp: 103044.0000 - val_fp: 1220.0000 - val_tn: 207310.0000 - val_fn: 1221.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9883 - val_auc: 0.9965\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0015 - tp: 354192.0000 - fp: 178.0000 - tn: 708568.0000 - fn: 181.0000 - accuracy: 0.6694 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0693 - val_tp: 103072.0000 - val_fp: 1191.0000 - val_tn: 207339.0000 - val_fn: 1193.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9886 - val_auc: 0.9955\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0013 - tp: 354219.0000 - fp: 151.0000 - tn: 708595.0000 - fn: 154.0000 - accuracy: 0.6694 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0013 - tp: 354219.0000 - fp: 151.0000 - tn: 708595.0000 - fn: 154.0000 - accuracy: 0.6694 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0673 - val_tp: 103063.0000 - val_fp: 1201.0000 - val_tn: 207329.0000 - val_fn: 1202.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9958\n",
            "Epoch 00013: early stopping\n",
            "Model saved at BiLSTM_2class/BiLSTM_2class_downweight_0.8.h5\n",
            "\n",
            "Down-weighting: 0.9\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 54s 427ms/step - loss: 0.1364 - tp: 436822.0000 - fp: 16274.0000 - tn: 901002.0000 - fn: 21816.0000 - accuracy: 0.7297 - precision: 0.9641 - recall: 0.9524 - auc: 0.9955 - val_loss: 0.0492 - val_tp: 103022.0000 - val_fp: 1239.0000 - val_tn: 207291.0000 - val_fn: 1243.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9991\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0358 - tp: 351052.0000 - fp: 3252.0000 - tn: 705494.0000 - fn: 3321.0000 - accuracy: 0.6636 - precision: 0.9908 - recall: 0.9906 - auc: 0.9994 - val_loss: 0.0401 - val_tp: 103026.0000 - val_fp: 1239.0000 - val_tn: 207291.0000 - val_fn: 1239.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 44s 414ms/step - loss: 0.0230 - tp: 351391.0000 - fp: 2839.0000 - tn: 705907.0000 - fn: 2982.0000 - accuracy: 0.6643 - precision: 0.9920 - recall: 0.9916 - auc: 0.9998 - val_loss: 0.0351 - val_tp: 103035.0000 - val_fp: 1227.0000 - val_tn: 207303.0000 - val_fn: 1230.0000 - val_accuracy: 0.9921 - val_precision: 0.9882 - val_recall: 0.9882 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0121 - tp: 352794.0000 - fp: 1488.0000 - tn: 707258.0000 - fn: 1579.0000 - accuracy: 0.6671 - precision: 0.9958 - recall: 0.9955 - auc: 0.9999 - val_loss: 0.0369 - val_tp: 103086.0000 - val_fp: 1172.0000 - val_tn: 207358.0000 - val_fn: 1179.0000 - val_accuracy: 0.9925 - val_precision: 0.9888 - val_recall: 0.9887 - val_auc: 0.9991\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 44s 412ms/step - loss: 0.0073 - tp: 353579.0000 - fp: 745.0000 - tn: 708001.0000 - fn: 794.0000 - accuracy: 0.6685 - precision: 0.9979 - recall: 0.9978 - auc: 0.9999 - val_loss: 0.0442 - val_tp: 103079.0000 - val_fp: 1183.0000 - val_tn: 207347.0000 - val_fn: 1186.0000 - val_accuracy: 0.9924 - val_precision: 0.9887 - val_recall: 0.9886 - val_auc: 0.9985\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0053 - tp: 353779.0000 - fp: 565.0000 - tn: 708181.0000 - fn: 594.0000 - accuracy: 0.6688 - precision: 0.9984 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0465 - val_tp: 103080.0000 - val_fp: 1184.0000 - val_tn: 207346.0000 - val_fn: 1185.0000 - val_accuracy: 0.9924 - val_precision: 0.9886 - val_recall: 0.9886 - val_auc: 0.9982\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 44s 411ms/step - loss: 0.0043 - tp: 353854.0000 - fp: 494.0000 - tn: 708252.0000 - fn: 519.0000 - accuracy: 0.6689 - precision: 0.9986 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0494 - val_tp: 103069.0000 - val_fp: 1194.0000 - val_tn: 207336.0000 - val_fn: 1196.0000 - val_accuracy: 0.9924 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9976\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 44s 415ms/step - loss: 0.0035 - tp: 353946.0000 - fp: 413.0000 - tn: 708333.0000 - fn: 427.0000 - accuracy: 0.6690 - precision: 0.9988 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0534 - val_tp: 103070.0000 - val_fp: 1194.0000 - val_tn: 207336.0000 - val_fn: 1195.0000 - val_accuracy: 0.9924 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9971\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 44s 415ms/step - loss: 0.0028 - tp: 354070.0000 - fp: 293.0000 - tn: 708453.0000 - fn: 303.0000 - accuracy: 0.6692 - precision: 0.9992 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0539 - val_tp: 103059.0000 - val_fp: 1203.0000 - val_tn: 207327.0000 - val_fn: 1206.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9884 - val_auc: 0.9971\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 44s 416ms/step - loss: 0.0024 - tp: 354099.0000 - fp: 260.0000 - tn: 708486.0000 - fn: 274.0000 - accuracy: 0.6692 - precision: 0.9993 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0574 - val_tp: 103065.0000 - val_fp: 1199.0000 - val_tn: 207331.0000 - val_fn: 1200.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9967\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0019 - tp: 354145.0000 - fp: 223.0000 - tn: 708523.0000 - fn: 228.0000 - accuracy: 0.6693 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0604 - val_tp: 103044.0000 - val_fp: 1218.0000 - val_tn: 207312.0000 - val_fn: 1221.0000 - val_accuracy: 0.9922 - val_precision: 0.9883 - val_recall: 0.9883 - val_auc: 0.9965\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0017 - tp: 354187.0000 - fp: 182.0000 - tn: 708564.0000 - fn: 186.0000 - accuracy: 0.6694 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0692 - val_tp: 103071.0000 - val_fp: 1194.0000 - val_tn: 207336.0000 - val_fn: 1194.0000 - val_accuracy: 0.9924 - val_precision: 0.9885 - val_recall: 0.9885 - val_auc: 0.9955\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0014 - tp: 354221.0000 - fp: 148.0000 - tn: 708598.0000 - fn: 152.0000 - accuracy: 0.6694 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0014 - tp: 354221.0000 - fp: 148.0000 - tn: 708598.0000 - fn: 152.0000 - accuracy: 0.6694 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0680 - val_tp: 103060.0000 - val_fp: 1204.0000 - val_tn: 207326.0000 - val_fn: 1205.0000 - val_accuracy: 0.9923 - val_precision: 0.9885 - val_recall: 0.9884 - val_auc: 0.9957\n",
            "Epoch 00013: early stopping\n",
            "Model saved at BiLSTM_2class/BiLSTM_2class_downweight_0.9.h5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2EHdu_7dw9B"
      },
      "source": [
        "def reverse_bo(ind):\n",
        "    bo = 'O'  # for any pad=2 predictions\n",
        "    if ind == 0:\n",
        "        bo = 'B'\n",
        "    elif ind == 1:\n",
        "        bo = 'O'\n",
        "    return bo\n",
        "\n",
        "def reverse_bio_from_bo(inds):\n",
        "    bo = [reverse_bo(i) for i in inds]\n",
        "    bio = copy.deepcopy(bo)\n",
        "    for i in range(len(bo)):\n",
        "        if i >= 1 and bo[i] == 'B' and bo[i - 1] == 'B':\n",
        "            bio[i] = 'I'\n",
        "    return bio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gjs05zGcjiY",
        "outputId": "66065a41-a53b-42a8-8d8b-adcf4a9993d8"
      },
      "source": [
        "downweight_models[1.0] = model\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n",
        "    preds = np.argmax(downweight_models[weight].predict(dev_seqs_bo_padded), axis=-1)\n",
        "\n",
        "    dev_seqs_bo['prediction'] = ''\n",
        "    for i in dev_seqs_bo.index:\n",
        "        this_seq_length = len(dev_seqs_bo['token'][i])\n",
        "        dev_seqs_bo['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    dev_long = dev_seqs_bo.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = reverse_bio_from_bo(dev_long['bio_only'])\n",
        "    dev_long['bio_only'] = bio_labs\n",
        "    pred_labs = reverse_bio_from_bo(dev_long['prediction'])\n",
        "    dev_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(dev_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 0.1:\n",
            "Sum of TP and FP = 710\n",
            "Sum of TP and FN = 805\n",
            "True positives = 166, False positives = 544, False negatives = 639\n",
            "Precision = 0.234, Recall = 0.206, F1 = 0.219\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 391\n",
            "Sum of TP and FN = 805\n",
            "True positives = 73, False positives = 318, False negatives = 732\n",
            "Precision = 0.187, Recall = 0.091, F1 = 0.122\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 108\n",
            "Sum of TP and FN = 805\n",
            "True positives = 14, False positives = 94, False negatives = 791\n",
            "Precision = 0.130, Recall = 0.017, F1 = 0.031\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 29\n",
            "Sum of TP and FN = 805\n",
            "True positives = 1, False positives = 28, False negatives = 804\n",
            "Precision = 0.034, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 4\n",
            "Sum of TP and FN = 805\n",
            "True positives = 0, False positives = 4, False negatives = 805\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 78\n",
            "Sum of TP and FN = 805\n",
            "True positives = 23, False positives = 55, False negatives = 782\n",
            "Precision = 0.295, Recall = 0.029, F1 = 0.052\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 54\n",
            "Sum of TP and FN = 805\n",
            "True positives = 13, False positives = 41, False negatives = 792\n",
            "Precision = 0.241, Recall = 0.016, F1 = 0.030\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 38\n",
            "Sum of TP and FN = 805\n",
            "True positives = 10, False positives = 28, False negatives = 795\n",
            "Precision = 0.263, Recall = 0.012, F1 = 0.024\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 16\n",
            "Sum of TP and FN = 805\n",
            "True positives = 5, False positives = 11, False negatives = 800\n",
            "Precision = 0.312, Recall = 0.006, F1 = 0.012\n",
            "\n",
            "Weight = 1.0:\n",
            "Sum of TP and FP = 5\n",
            "Sum of TP and FN = 805\n",
            "True positives = 1, False positives = 4, False negatives = 804\n",
            "Precision = 0.200, Recall = 0.001, F1 = 0.002\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMj7LjWVikBs"
      },
      "source": [
        "#### Down-sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wZHi4Y4sijm9",
        "outputId": "d5bf5b3e-7d8f-4c0f-d66d-f8ea1b7615e6"
      },
      "source": [
        "mask = train_seqs.bio_only.apply(lambda labels: 0.0 in labels)\n",
        "train_seqs_downsampled = train_seqs_bo[mask]\n",
        "train_seqs_downsampled.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[@paulwalk, It, 's, the, view, from, where, I,...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[From, Green, Newsfeed, :, AHFA, extends, dead...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Pxleyes, Top, 50, Photography, Contest, Pictu...</td>\n",
              "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[4Dbling, 's, place, til, monday, ,, party, pa...</td>\n",
              "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>[watching, the, VMA, pre-show, again, lol, it,...</td>\n",
              "      <td>[1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[65.0, 3.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                      token_indices\n",
              "0             0  ...  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...\n",
              "1             1  ...  [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
              "2             2  ...  [39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....\n",
              "4             4  ...  [57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...\n",
              "5             5  ...  [65.0, 3.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLsyo5J8lHoM",
        "outputId": "8e21740f-f3ae-4122-fedc-aef835b510eb"
      },
      "source": [
        "train_seqs_downsampled_padded = pad_sequences(train_seqs_downsampled['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                              dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "train_labs_downsampled_padded = pad_sequences(train_seqs_downsampled['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                              dtype='int32', padding='post', truncating='post', value=padlab_bo)\n",
        "train_labs_downsampled_onehot = [to_categorical(i, num_classes=n_labs_bo) for i in train_labs_downsampled_padded]\n",
        "\n",
        "# follow the print outputs below to see how the labels are transformed\n",
        "print('Example of padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(train_seqs_downsampled.loc[1])\n",
        "print('Length of input sequence: %i' % len(train_seqs_downsampled_padded[1]))\n",
        "print('Length of label sequence: %i' % len(train_labs_downsampled_onehot[1]))\n",
        "print(train_labs_downsampled_padded[1][:11])\n",
        "print(train_labs_downsampled_onehot[1][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     1\n",
            "token            [From, Green, Newsfeed, :, AHFA, extends, dead...\n",
            "bio_only         [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...\n",
            "token_indices    [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
            "Name: 1, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of label sequence: 105\n",
            "[1 1 1 1 0 1 1 1 1 1 1]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK5ENztDlikO",
        "outputId": "e1e949fb-6a41-4d42-d533-58fae9bfc9d5"
      },
      "source": [
        "# figure out the label distribution in our downsampled fixed-length texts\n",
        "all_labs = [l for lab in train_labs_downsampled_padded for l in lab]\n",
        "label_count = Counter(all_labs)\n",
        "total_labs = len(all_labs)\n",
        "print(label_count)\n",
        "print(total_labs)\n",
        "\n",
        "# use this to define an initial model bias\n",
        "initial_bias = [(label_count[0]/total_labs), (label_count[1]/total_labs), (label_count[2]/total_labs)]\n",
        "print('Initial bias:')\n",
        "print(initial_bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({2: 103017, 1: 22152, 0: 3141})\n",
            "128310\n",
            "Initial bias:\n",
            "[0.02447977554360533, 0.17264437689969606, 0.8028758475566986]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISlJBVKQlsEm",
        "outputId": "47b9563f-8b86-431b-c167-3b741bfb89aa"
      },
      "source": [
        "# prepare sequences and labels as numpy arrays, check dimensions\n",
        "X = np.array(train_seqs_downsampled_padded)\n",
        "y = np.array(train_labs_downsampled_onehot)\n",
        "print('Input sequence dimensions (n.docs, seq.length):')\n",
        "print(X.shape)\n",
        "print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence dimensions (n.docs, seq.length):\n",
            "(1222, 105)\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):\n",
            "(1222, 105, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUjt9C_Mlzyd",
        "outputId": "4925cb29-70c6-46c6-af28-b42b36c958e8"
      },
      "source": [
        "# re-initiate model with bias\n",
        "model = make_model_bo(output_bias=initial_bias)\n",
        "\n",
        "# and fit...\n",
        "model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(dev_X, dev_y))\n",
        "\n",
        "# save the model\n",
        "model.save('BiLSTM_2class_downsampled/BiLSTM_2class_downsampled.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "39/39 [==============================] - 24s 380ms/step - loss: 0.3438 - tp: 107553.0000 - fp: 13148.0000 - tn: 1306587.0000 - fn: 375128.0000 - accuracy: 0.7930 - precision: 0.8911 - recall: 0.2228 - auc: 0.9643 - val_loss: 0.0940 - val_tp: 101972.0000 - val_fp: 1451.0000 - val_tn: 207079.0000 - val_fn: 2293.0000 - val_accuracy: 0.9880 - val_precision: 0.9860 - val_recall: 0.9780 - val_auc: 0.9981\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 13s 335ms/step - loss: 0.1049 - tp: 124518.0000 - fp: 3405.0000 - tn: 253211.0000 - fn: 3790.0000 - accuracy: 0.9813 - precision: 0.9734 - recall: 0.9705 - auc: 0.9964 - val_loss: 0.0534 - val_tp: 102951.0000 - val_fp: 1253.0000 - val_tn: 207277.0000 - val_fn: 1314.0000 - val_accuracy: 0.9918 - val_precision: 0.9880 - val_recall: 0.9874 - val_auc: 0.9991\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 14s 348ms/step - loss: 0.0811 - tp: 124997.0000 - fp: 3196.0000 - tn: 253420.0000 - fn: 3311.0000 - accuracy: 0.9831 - precision: 0.9751 - recall: 0.9742 - auc: 0.9978 - val_loss: 0.0457 - val_tp: 102972.0000 - val_fp: 1253.0000 - val_tn: 207277.0000 - val_fn: 1293.0000 - val_accuracy: 0.9919 - val_precision: 0.9880 - val_recall: 0.9876 - val_auc: 0.9994\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 13s 332ms/step - loss: 0.0700 - tp: 125097.0000 - fp: 3156.0000 - tn: 253460.0000 - fn: 3211.0000 - accuracy: 0.9835 - precision: 0.9754 - recall: 0.9750 - auc: 0.9986 - val_loss: 0.0405 - val_tp: 102990.0000 - val_fp: 1242.0000 - val_tn: 207288.0000 - val_fn: 1275.0000 - val_accuracy: 0.9920 - val_precision: 0.9881 - val_recall: 0.9878 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 13s 338ms/step - loss: 0.0545 - tp: 125399.0000 - fp: 2815.0000 - tn: 253801.0000 - fn: 2909.0000 - accuracy: 0.9851 - precision: 0.9780 - recall: 0.9773 - auc: 0.9992 - val_loss: 0.0348 - val_tp: 102978.0000 - val_fp: 1250.0000 - val_tn: 207280.0000 - val_fn: 1287.0000 - val_accuracy: 0.9919 - val_precision: 0.9880 - val_recall: 0.9877 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 13s 330ms/step - loss: 0.0365 - tp: 126524.0000 - fp: 1664.0000 - tn: 254952.0000 - fn: 1784.0000 - accuracy: 0.9910 - precision: 0.9870 - recall: 0.9861 - auc: 0.9997 - val_loss: 0.0321 - val_tp: 102988.0000 - val_fp: 1233.0000 - val_tn: 207297.0000 - val_fn: 1277.0000 - val_accuracy: 0.9920 - val_precision: 0.9882 - val_recall: 0.9878 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 13s 335ms/step - loss: 0.0229 - tp: 127433.0000 - fp: 819.0000 - tn: 255797.0000 - fn: 875.0000 - accuracy: 0.9956 - precision: 0.9936 - recall: 0.9932 - auc: 0.9998 - val_loss: 0.0322 - val_tp: 103000.0000 - val_fp: 1226.0000 - val_tn: 207304.0000 - val_fn: 1265.0000 - val_accuracy: 0.9920 - val_precision: 0.9882 - val_recall: 0.9879 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 13s 341ms/step - loss: 0.0159 - tp: 127746.0000 - fp: 521.0000 - tn: 256095.0000 - fn: 562.0000 - accuracy: 0.9972 - precision: 0.9959 - recall: 0.9956 - auc: 0.9999 - val_loss: 0.0331 - val_tp: 102974.0000 - val_fp: 1256.0000 - val_tn: 207274.0000 - val_fn: 1291.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9876 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 13s 343ms/step - loss: 0.0125 - tp: 127843.0000 - fp: 433.0000 - tn: 256183.0000 - fn: 465.0000 - accuracy: 0.9977 - precision: 0.9966 - recall: 0.9964 - auc: 0.9999 - val_loss: 0.0345 - val_tp: 103010.0000 - val_fp: 1222.0000 - val_tn: 207308.0000 - val_fn: 1255.0000 - val_accuracy: 0.9921 - val_precision: 0.9883 - val_recall: 0.9880 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 13s 334ms/step - loss: 0.0108 - tp: 127872.0000 - fp: 398.0000 - tn: 256218.0000 - fn: 436.0000 - accuracy: 0.9978 - precision: 0.9969 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0359 - val_tp: 102994.0000 - val_fp: 1255.0000 - val_tn: 207275.0000 - val_fn: 1271.0000 - val_accuracy: 0.9919 - val_precision: 0.9880 - val_recall: 0.9878 - val_auc: 0.9991\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 13s 330ms/step - loss: 0.0093 - tp: 127943.0000 - fp: 342.0000 - tn: 256274.0000 - fn: 365.0000 - accuracy: 0.9982 - precision: 0.9973 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0364 - val_tp: 102987.0000 - val_fp: 1263.0000 - val_tn: 207267.0000 - val_fn: 1278.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9877 - val_auc: 0.9991\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 13s 340ms/step - loss: 0.0082 - tp: 127979.0000 - fp: 305.0000 - tn: 256311.0000 - fn: 329.0000 - accuracy: 0.9984 - precision: 0.9976 - recall: 0.9974 - auc: 0.9999 - val_loss: 0.0380 - val_tp: 102962.0000 - val_fp: 1280.0000 - val_tn: 207250.0000 - val_fn: 1303.0000 - val_accuracy: 0.9917 - val_precision: 0.9877 - val_recall: 0.9875 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 13s 332ms/step - loss: 0.0074 - tp: 128003.0000 - fp: 282.0000 - tn: 256334.0000 - fn: 305.0000 - accuracy: 0.9985 - precision: 0.9978 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0388 - val_tp: 102953.0000 - val_fp: 1291.0000 - val_tn: 207239.0000 - val_fn: 1312.0000 - val_accuracy: 0.9917 - val_precision: 0.9876 - val_recall: 0.9874 - val_auc: 0.9989\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 14s 355ms/step - loss: 0.0063 - tp: 128071.0000 - fp: 220.0000 - tn: 256396.0000 - fn: 237.0000 - accuracy: 0.9988 - precision: 0.9983 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0403 - val_tp: 102948.0000 - val_fp: 1291.0000 - val_tn: 207239.0000 - val_fn: 1317.0000 - val_accuracy: 0.9917 - val_precision: 0.9876 - val_recall: 0.9874 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0056 - tp: 128087.0000 - fp: 207.0000 - tn: 256409.0000 - fn: 221.0000 - accuracy: 0.9989 - precision: 0.9984 - recall: 0.9983 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 13s 338ms/step - loss: 0.0056 - tp: 128087.0000 - fp: 207.0000 - tn: 256409.0000 - fn: 221.0000 - accuracy: 0.9989 - precision: 0.9984 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0423 - val_tp: 102952.0000 - val_fp: 1293.0000 - val_tn: 207237.0000 - val_fn: 1313.0000 - val_accuracy: 0.9917 - val_precision: 0.9876 - val_recall: 0.9874 - val_auc: 0.9986\n",
            "Epoch 00015: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDRdGgGfmHp5",
        "outputId": "50067f4b-7215-454a-99c8-c3e76bafba5d"
      },
      "source": [
        "# now try the weighted one-hot encoding\n",
        "downweight_models = {}\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9]:\n",
        "    train_weights_onehot = down_weight_bo(train_labs_downsampled_onehot, weight)\n",
        "    y = np.array(train_weights_onehot)\n",
        "    print('Down-weighting:', weight)\n",
        "    print('Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels):', np.shape(y))\n",
        "\n",
        "    downweight_model = make_model_bo(output_bias=initial_bias)\n",
        "    downweight_model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(dev_X, dev_y))\n",
        "\n",
        "    downweight_models[weight] = downweight_model\n",
        "    downweight_model.save(f'BiLSTM_2class_downsampled/BiLSTM_2class_downsampled_downweight_{weight}.h5')\n",
        "    print(f'Model saved at BiLSTM_2class_downsampled/BiLSTM_2class_downsampled_downweight_{weight}.h5')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Down-weighting: 0.1\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 22s 359ms/step - loss: 0.0655 - tp: 205463.0000 - fp: 13063.0000 - tn: 452083.0000 - fn: 27110.0000 - accuracy: 0.7957 - precision: 0.9402 - recall: 0.8834 - auc: 0.9886 - val_loss: 0.1624 - val_tp: 88286.0000 - val_fp: 7888.0000 - val_tn: 200642.0000 - val_fn: 15979.0000 - val_accuracy: 0.9237 - val_precision: 0.9180 - val_recall: 0.8467 - val_auc: 0.9896\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0317 - tp: 109552.0000 - fp: 14699.0000 - tn: 241917.0000 - fn: 18756.0000 - accuracy: 0.6344 - precision: 0.8817 - recall: 0.8538 - auc: 0.9854 - val_loss: 0.1407 - val_tp: 90476.0000 - val_fp: 11677.0000 - val_tn: 196853.0000 - val_fn: 13789.0000 - val_accuracy: 0.9186 - val_precision: 0.8857 - val_recall: 0.8678 - val_auc: 0.9903\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 12s 317ms/step - loss: 0.0268 - tp: 114056.0000 - fp: 12562.0000 - tn: 244054.0000 - fn: 14252.0000 - accuracy: 0.6408 - precision: 0.9008 - recall: 0.8889 - auc: 0.9896 - val_loss: 0.0985 - val_tp: 98459.0000 - val_fp: 4905.0000 - val_tn: 203625.0000 - val_fn: 5806.0000 - val_accuracy: 0.9658 - val_precision: 0.9525 - val_recall: 0.9443 - val_auc: 0.9969\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 12s 312ms/step - loss: 0.0195 - tp: 120632.0000 - fp: 6959.0000 - tn: 249657.0000 - fn: 7676.0000 - accuracy: 0.6559 - precision: 0.9455 - recall: 0.9402 - auc: 0.9958 - val_loss: 0.0697 - val_tp: 100637.0000 - val_fp: 3247.0000 - val_tn: 205283.0000 - val_fn: 3628.0000 - val_accuracy: 0.9780 - val_precision: 0.9687 - val_recall: 0.9652 - val_auc: 0.9986\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 12s 314ms/step - loss: 0.0118 - tp: 124546.0000 - fp: 3453.0000 - tn: 253163.0000 - fn: 3762.0000 - accuracy: 0.6654 - precision: 0.9730 - recall: 0.9707 - auc: 0.9987 - val_loss: 0.0529 - val_tp: 101516.0000 - val_fp: 2583.0000 - val_tn: 205947.0000 - val_fn: 2749.0000 - val_accuracy: 0.9830 - val_precision: 0.9752 - val_recall: 0.9736 - val_auc: 0.9992\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 13s 324ms/step - loss: 0.0068 - tp: 126456.0000 - fp: 1755.0000 - tn: 254861.0000 - fn: 1852.0000 - accuracy: 0.6700 - precision: 0.9863 - recall: 0.9856 - auc: 0.9995 - val_loss: 0.0453 - val_tp: 101896.0000 - val_fp: 2249.0000 - val_tn: 206281.0000 - val_fn: 2369.0000 - val_accuracy: 0.9852 - val_precision: 0.9784 - val_recall: 0.9773 - val_auc: 0.9994\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 13s 323ms/step - loss: 0.0044 - tp: 127204.0000 - fp: 1063.0000 - tn: 255553.0000 - fn: 1104.0000 - accuracy: 0.6719 - precision: 0.9917 - recall: 0.9914 - auc: 0.9998 - val_loss: 0.0454 - val_tp: 101982.0000 - val_fp: 2194.0000 - val_tn: 206336.0000 - val_fn: 2283.0000 - val_accuracy: 0.9857 - val_precision: 0.9789 - val_recall: 0.9781 - val_auc: 0.9993\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 13s 325ms/step - loss: 0.0032 - tp: 127480.0000 - fp: 811.0000 - tn: 255805.0000 - fn: 828.0000 - accuracy: 0.6726 - precision: 0.9937 - recall: 0.9935 - auc: 0.9998 - val_loss: 0.0383 - val_tp: 102472.0000 - val_fp: 1716.0000 - val_tn: 206814.0000 - val_fn: 1793.0000 - val_accuracy: 0.9888 - val_precision: 0.9835 - val_recall: 0.9828 - val_auc: 0.9994\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 13s 322ms/step - loss: 0.0025 - tp: 127712.0000 - fp: 589.0000 - tn: 256027.0000 - fn: 596.0000 - accuracy: 0.6732 - precision: 0.9954 - recall: 0.9954 - auc: 0.9998 - val_loss: 0.0424 - val_tp: 102302.0000 - val_fp: 1896.0000 - val_tn: 206634.0000 - val_fn: 1963.0000 - val_accuracy: 0.9877 - val_precision: 0.9818 - val_recall: 0.9812 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 12s 317ms/step - loss: 0.0020 - tp: 127811.0000 - fp: 491.0000 - tn: 256125.0000 - fn: 497.0000 - accuracy: 0.6735 - precision: 0.9962 - recall: 0.9961 - auc: 0.9999 - val_loss: 0.0436 - val_tp: 102324.0000 - val_fp: 1874.0000 - val_tn: 206656.0000 - val_fn: 1941.0000 - val_accuracy: 0.9878 - val_precision: 0.9820 - val_recall: 0.9814 - val_auc: 0.9991\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 12s 319ms/step - loss: 0.0016 - tp: 127872.0000 - fp: 430.0000 - tn: 256186.0000 - fn: 436.0000 - accuracy: 0.6737 - precision: 0.9966 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0425 - val_tp: 102461.0000 - val_fp: 1741.0000 - val_tn: 206789.0000 - val_fn: 1804.0000 - val_accuracy: 0.9887 - val_precision: 0.9833 - val_recall: 0.9827 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0014 - tp: 127960.0000 - fp: 341.0000 - tn: 256275.0000 - fn: 348.0000 - accuracy: 0.6739 - precision: 0.9973 - recall: 0.9973 - auc: 0.9999 - val_loss: 0.0444 - val_tp: 102464.0000 - val_fp: 1750.0000 - val_tn: 206780.0000 - val_fn: 1801.0000 - val_accuracy: 0.9886 - val_precision: 0.9832 - val_recall: 0.9827 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0012 - tp: 127992.0000 - fp: 315.0000 - tn: 256301.0000 - fn: 316.0000 - accuracy: 0.6740 - precision: 0.9975 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0473 - val_tp: 102333.0000 - val_fp: 1873.0000 - val_tn: 206657.0000 - val_fn: 1932.0000 - val_accuracy: 0.9878 - val_precision: 0.9820 - val_recall: 0.9815 - val_auc: 0.9987\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0011 - tp: 128058.0000 - fp: 248.0000 - tn: 256368.0000 - fn: 250.0000 - accuracy: 0.6742 - precision: 0.9981 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0468 - val_tp: 102430.0000 - val_fp: 1797.0000 - val_tn: 206733.0000 - val_fn: 1835.0000 - val_accuracy: 0.9884 - val_precision: 0.9828 - val_recall: 0.9824 - val_auc: 0.9987\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - 12s 312ms/step - loss: 0.0010 - tp: 128068.0000 - fp: 235.0000 - tn: 256381.0000 - fn: 240.0000 - accuracy: 0.6742 - precision: 0.9982 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0486 - val_tp: 102395.0000 - val_fp: 1825.0000 - val_tn: 206705.0000 - val_fn: 1870.0000 - val_accuracy: 0.9882 - val_precision: 0.9825 - val_recall: 0.9821 - val_auc: 0.9986\n",
            "Epoch 16/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 8.4887e-04 - tp: 128078.0000 - fp: 229.0000 - tn: 256387.0000 - fn: 230.0000 - accuracy: 0.6742 - precision: 0.9982 - recall: 0.9982 - auc: 0.9999Restoring model weights from the end of the best epoch: 6.\n",
            "39/39 [==============================] - 12s 314ms/step - loss: 8.4887e-04 - tp: 128078.0000 - fp: 229.0000 - tn: 256387.0000 - fn: 230.0000 - accuracy: 0.6742 - precision: 0.9982 - recall: 0.9982 - auc: 0.9999 - val_loss: 0.0485 - val_tp: 102498.0000 - val_fp: 1735.0000 - val_tn: 206795.0000 - val_fn: 1767.0000 - val_accuracy: 0.9888 - val_precision: 0.9834 - val_recall: 0.9831 - val_auc: 0.9985\n",
            "Epoch 00016: early stopping\n",
            "Model saved at BiLSTM_2class_downsampled/BiLSTM_2class_downsampled_downweight_0.1.h5\n",
            "\n",
            "Down-weighting: 0.2\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 22s 357ms/step - loss: 0.0999 - tp: 206917.0000 - fp: 13529.0000 - tn: 451617.0000 - fn: 25656.0000 - accuracy: 0.7942 - precision: 0.9386 - recall: 0.8897 - auc: 0.9890 - val_loss: 0.1272 - val_tp: 97165.0000 - val_fp: 624.0000 - val_tn: 207906.0000 - val_fn: 7100.0000 - val_accuracy: 0.9753 - val_precision: 0.9936 - val_recall: 0.9319 - val_auc: 0.9988\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 12s 313ms/step - loss: 0.0466 - tp: 119648.0000 - fp: 4517.0000 - tn: 252099.0000 - fn: 8660.0000 - accuracy: 0.6561 - precision: 0.9636 - recall: 0.9325 - auc: 0.9964 - val_loss: 0.0935 - val_tp: 101316.0000 - val_fp: 1980.0000 - val_tn: 206550.0000 - val_fn: 2949.0000 - val_accuracy: 0.9842 - val_precision: 0.9808 - val_recall: 0.9717 - val_auc: 0.9991\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 12s 313ms/step - loss: 0.0378 - tp: 122479.0000 - fp: 4807.0000 - tn: 251809.0000 - fn: 5829.0000 - accuracy: 0.6574 - precision: 0.9622 - recall: 0.9546 - auc: 0.9975 - val_loss: 0.0678 - val_tp: 102116.0000 - val_fp: 1680.0000 - val_tn: 206850.0000 - val_fn: 2149.0000 - val_accuracy: 0.9878 - val_precision: 0.9838 - val_recall: 0.9794 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 12s 318ms/step - loss: 0.0277 - tp: 124250.0000 - fp: 3642.0000 - tn: 252974.0000 - fn: 4058.0000 - accuracy: 0.6630 - precision: 0.9715 - recall: 0.9684 - auc: 0.9986 - val_loss: 0.0504 - val_tp: 102006.0000 - val_fp: 2016.0000 - val_tn: 206514.0000 - val_fn: 2259.0000 - val_accuracy: 0.9863 - val_precision: 0.9806 - val_recall: 0.9783 - val_auc: 0.9994\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 12s 311ms/step - loss: 0.0166 - tp: 126153.0000 - fp: 1972.0000 - tn: 254644.0000 - fn: 2155.0000 - accuracy: 0.6688 - precision: 0.9846 - recall: 0.9832 - auc: 0.9995 - val_loss: 0.0417 - val_tp: 102085.0000 - val_fp: 1956.0000 - val_tn: 206574.0000 - val_fn: 2180.0000 - val_accuracy: 0.9868 - val_precision: 0.9812 - val_recall: 0.9791 - val_auc: 0.9995\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 12s 317ms/step - loss: 0.0095 - tp: 127158.0000 - fp: 1065.0000 - tn: 255551.0000 - fn: 1150.0000 - accuracy: 0.6716 - precision: 0.9917 - recall: 0.9910 - auc: 0.9998 - val_loss: 0.0375 - val_tp: 102323.0000 - val_fp: 1812.0000 - val_tn: 206718.0000 - val_fn: 1942.0000 - val_accuracy: 0.9880 - val_precision: 0.9826 - val_recall: 0.9814 - val_auc: 0.9995\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 13s 330ms/step - loss: 0.0064 - tp: 127546.0000 - fp: 734.0000 - tn: 255882.0000 - fn: 762.0000 - accuracy: 0.6726 - precision: 0.9943 - recall: 0.9941 - auc: 0.9999 - val_loss: 0.0374 - val_tp: 102384.0000 - val_fp: 1748.0000 - val_tn: 206782.0000 - val_fn: 1881.0000 - val_accuracy: 0.9884 - val_precision: 0.9832 - val_recall: 0.9820 - val_auc: 0.9994\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 12s 320ms/step - loss: 0.0049 - tp: 127686.0000 - fp: 606.0000 - tn: 256010.0000 - fn: 622.0000 - accuracy: 0.6730 - precision: 0.9953 - recall: 0.9952 - auc: 0.9999 - val_loss: 0.0364 - val_tp: 102521.0000 - val_fp: 1627.0000 - val_tn: 206903.0000 - val_fn: 1744.0000 - val_accuracy: 0.9892 - val_precision: 0.9844 - val_recall: 0.9833 - val_auc: 0.9994\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 12s 321ms/step - loss: 0.0038 - tp: 127827.0000 - fp: 476.0000 - tn: 256140.0000 - fn: 481.0000 - accuracy: 0.6734 - precision: 0.9963 - recall: 0.9963 - auc: 0.9999 - val_loss: 0.0371 - val_tp: 102556.0000 - val_fp: 1620.0000 - val_tn: 206910.0000 - val_fn: 1709.0000 - val_accuracy: 0.9894 - val_precision: 0.9844 - val_recall: 0.9836 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 12s 321ms/step - loss: 0.0031 - tp: 127927.0000 - fp: 371.0000 - tn: 256245.0000 - fn: 381.0000 - accuracy: 0.6738 - precision: 0.9971 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0388 - val_tp: 102548.0000 - val_fp: 1648.0000 - val_tn: 206882.0000 - val_fn: 1717.0000 - val_accuracy: 0.9892 - val_precision: 0.9842 - val_recall: 0.9835 - val_auc: 0.9991\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 12s 318ms/step - loss: 0.0026 - tp: 127980.0000 - fp: 323.0000 - tn: 256293.0000 - fn: 328.0000 - accuracy: 0.6739 - precision: 0.9975 - recall: 0.9974 - auc: 0.9999 - val_loss: 0.0388 - val_tp: 102673.0000 - val_fp: 1525.0000 - val_tn: 207005.0000 - val_fn: 1592.0000 - val_accuracy: 0.9900 - val_precision: 0.9854 - val_recall: 0.9847 - val_auc: 0.9989\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 12s 317ms/step - loss: 0.0023 - tp: 128017.0000 - fp: 286.0000 - tn: 256330.0000 - fn: 291.0000 - accuracy: 0.6740 - precision: 0.9978 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0407 - val_tp: 102602.0000 - val_fp: 1609.0000 - val_tn: 206921.0000 - val_fn: 1663.0000 - val_accuracy: 0.9895 - val_precision: 0.9846 - val_recall: 0.9841 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 12s 317ms/step - loss: 0.0020 - tp: 128060.0000 - fp: 241.0000 - tn: 256375.0000 - fn: 248.0000 - accuracy: 0.6741 - precision: 0.9981 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0424 - val_tp: 102581.0000 - val_fp: 1628.0000 - val_tn: 206902.0000 - val_fn: 1684.0000 - val_accuracy: 0.9894 - val_precision: 0.9844 - val_recall: 0.9838 - val_auc: 0.9988\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 12s 315ms/step - loss: 0.0017 - tp: 128100.0000 - fp: 207.0000 - tn: 256409.0000 - fn: 208.0000 - accuracy: 0.6742 - precision: 0.9984 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0429 - val_tp: 102595.0000 - val_fp: 1625.0000 - val_tn: 206905.0000 - val_fn: 1670.0000 - val_accuracy: 0.9895 - val_precision: 0.9844 - val_recall: 0.9840 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - 12s 314ms/step - loss: 0.0016 - tp: 128121.0000 - fp: 184.0000 - tn: 256432.0000 - fn: 187.0000 - accuracy: 0.6743 - precision: 0.9986 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0442 - val_tp: 102584.0000 - val_fp: 1637.0000 - val_tn: 206893.0000 - val_fn: 1681.0000 - val_accuracy: 0.9894 - val_precision: 0.9843 - val_recall: 0.9839 - val_auc: 0.9987\n",
            "Epoch 16/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0014 - tp: 128142.0000 - fp: 162.0000 - tn: 256454.0000 - fn: 166.0000 - accuracy: 0.6744 - precision: 0.9987 - recall: 0.9987 - auc: 1.0000Restoring model weights from the end of the best epoch: 6.\n",
            "39/39 [==============================] - 12s 314ms/step - loss: 0.0014 - tp: 128142.0000 - fp: 162.0000 - tn: 256454.0000 - fn: 166.0000 - accuracy: 0.6744 - precision: 0.9987 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0452 - val_tp: 102589.0000 - val_fp: 1628.0000 - val_tn: 206902.0000 - val_fn: 1676.0000 - val_accuracy: 0.9894 - val_precision: 0.9844 - val_recall: 0.9839 - val_auc: 0.9986\n",
            "Epoch 00016: early stopping\n",
            "Model saved at BiLSTM_2class_downsampled/BiLSTM_2class_downsampled_downweight_0.2.h5\n",
            "\n",
            "Down-weighting: 0.3\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 23s 378ms/step - loss: 0.1319 - tp: 208030.0000 - fp: 13836.0000 - tn: 451310.0000 - fn: 24543.0000 - accuracy: 0.7939 - precision: 0.9376 - recall: 0.8945 - auc: 0.9894 - val_loss: 0.1153 - val_tp: 99395.0000 - val_fp: 873.0000 - val_tn: 207657.0000 - val_fn: 4870.0000 - val_accuracy: 0.9816 - val_precision: 0.9913 - val_recall: 0.9533 - val_auc: 0.9987\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 13s 332ms/step - loss: 0.0572 - tp: 122941.0000 - fp: 3293.0000 - tn: 253323.0000 - fn: 5367.0000 - accuracy: 0.6583 - precision: 0.9739 - recall: 0.9582 - auc: 0.9974 - val_loss: 0.0739 - val_tp: 102719.0000 - val_fp: 1202.0000 - val_tn: 207328.0000 - val_fn: 1546.0000 - val_accuracy: 0.9912 - val_precision: 0.9884 - val_recall: 0.9852 - val_auc: 0.9994\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 12s 320ms/step - loss: 0.0459 - tp: 124397.0000 - fp: 3368.0000 - tn: 253248.0000 - fn: 3911.0000 - accuracy: 0.6592 - precision: 0.9736 - recall: 0.9695 - auc: 0.9984 - val_loss: 0.0577 - val_tp: 102736.0000 - val_fp: 1319.0000 - val_tn: 207211.0000 - val_fn: 1529.0000 - val_accuracy: 0.9909 - val_precision: 0.9873 - val_recall: 0.9853 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 12s 314ms/step - loss: 0.0346 - tp: 125253.0000 - fp: 2781.0000 - tn: 253835.0000 - fn: 3055.0000 - accuracy: 0.6637 - precision: 0.9783 - recall: 0.9762 - auc: 0.9991 - val_loss: 0.0444 - val_tp: 102496.0000 - val_fp: 1610.0000 - val_tn: 206920.0000 - val_fn: 1769.0000 - val_accuracy: 0.9892 - val_precision: 0.9845 - val_recall: 0.9830 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 12s 317ms/step - loss: 0.0215 - tp: 126566.0000 - fp: 1608.0000 - tn: 255008.0000 - fn: 1742.0000 - accuracy: 0.6692 - precision: 0.9875 - recall: 0.9864 - auc: 0.9996 - val_loss: 0.0378 - val_tp: 102402.0000 - val_fp: 1701.0000 - val_tn: 206829.0000 - val_fn: 1863.0000 - val_accuracy: 0.9886 - val_precision: 0.9837 - val_recall: 0.9821 - val_auc: 0.9996\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 12s 316ms/step - loss: 0.0124 - tp: 127321.0000 - fp: 911.0000 - tn: 255705.0000 - fn: 987.0000 - accuracy: 0.6718 - precision: 0.9929 - recall: 0.9923 - auc: 0.9998 - val_loss: 0.0341 - val_tp: 102613.0000 - val_fp: 1570.0000 - val_tn: 206960.0000 - val_fn: 1652.0000 - val_accuracy: 0.9897 - val_precision: 0.9849 - val_recall: 0.9842 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 12s 321ms/step - loss: 0.0084 - tp: 127625.0000 - fp: 648.0000 - tn: 255968.0000 - fn: 683.0000 - accuracy: 0.6727 - precision: 0.9949 - recall: 0.9947 - auc: 0.9999 - val_loss: 0.0339 - val_tp: 102637.0000 - val_fp: 1535.0000 - val_tn: 206995.0000 - val_fn: 1628.0000 - val_accuracy: 0.9899 - val_precision: 0.9853 - val_recall: 0.9844 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 12s 320ms/step - loss: 0.0064 - tp: 127760.0000 - fp: 521.0000 - tn: 256095.0000 - fn: 548.0000 - accuracy: 0.6731 - precision: 0.9959 - recall: 0.9957 - auc: 0.9999 - val_loss: 0.0348 - val_tp: 102636.0000 - val_fp: 1535.0000 - val_tn: 206995.0000 - val_fn: 1629.0000 - val_accuracy: 0.9899 - val_precision: 0.9853 - val_recall: 0.9844 - val_auc: 0.9994\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 13s 322ms/step - loss: 0.0051 - tp: 127860.0000 - fp: 432.0000 - tn: 256184.0000 - fn: 448.0000 - accuracy: 0.6734 - precision: 0.9966 - recall: 0.9965 - auc: 0.9999 - val_loss: 0.0353 - val_tp: 102670.0000 - val_fp: 1528.0000 - val_tn: 207002.0000 - val_fn: 1595.0000 - val_accuracy: 0.9900 - val_precision: 0.9853 - val_recall: 0.9847 - val_auc: 0.9993\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 13s 329ms/step - loss: 0.0043 - tp: 127943.0000 - fp: 353.0000 - tn: 256263.0000 - fn: 365.0000 - accuracy: 0.6737 - precision: 0.9972 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0367 - val_tp: 102660.0000 - val_fp: 1555.0000 - val_tn: 206975.0000 - val_fn: 1605.0000 - val_accuracy: 0.9899 - val_precision: 0.9851 - val_recall: 0.9846 - val_auc: 0.9991\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 13s 327ms/step - loss: 0.0036 - tp: 127984.0000 - fp: 314.0000 - tn: 256302.0000 - fn: 324.0000 - accuracy: 0.6739 - precision: 0.9976 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0372 - val_tp: 102721.0000 - val_fp: 1492.0000 - val_tn: 207038.0000 - val_fn: 1544.0000 - val_accuracy: 0.9903 - val_precision: 0.9857 - val_recall: 0.9852 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 13s 321ms/step - loss: 0.0033 - tp: 128021.0000 - fp: 280.0000 - tn: 256336.0000 - fn: 287.0000 - accuracy: 0.6739 - precision: 0.9978 - recall: 0.9978 - auc: 0.9999 - val_loss: 0.0387 - val_tp: 102646.0000 - val_fp: 1571.0000 - val_tn: 206959.0000 - val_fn: 1619.0000 - val_accuracy: 0.9898 - val_precision: 0.9849 - val_recall: 0.9845 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 12s 316ms/step - loss: 0.0028 - tp: 128058.0000 - fp: 245.0000 - tn: 256371.0000 - fn: 250.0000 - accuracy: 0.6741 - precision: 0.9981 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0402 - val_tp: 102637.0000 - val_fp: 1579.0000 - val_tn: 206951.0000 - val_fn: 1628.0000 - val_accuracy: 0.9897 - val_precision: 0.9848 - val_recall: 0.9844 - val_auc: 0.9989\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 12s 312ms/step - loss: 0.0023 - tp: 128113.0000 - fp: 191.0000 - tn: 256425.0000 - fn: 195.0000 - accuracy: 0.6742 - precision: 0.9985 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0403 - val_tp: 102709.0000 - val_fp: 1524.0000 - val_tn: 207006.0000 - val_fn: 1556.0000 - val_accuracy: 0.9902 - val_precision: 0.9854 - val_recall: 0.9851 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0022 - tp: 128139.0000 - fp: 164.0000 - tn: 256452.0000 - fn: 169.0000 - accuracy: 0.6743 - precision: 0.9987 - recall: 0.9987 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 12s 315ms/step - loss: 0.0022 - tp: 128139.0000 - fp: 164.0000 - tn: 256452.0000 - fn: 169.0000 - accuracy: 0.6743 - precision: 0.9987 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0412 - val_tp: 102707.0000 - val_fp: 1527.0000 - val_tn: 207003.0000 - val_fn: 1558.0000 - val_accuracy: 0.9901 - val_precision: 0.9854 - val_recall: 0.9851 - val_auc: 0.9988\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_2class_downsampled/BiLSTM_2class_downsampled_downweight_0.3.h5\n",
            "\n",
            "Down-weighting: 0.4\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 23s 382ms/step - loss: 0.1631 - tp: 208714.0000 - fp: 13978.0000 - tn: 451168.0000 - fn: 23859.0000 - accuracy: 0.7938 - precision: 0.9372 - recall: 0.8974 - auc: 0.9898 - val_loss: 0.1082 - val_tp: 100431.0000 - val_fp: 1076.0000 - val_tn: 207454.0000 - val_fn: 3834.0000 - val_accuracy: 0.9843 - val_precision: 0.9894 - val_recall: 0.9632 - val_auc: 0.9985\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 12s 311ms/step - loss: 0.0660 - tp: 123784.0000 - fp: 3201.0000 - tn: 253415.0000 - fn: 4524.0000 - accuracy: 0.6584 - precision: 0.9748 - recall: 0.9647 - auc: 0.9973 - val_loss: 0.0663 - val_tp: 102792.0000 - val_fp: 1234.0000 - val_tn: 207296.0000 - val_fn: 1473.0000 - val_accuracy: 0.9913 - val_precision: 0.9881 - val_recall: 0.9859 - val_auc: 0.9993\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0528 - tp: 124771.0000 - fp: 3170.0000 - tn: 253446.0000 - fn: 3537.0000 - accuracy: 0.6588 - precision: 0.9752 - recall: 0.9724 - auc: 0.9984 - val_loss: 0.0535 - val_tp: 102885.0000 - val_fp: 1243.0000 - val_tn: 207287.0000 - val_fn: 1380.0000 - val_accuracy: 0.9916 - val_precision: 0.9881 - val_recall: 0.9868 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 12s 313ms/step - loss: 0.0410 - tp: 125389.0000 - fp: 2702.0000 - tn: 253914.0000 - fn: 2919.0000 - accuracy: 0.6624 - precision: 0.9789 - recall: 0.9773 - auc: 0.9991 - val_loss: 0.0418 - val_tp: 102753.0000 - val_fp: 1422.0000 - val_tn: 207108.0000 - val_fn: 1512.0000 - val_accuracy: 0.9906 - val_precision: 0.9863 - val_recall: 0.9855 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0265 - tp: 126587.0000 - fp: 1617.0000 - tn: 254999.0000 - fn: 1721.0000 - accuracy: 0.6684 - precision: 0.9874 - recall: 0.9866 - auc: 0.9996 - val_loss: 0.0358 - val_tp: 102645.0000 - val_fp: 1519.0000 - val_tn: 207011.0000 - val_fn: 1620.0000 - val_accuracy: 0.9900 - val_precision: 0.9854 - val_recall: 0.9845 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 12s 317ms/step - loss: 0.0154 - tp: 127368.0000 - fp: 868.0000 - tn: 255748.0000 - fn: 940.0000 - accuracy: 0.6717 - precision: 0.9932 - recall: 0.9927 - auc: 0.9998 - val_loss: 0.0327 - val_tp: 102731.0000 - val_fp: 1476.0000 - val_tn: 207054.0000 - val_fn: 1534.0000 - val_accuracy: 0.9904 - val_precision: 0.9858 - val_recall: 0.9853 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 12s 316ms/step - loss: 0.0103 - tp: 127663.0000 - fp: 608.0000 - tn: 256008.0000 - fn: 645.0000 - accuracy: 0.6727 - precision: 0.9953 - recall: 0.9950 - auc: 0.9999 - val_loss: 0.0325 - val_tp: 102783.0000 - val_fp: 1428.0000 - val_tn: 207102.0000 - val_fn: 1482.0000 - val_accuracy: 0.9907 - val_precision: 0.9863 - val_recall: 0.9858 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 12s 315ms/step - loss: 0.0078 - tp: 127793.0000 - fp: 483.0000 - tn: 256133.0000 - fn: 515.0000 - accuracy: 0.6731 - precision: 0.9962 - recall: 0.9960 - auc: 0.9999 - val_loss: 0.0337 - val_tp: 102721.0000 - val_fp: 1475.0000 - val_tn: 207055.0000 - val_fn: 1544.0000 - val_accuracy: 0.9903 - val_precision: 0.9858 - val_recall: 0.9852 - val_auc: 0.9994\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 12s 315ms/step - loss: 0.0064 - tp: 127897.0000 - fp: 383.0000 - tn: 256233.0000 - fn: 411.0000 - accuracy: 0.6735 - precision: 0.9970 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0343 - val_tp: 102772.0000 - val_fp: 1429.0000 - val_tn: 207101.0000 - val_fn: 1493.0000 - val_accuracy: 0.9907 - val_precision: 0.9863 - val_recall: 0.9857 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 12s 312ms/step - loss: 0.0054 - tp: 127939.0000 - fp: 350.0000 - tn: 256266.0000 - fn: 369.0000 - accuracy: 0.6736 - precision: 0.9973 - recall: 0.9971 - auc: 0.9999 - val_loss: 0.0357 - val_tp: 102752.0000 - val_fp: 1467.0000 - val_tn: 207063.0000 - val_fn: 1513.0000 - val_accuracy: 0.9905 - val_precision: 0.9859 - val_recall: 0.9855 - val_auc: 0.9991\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 12s 312ms/step - loss: 0.0046 - tp: 127991.0000 - fp: 298.0000 - tn: 256318.0000 - fn: 317.0000 - accuracy: 0.6738 - precision: 0.9977 - recall: 0.9975 - auc: 1.0000 - val_loss: 0.0366 - val_tp: 102733.0000 - val_fp: 1484.0000 - val_tn: 207046.0000 - val_fn: 1532.0000 - val_accuracy: 0.9904 - val_precision: 0.9858 - val_recall: 0.9853 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 12s 314ms/step - loss: 0.0042 - tp: 128020.0000 - fp: 276.0000 - tn: 256340.0000 - fn: 288.0000 - accuracy: 0.6739 - precision: 0.9978 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0377 - val_tp: 102696.0000 - val_fp: 1530.0000 - val_tn: 207000.0000 - val_fn: 1569.0000 - val_accuracy: 0.9901 - val_precision: 0.9853 - val_recall: 0.9850 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 12s 311ms/step - loss: 0.0037 - tp: 128050.0000 - fp: 251.0000 - tn: 256365.0000 - fn: 258.0000 - accuracy: 0.6740 - precision: 0.9980 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0382 - val_tp: 102728.0000 - val_fp: 1502.0000 - val_tn: 207028.0000 - val_fn: 1537.0000 - val_accuracy: 0.9903 - val_precision: 0.9856 - val_recall: 0.9853 - val_auc: 0.9989\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 12s 311ms/step - loss: 0.0029 - tp: 128109.0000 - fp: 194.0000 - tn: 256422.0000 - fn: 199.0000 - accuracy: 0.6742 - precision: 0.9985 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0390 - val_tp: 102778.0000 - val_fp: 1452.0000 - val_tn: 207078.0000 - val_fn: 1487.0000 - val_accuracy: 0.9906 - val_precision: 0.9861 - val_recall: 0.9857 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0028 - tp: 128143.0000 - fp: 162.0000 - tn: 256454.0000 - fn: 165.0000 - accuracy: 0.6743 - precision: 0.9987 - recall: 0.9987 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0028 - tp: 128143.0000 - fp: 162.0000 - tn: 256454.0000 - fn: 165.0000 - accuracy: 0.6743 - precision: 0.9987 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0399 - val_tp: 102805.0000 - val_fp: 1430.0000 - val_tn: 207100.0000 - val_fn: 1460.0000 - val_accuracy: 0.9908 - val_precision: 0.9863 - val_recall: 0.9860 - val_auc: 0.9988\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_2class_downsampled/BiLSTM_2class_downsampled_downweight_0.4.h5\n",
            "\n",
            "Down-weighting: 0.5\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 22s 358ms/step - loss: 0.1938 - tp: 209252.0000 - fp: 14097.0000 - tn: 451049.0000 - fn: 23321.0000 - accuracy: 0.7938 - precision: 0.9369 - recall: 0.8997 - auc: 0.9899 - val_loss: 0.1033 - val_tp: 100913.0000 - val_fp: 1188.0000 - val_tn: 207342.0000 - val_fn: 3352.0000 - val_accuracy: 0.9855 - val_precision: 0.9884 - val_recall: 0.9679 - val_auc: 0.9984\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 12s 305ms/step - loss: 0.0739 - tp: 124114.0000 - fp: 3248.0000 - tn: 253368.0000 - fn: 4194.0000 - accuracy: 0.6582 - precision: 0.9745 - recall: 0.9673 - auc: 0.9972 - val_loss: 0.0619 - val_tp: 102837.0000 - val_fp: 1244.0000 - val_tn: 207286.0000 - val_fn: 1428.0000 - val_accuracy: 0.9915 - val_precision: 0.9880 - val_recall: 0.9863 - val_auc: 0.9993\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0588 - tp: 124865.0000 - fp: 3166.0000 - tn: 253450.0000 - fn: 3443.0000 - accuracy: 0.6585 - precision: 0.9753 - recall: 0.9732 - auc: 0.9983 - val_loss: 0.0510 - val_tp: 102939.0000 - val_fp: 1248.0000 - val_tn: 207282.0000 - val_fn: 1326.0000 - val_accuracy: 0.9918 - val_precision: 0.9880 - val_recall: 0.9873 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 12s 310ms/step - loss: 0.0470 - tp: 125304.0000 - fp: 2809.0000 - tn: 253807.0000 - fn: 3004.0000 - accuracy: 0.6609 - precision: 0.9781 - recall: 0.9766 - auc: 0.9991 - val_loss: 0.0405 - val_tp: 102866.0000 - val_fp: 1338.0000 - val_tn: 207192.0000 - val_fn: 1399.0000 - val_accuracy: 0.9912 - val_precision: 0.9872 - val_recall: 0.9866 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0316 - tp: 126437.0000 - fp: 1773.0000 - tn: 254843.0000 - fn: 1871.0000 - accuracy: 0.6670 - precision: 0.9862 - recall: 0.9854 - auc: 0.9996 - val_loss: 0.0348 - val_tp: 102797.0000 - val_fp: 1390.0000 - val_tn: 207140.0000 - val_fn: 1468.0000 - val_accuracy: 0.9909 - val_precision: 0.9867 - val_recall: 0.9859 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0187 - tp: 127338.0000 - fp: 898.0000 - tn: 255718.0000 - fn: 970.0000 - accuracy: 0.6713 - precision: 0.9930 - recall: 0.9924 - auc: 0.9998 - val_loss: 0.0320 - val_tp: 102831.0000 - val_fp: 1384.0000 - val_tn: 207146.0000 - val_fn: 1434.0000 - val_accuracy: 0.9910 - val_precision: 0.9867 - val_recall: 0.9862 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 12s 312ms/step - loss: 0.0122 - tp: 127673.0000 - fp: 594.0000 - tn: 256022.0000 - fn: 635.0000 - accuracy: 0.6726 - precision: 0.9954 - recall: 0.9951 - auc: 0.9999 - val_loss: 0.0319 - val_tp: 102884.0000 - val_fp: 1334.0000 - val_tn: 207196.0000 - val_fn: 1381.0000 - val_accuracy: 0.9913 - val_precision: 0.9872 - val_recall: 0.9868 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 12s 313ms/step - loss: 0.0091 - tp: 127813.0000 - fp: 462.0000 - tn: 256154.0000 - fn: 495.0000 - accuracy: 0.6731 - precision: 0.9964 - recall: 0.9961 - auc: 0.9999 - val_loss: 0.0330 - val_tp: 102824.0000 - val_fp: 1399.0000 - val_tn: 207131.0000 - val_fn: 1441.0000 - val_accuracy: 0.9909 - val_precision: 0.9866 - val_recall: 0.9862 - val_auc: 0.9994\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 13s 323ms/step - loss: 0.0074 - tp: 127905.0000 - fp: 375.0000 - tn: 256241.0000 - fn: 403.0000 - accuracy: 0.6734 - precision: 0.9971 - recall: 0.9969 - auc: 0.9999 - val_loss: 0.0338 - val_tp: 102850.0000 - val_fp: 1375.0000 - val_tn: 207155.0000 - val_fn: 1415.0000 - val_accuracy: 0.9911 - val_precision: 0.9868 - val_recall: 0.9864 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 12s 315ms/step - loss: 0.0064 - tp: 127931.0000 - fp: 352.0000 - tn: 256264.0000 - fn: 377.0000 - accuracy: 0.6735 - precision: 0.9973 - recall: 0.9971 - auc: 0.9999 - val_loss: 0.0350 - val_tp: 102850.0000 - val_fp: 1393.0000 - val_tn: 207137.0000 - val_fn: 1415.0000 - val_accuracy: 0.9910 - val_precision: 0.9866 - val_recall: 0.9864 - val_auc: 0.9991\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 12s 313ms/step - loss: 0.0055 - tp: 127991.0000 - fp: 291.0000 - tn: 256325.0000 - fn: 317.0000 - accuracy: 0.6738 - precision: 0.9977 - recall: 0.9975 - auc: 1.0000 - val_loss: 0.0360 - val_tp: 102827.0000 - val_fp: 1416.0000 - val_tn: 207114.0000 - val_fn: 1438.0000 - val_accuracy: 0.9909 - val_precision: 0.9864 - val_recall: 0.9862 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 12s 312ms/step - loss: 0.0050 - tp: 128026.0000 - fp: 270.0000 - tn: 256346.0000 - fn: 282.0000 - accuracy: 0.6739 - precision: 0.9979 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0367 - val_tp: 102798.0000 - val_fp: 1442.0000 - val_tn: 207088.0000 - val_fn: 1467.0000 - val_accuracy: 0.9907 - val_precision: 0.9862 - val_recall: 0.9859 - val_auc: 0.9990\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 12s 304ms/step - loss: 0.0045 - tp: 128042.0000 - fp: 256.0000 - tn: 256360.0000 - fn: 266.0000 - accuracy: 0.6739 - precision: 0.9980 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0375 - val_tp: 102838.0000 - val_fp: 1412.0000 - val_tn: 207118.0000 - val_fn: 1427.0000 - val_accuracy: 0.9909 - val_precision: 0.9865 - val_recall: 0.9863 - val_auc: 0.9989\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0036 - tp: 128088.0000 - fp: 209.0000 - tn: 256407.0000 - fn: 220.0000 - accuracy: 0.6741 - precision: 0.9984 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0386 - val_tp: 102838.0000 - val_fp: 1406.0000 - val_tn: 207124.0000 - val_fn: 1427.0000 - val_accuracy: 0.9909 - val_precision: 0.9865 - val_recall: 0.9863 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0033 - tp: 128128.0000 - fp: 175.0000 - tn: 256441.0000 - fn: 180.0000 - accuracy: 0.6742 - precision: 0.9986 - recall: 0.9986 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 12s 306ms/step - loss: 0.0033 - tp: 128128.0000 - fp: 175.0000 - tn: 256441.0000 - fn: 180.0000 - accuracy: 0.6742 - precision: 0.9986 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0393 - val_tp: 102843.0000 - val_fp: 1412.0000 - val_tn: 207118.0000 - val_fn: 1422.0000 - val_accuracy: 0.9909 - val_precision: 0.9865 - val_recall: 0.9864 - val_auc: 0.9988\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_2class_downsampled/BiLSTM_2class_downsampled_downweight_0.5.h5\n",
            "\n",
            "Down-weighting: 0.6\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 22s 360ms/step - loss: 0.2242 - tp: 209613.0000 - fp: 14224.0000 - tn: 450922.0000 - fn: 22960.0000 - accuracy: 0.7937 - precision: 0.9365 - recall: 0.9013 - auc: 0.9900 - val_loss: 0.0999 - val_tp: 101264.0000 - val_fp: 1260.0000 - val_tn: 207270.0000 - val_fn: 3001.0000 - val_accuracy: 0.9864 - val_precision: 0.9877 - val_recall: 0.9712 - val_auc: 0.9983\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0811 - tp: 124268.0000 - fp: 3303.0000 - tn: 253313.0000 - fn: 4040.0000 - accuracy: 0.6581 - precision: 0.9741 - recall: 0.9685 - auc: 0.9970 - val_loss: 0.0586 - val_tp: 102871.0000 - val_fp: 1244.0000 - val_tn: 207286.0000 - val_fn: 1394.0000 - val_accuracy: 0.9916 - val_precision: 0.9881 - val_recall: 0.9866 - val_auc: 0.9993\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 12s 305ms/step - loss: 0.0642 - tp: 124912.0000 - fp: 3185.0000 - tn: 253431.0000 - fn: 3396.0000 - accuracy: 0.6584 - precision: 0.9751 - recall: 0.9735 - auc: 0.9982 - val_loss: 0.0492 - val_tp: 102952.0000 - val_fp: 1251.0000 - val_tn: 207279.0000 - val_fn: 1313.0000 - val_accuracy: 0.9918 - val_precision: 0.9880 - val_recall: 0.9874 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0525 - tp: 125210.0000 - fp: 2942.0000 - tn: 253674.0000 - fn: 3098.0000 - accuracy: 0.6597 - precision: 0.9770 - recall: 0.9759 - auc: 0.9990 - val_loss: 0.0401 - val_tp: 102942.0000 - val_fp: 1275.0000 - val_tn: 207255.0000 - val_fn: 1323.0000 - val_accuracy: 0.9917 - val_precision: 0.9878 - val_recall: 0.9873 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0366 - tp: 126265.0000 - fp: 1944.0000 - tn: 254672.0000 - fn: 2043.0000 - accuracy: 0.6656 - precision: 0.9848 - recall: 0.9841 - auc: 0.9995 - val_loss: 0.0343 - val_tp: 102874.0000 - val_fp: 1330.0000 - val_tn: 207200.0000 - val_fn: 1391.0000 - val_accuracy: 0.9913 - val_precision: 0.9872 - val_recall: 0.9867 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0219 - tp: 127241.0000 - fp: 1007.0000 - tn: 255609.0000 - fn: 1067.0000 - accuracy: 0.6706 - precision: 0.9921 - recall: 0.9917 - auc: 0.9998 - val_loss: 0.0316 - val_tp: 102906.0000 - val_fp: 1308.0000 - val_tn: 207222.0000 - val_fn: 1359.0000 - val_accuracy: 0.9915 - val_precision: 0.9874 - val_recall: 0.9870 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 12s 306ms/step - loss: 0.0141 - tp: 127667.0000 - fp: 602.0000 - tn: 256014.0000 - fn: 641.0000 - accuracy: 0.6725 - precision: 0.9953 - recall: 0.9950 - auc: 0.9999 - val_loss: 0.0318 - val_tp: 102930.0000 - val_fp: 1290.0000 - val_tn: 207240.0000 - val_fn: 1335.0000 - val_accuracy: 0.9916 - val_precision: 0.9876 - val_recall: 0.9872 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 12s 311ms/step - loss: 0.0104 - tp: 127822.0000 - fp: 458.0000 - tn: 256158.0000 - fn: 486.0000 - accuracy: 0.6730 - precision: 0.9964 - recall: 0.9962 - auc: 0.9999 - val_loss: 0.0328 - val_tp: 102895.0000 - val_fp: 1331.0000 - val_tn: 207199.0000 - val_fn: 1370.0000 - val_accuracy: 0.9914 - val_precision: 0.9872 - val_recall: 0.9869 - val_auc: 0.9994\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0085 - tp: 127892.0000 - fp: 382.0000 - tn: 256234.0000 - fn: 416.0000 - accuracy: 0.6733 - precision: 0.9970 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0337 - val_tp: 102916.0000 - val_fp: 1308.0000 - val_tn: 207222.0000 - val_fn: 1349.0000 - val_accuracy: 0.9915 - val_precision: 0.9875 - val_recall: 0.9871 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0073 - tp: 127930.0000 - fp: 346.0000 - tn: 256270.0000 - fn: 378.0000 - accuracy: 0.6735 - precision: 0.9973 - recall: 0.9971 - auc: 0.9999 - val_loss: 0.0349 - val_tp: 102894.0000 - val_fp: 1348.0000 - val_tn: 207182.0000 - val_fn: 1371.0000 - val_accuracy: 0.9913 - val_precision: 0.9871 - val_recall: 0.9869 - val_auc: 0.9991\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0063 - tp: 127997.0000 - fp: 290.0000 - tn: 256326.0000 - fn: 311.0000 - accuracy: 0.6737 - precision: 0.9977 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0359 - val_tp: 102891.0000 - val_fp: 1352.0000 - val_tn: 207178.0000 - val_fn: 1374.0000 - val_accuracy: 0.9913 - val_precision: 0.9870 - val_recall: 0.9868 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0056 - tp: 128025.0000 - fp: 266.0000 - tn: 256350.0000 - fn: 283.0000 - accuracy: 0.6738 - precision: 0.9979 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0366 - val_tp: 102818.0000 - val_fp: 1423.0000 - val_tn: 207107.0000 - val_fn: 1447.0000 - val_accuracy: 0.9908 - val_precision: 0.9863 - val_recall: 0.9861 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 12s 303ms/step - loss: 0.0051 - tp: 128046.0000 - fp: 248.0000 - tn: 256368.0000 - fn: 262.0000 - accuracy: 0.6739 - precision: 0.9981 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0379 - val_tp: 102860.0000 - val_fp: 1390.0000 - val_tn: 207140.0000 - val_fn: 1405.0000 - val_accuracy: 0.9911 - val_precision: 0.9867 - val_recall: 0.9865 - val_auc: 0.9989\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0041 - tp: 128102.0000 - fp: 198.0000 - tn: 256418.0000 - fn: 206.0000 - accuracy: 0.6741 - precision: 0.9985 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0386 - val_tp: 102855.0000 - val_fp: 1396.0000 - val_tn: 207134.0000 - val_fn: 1410.0000 - val_accuracy: 0.9910 - val_precision: 0.9866 - val_recall: 0.9865 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0037 - tp: 128136.0000 - fp: 166.0000 - tn: 256450.0000 - fn: 172.0000 - accuracy: 0.6742 - precision: 0.9987 - recall: 0.9987 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0037 - tp: 128136.0000 - fp: 166.0000 - tn: 256450.0000 - fn: 172.0000 - accuracy: 0.6742 - precision: 0.9987 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0394 - val_tp: 102913.0000 - val_fp: 1341.0000 - val_tn: 207189.0000 - val_fn: 1352.0000 - val_accuracy: 0.9914 - val_precision: 0.9871 - val_recall: 0.9870 - val_auc: 0.9988\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_2class_downsampled/BiLSTM_2class_downsampled_downweight_0.6.h5\n",
            "\n",
            "Down-weighting: 0.7\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 21s 352ms/step - loss: 0.2543 - tp: 209929.0000 - fp: 14261.0000 - tn: 450885.0000 - fn: 22644.0000 - accuracy: 0.7937 - precision: 0.9364 - recall: 0.9026 - auc: 0.9900 - val_loss: 0.0975 - val_tp: 101520.0000 - val_fp: 1308.0000 - val_tn: 207222.0000 - val_fn: 2745.0000 - val_accuracy: 0.9870 - val_precision: 0.9873 - val_recall: 0.9737 - val_auc: 0.9982\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0877 - tp: 124375.0000 - fp: 3326.0000 - tn: 253290.0000 - fn: 3933.0000 - accuracy: 0.6580 - precision: 0.9740 - recall: 0.9693 - auc: 0.9968 - val_loss: 0.0563 - val_tp: 102906.0000 - val_fp: 1243.0000 - val_tn: 207287.0000 - val_fn: 1359.0000 - val_accuracy: 0.9917 - val_precision: 0.9881 - val_recall: 0.9870 - val_auc: 0.9992\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 12s 305ms/step - loss: 0.0690 - tp: 124957.0000 - fp: 3192.0000 - tn: 253424.0000 - fn: 3351.0000 - accuracy: 0.6584 - precision: 0.9751 - recall: 0.9739 - auc: 0.9981 - val_loss: 0.0478 - val_tp: 102964.0000 - val_fp: 1252.0000 - val_tn: 207278.0000 - val_fn: 1301.0000 - val_accuracy: 0.9918 - val_precision: 0.9880 - val_recall: 0.9875 - val_auc: 0.9995\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 12s 310ms/step - loss: 0.0576 - tp: 125144.0000 - fp: 3045.0000 - tn: 253571.0000 - fn: 3164.0000 - accuracy: 0.6591 - precision: 0.9762 - recall: 0.9753 - auc: 0.9989 - val_loss: 0.0403 - val_tp: 102959.0000 - val_fp: 1262.0000 - val_tn: 207268.0000 - val_fn: 1306.0000 - val_accuracy: 0.9918 - val_precision: 0.9879 - val_recall: 0.9875 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 12s 315ms/step - loss: 0.0414 - tp: 125990.0000 - fp: 2207.0000 - tn: 254409.0000 - fn: 2318.0000 - accuracy: 0.6639 - precision: 0.9828 - recall: 0.9819 - auc: 0.9995 - val_loss: 0.0342 - val_tp: 102926.0000 - val_fp: 1285.0000 - val_tn: 207245.0000 - val_fn: 1339.0000 - val_accuracy: 0.9916 - val_precision: 0.9877 - val_recall: 0.9872 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 12s 305ms/step - loss: 0.0254 - tp: 127121.0000 - fp: 1097.0000 - tn: 255519.0000 - fn: 1187.0000 - accuracy: 0.6699 - precision: 0.9914 - recall: 0.9907 - auc: 0.9998 - val_loss: 0.0316 - val_tp: 102918.0000 - val_fp: 1299.0000 - val_tn: 207231.0000 - val_fn: 1347.0000 - val_accuracy: 0.9915 - val_precision: 0.9875 - val_recall: 0.9871 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0162 - tp: 127644.0000 - fp: 622.0000 - tn: 255994.0000 - fn: 664.0000 - accuracy: 0.6723 - precision: 0.9952 - recall: 0.9948 - auc: 0.9999 - val_loss: 0.0317 - val_tp: 102954.0000 - val_fp: 1261.0000 - val_tn: 207269.0000 - val_fn: 1311.0000 - val_accuracy: 0.9918 - val_precision: 0.9879 - val_recall: 0.9874 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0117 - tp: 127825.0000 - fp: 447.0000 - tn: 256169.0000 - fn: 483.0000 - accuracy: 0.6730 - precision: 0.9965 - recall: 0.9962 - auc: 0.9999 - val_loss: 0.0327 - val_tp: 102950.0000 - val_fp: 1284.0000 - val_tn: 207246.0000 - val_fn: 1315.0000 - val_accuracy: 0.9917 - val_precision: 0.9877 - val_recall: 0.9874 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 12s 304ms/step - loss: 0.0095 - tp: 127885.0000 - fp: 382.0000 - tn: 256234.0000 - fn: 423.0000 - accuracy: 0.6732 - precision: 0.9970 - recall: 0.9967 - auc: 0.9999 - val_loss: 0.0336 - val_tp: 102951.0000 - val_fp: 1276.0000 - val_tn: 207254.0000 - val_fn: 1314.0000 - val_accuracy: 0.9917 - val_precision: 0.9878 - val_recall: 0.9874 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 12s 313ms/step - loss: 0.0081 - tp: 127914.0000 - fp: 367.0000 - tn: 256249.0000 - fn: 394.0000 - accuracy: 0.6733 - precision: 0.9971 - recall: 0.9969 - auc: 0.9999 - val_loss: 0.0349 - val_tp: 102956.0000 - val_fp: 1287.0000 - val_tn: 207243.0000 - val_fn: 1309.0000 - val_accuracy: 0.9917 - val_precision: 0.9877 - val_recall: 0.9874 - val_auc: 0.9991\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0069 - tp: 127982.0000 - fp: 296.0000 - tn: 256320.0000 - fn: 326.0000 - accuracy: 0.6736 - precision: 0.9977 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0360 - val_tp: 102977.0000 - val_fp: 1267.0000 - val_tn: 207263.0000 - val_fn: 1288.0000 - val_accuracy: 0.9918 - val_precision: 0.9878 - val_recall: 0.9876 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0060 - tp: 128024.0000 - fp: 268.0000 - tn: 256348.0000 - fn: 284.0000 - accuracy: 0.6738 - precision: 0.9979 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0374 - val_tp: 102964.0000 - val_fp: 1280.0000 - val_tn: 207250.0000 - val_fn: 1301.0000 - val_accuracy: 0.9917 - val_precision: 0.9877 - val_recall: 0.9875 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0055 - tp: 128050.0000 - fp: 243.0000 - tn: 256373.0000 - fn: 258.0000 - accuracy: 0.6739 - precision: 0.9981 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0384 - val_tp: 102964.0000 - val_fp: 1286.0000 - val_tn: 207244.0000 - val_fn: 1301.0000 - val_accuracy: 0.9917 - val_precision: 0.9877 - val_recall: 0.9875 - val_auc: 0.9988\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0043 - tp: 128128.0000 - fp: 171.0000 - tn: 256445.0000 - fn: 180.0000 - accuracy: 0.6742 - precision: 0.9987 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0393 - val_tp: 102916.0000 - val_fp: 1331.0000 - val_tn: 207199.0000 - val_fn: 1349.0000 - val_accuracy: 0.9914 - val_precision: 0.9872 - val_recall: 0.9871 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0039 - tp: 128147.0000 - fp: 157.0000 - tn: 256459.0000 - fn: 161.0000 - accuracy: 0.6742 - precision: 0.9988 - recall: 0.9987 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 12s 306ms/step - loss: 0.0039 - tp: 128147.0000 - fp: 157.0000 - tn: 256459.0000 - fn: 161.0000 - accuracy: 0.6742 - precision: 0.9988 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0411 - val_tp: 102990.0000 - val_fp: 1267.0000 - val_tn: 207263.0000 - val_fn: 1275.0000 - val_accuracy: 0.9919 - val_precision: 0.9878 - val_recall: 0.9878 - val_auc: 0.9986\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_2class_downsampled/BiLSTM_2class_downsampled_downweight_0.7.h5\n",
            "\n",
            "Down-weighting: 0.8\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 21s 358ms/step - loss: 0.2843 - tp: 210243.0000 - fp: 14277.0000 - tn: 450869.0000 - fn: 22330.0000 - accuracy: 0.7938 - precision: 0.9364 - recall: 0.9040 - auc: 0.9899 - val_loss: 0.0959 - val_tp: 101730.0000 - val_fp: 1360.0000 - val_tn: 207170.0000 - val_fn: 2535.0000 - val_accuracy: 0.9875 - val_precision: 0.9868 - val_recall: 0.9757 - val_auc: 0.9981\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 12s 314ms/step - loss: 0.0938 - tp: 124442.0000 - fp: 3345.0000 - tn: 253271.0000 - fn: 3866.0000 - accuracy: 0.6580 - precision: 0.9738 - recall: 0.9699 - auc: 0.9967 - val_loss: 0.0549 - val_tp: 102930.0000 - val_fp: 1242.0000 - val_tn: 207288.0000 - val_fn: 1335.0000 - val_accuracy: 0.9918 - val_precision: 0.9881 - val_recall: 0.9872 - val_auc: 0.9992\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 12s 314ms/step - loss: 0.0734 - tp: 124973.0000 - fp: 3191.0000 - tn: 253425.0000 - fn: 3335.0000 - accuracy: 0.6584 - precision: 0.9751 - recall: 0.9740 - auc: 0.9980 - val_loss: 0.0468 - val_tp: 102968.0000 - val_fp: 1254.0000 - val_tn: 207276.0000 - val_fn: 1297.0000 - val_accuracy: 0.9918 - val_precision: 0.9880 - val_recall: 0.9876 - val_auc: 0.9994\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0621 - tp: 125117.0000 - fp: 3110.0000 - tn: 253506.0000 - fn: 3191.0000 - accuracy: 0.6587 - precision: 0.9757 - recall: 0.9751 - auc: 0.9988 - val_loss: 0.0405 - val_tp: 102978.0000 - val_fp: 1248.0000 - val_tn: 207282.0000 - val_fn: 1287.0000 - val_accuracy: 0.9919 - val_precision: 0.9880 - val_recall: 0.9877 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0461 - tp: 125746.0000 - fp: 2463.0000 - tn: 254153.0000 - fn: 2562.0000 - accuracy: 0.6623 - precision: 0.9808 - recall: 0.9800 - auc: 0.9994 - val_loss: 0.0344 - val_tp: 102969.0000 - val_fp: 1250.0000 - val_tn: 207280.0000 - val_fn: 1296.0000 - val_accuracy: 0.9919 - val_precision: 0.9880 - val_recall: 0.9876 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0291 - tp: 126921.0000 - fp: 1289.0000 - tn: 255327.0000 - fn: 1387.0000 - accuracy: 0.6687 - precision: 0.9899 - recall: 0.9892 - auc: 0.9998 - val_loss: 0.0316 - val_tp: 102923.0000 - val_fp: 1290.0000 - val_tn: 207240.0000 - val_fn: 1342.0000 - val_accuracy: 0.9916 - val_precision: 0.9876 - val_recall: 0.9871 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0183 - tp: 127601.0000 - fp: 661.0000 - tn: 255955.0000 - fn: 707.0000 - accuracy: 0.6720 - precision: 0.9948 - recall: 0.9945 - auc: 0.9999 - val_loss: 0.0318 - val_tp: 102970.0000 - val_fp: 1249.0000 - val_tn: 207281.0000 - val_fn: 1295.0000 - val_accuracy: 0.9919 - val_precision: 0.9880 - val_recall: 0.9876 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0130 - tp: 127814.0000 - fp: 454.0000 - tn: 256162.0000 - fn: 494.0000 - accuracy: 0.6729 - precision: 0.9965 - recall: 0.9961 - auc: 0.9999 - val_loss: 0.0327 - val_tp: 102972.0000 - val_fp: 1263.0000 - val_tn: 207267.0000 - val_fn: 1293.0000 - val_accuracy: 0.9918 - val_precision: 0.9879 - val_recall: 0.9876 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0105 - tp: 127864.0000 - fp: 405.0000 - tn: 256211.0000 - fn: 444.0000 - accuracy: 0.6731 - precision: 0.9968 - recall: 0.9965 - auc: 0.9999 - val_loss: 0.0336 - val_tp: 102985.0000 - val_fp: 1246.0000 - val_tn: 207284.0000 - val_fn: 1280.0000 - val_accuracy: 0.9919 - val_precision: 0.9880 - val_recall: 0.9877 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 12s 310ms/step - loss: 0.0089 - tp: 127898.0000 - fp: 379.0000 - tn: 256237.0000 - fn: 410.0000 - accuracy: 0.6732 - precision: 0.9970 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0351 - val_tp: 103000.0000 - val_fp: 1254.0000 - val_tn: 207276.0000 - val_fn: 1265.0000 - val_accuracy: 0.9919 - val_precision: 0.9880 - val_recall: 0.9879 - val_auc: 0.9991\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 12s 313ms/step - loss: 0.0076 - tp: 127981.0000 - fp: 302.0000 - tn: 256314.0000 - fn: 327.0000 - accuracy: 0.6736 - precision: 0.9976 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0360 - val_tp: 103004.0000 - val_fp: 1248.0000 - val_tn: 207282.0000 - val_fn: 1261.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9879 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 12s 315ms/step - loss: 0.0066 - tp: 128023.0000 - fp: 264.0000 - tn: 256352.0000 - fn: 285.0000 - accuracy: 0.6737 - precision: 0.9979 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0377 - val_tp: 102989.0000 - val_fp: 1259.0000 - val_tn: 207271.0000 - val_fn: 1276.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9878 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 12s 314ms/step - loss: 0.0060 - tp: 128045.0000 - fp: 246.0000 - tn: 256370.0000 - fn: 263.0000 - accuracy: 0.6738 - precision: 0.9981 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0385 - val_tp: 103004.0000 - val_fp: 1253.0000 - val_tn: 207277.0000 - val_fn: 1261.0000 - val_accuracy: 0.9920 - val_precision: 0.9880 - val_recall: 0.9879 - val_auc: 0.9988\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0048 - tp: 128119.0000 - fp: 177.0000 - tn: 256439.0000 - fn: 189.0000 - accuracy: 0.6741 - precision: 0.9986 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0395 - val_tp: 102967.0000 - val_fp: 1283.0000 - val_tn: 207247.0000 - val_fn: 1298.0000 - val_accuracy: 0.9917 - val_precision: 0.9877 - val_recall: 0.9876 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0043 - tp: 128131.0000 - fp: 167.0000 - tn: 256449.0000 - fn: 177.0000 - accuracy: 0.6741 - precision: 0.9987 - recall: 0.9986 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0043 - tp: 128131.0000 - fp: 167.0000 - tn: 256449.0000 - fn: 177.0000 - accuracy: 0.6741 - precision: 0.9987 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0420 - val_tp: 103017.0000 - val_fp: 1241.0000 - val_tn: 207289.0000 - val_fn: 1248.0000 - val_accuracy: 0.9920 - val_precision: 0.9881 - val_recall: 0.9880 - val_auc: 0.9986\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_2class_downsampled/BiLSTM_2class_downsampled_downweight_0.8.h5\n",
            "\n",
            "Down-weighting: 0.9\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 21s 355ms/step - loss: 0.3141 - tp: 210428.0000 - fp: 14330.0000 - tn: 450816.0000 - fn: 22145.0000 - accuracy: 0.7938 - precision: 0.9362 - recall: 0.9048 - auc: 0.9899 - val_loss: 0.0947 - val_tp: 101876.0000 - val_fp: 1404.0000 - val_tn: 207126.0000 - val_fn: 2389.0000 - val_accuracy: 0.9879 - val_precision: 0.9864 - val_recall: 0.9771 - val_auc: 0.9981\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 12s 304ms/step - loss: 0.0995 - tp: 124480.0000 - fp: 3371.0000 - tn: 253245.0000 - fn: 3828.0000 - accuracy: 0.6579 - precision: 0.9736 - recall: 0.9702 - auc: 0.9965 - val_loss: 0.0540 - val_tp: 102945.0000 - val_fp: 1246.0000 - val_tn: 207284.0000 - val_fn: 1320.0000 - val_accuracy: 0.9918 - val_precision: 0.9880 - val_recall: 0.9873 - val_auc: 0.9992\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 12s 306ms/step - loss: 0.0774 - tp: 124985.0000 - fp: 3202.0000 - tn: 253414.0000 - fn: 3323.0000 - accuracy: 0.6584 - precision: 0.9750 - recall: 0.9741 - auc: 0.9979 - val_loss: 0.0462 - val_tp: 102970.0000 - val_fp: 1254.0000 - val_tn: 207276.0000 - val_fn: 1295.0000 - val_accuracy: 0.9919 - val_precision: 0.9880 - val_recall: 0.9876 - val_auc: 0.9994\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 12s 306ms/step - loss: 0.0662 - tp: 125106.0000 - fp: 3141.0000 - tn: 253475.0000 - fn: 3202.0000 - accuracy: 0.6585 - precision: 0.9755 - recall: 0.9750 - auc: 0.9987 - val_loss: 0.0405 - val_tp: 102987.0000 - val_fp: 1242.0000 - val_tn: 207288.0000 - val_fn: 1278.0000 - val_accuracy: 0.9919 - val_precision: 0.9881 - val_recall: 0.9877 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0504 - tp: 125535.0000 - fp: 2647.0000 - tn: 253969.0000 - fn: 2773.0000 - accuracy: 0.6611 - precision: 0.9793 - recall: 0.9784 - auc: 0.9993 - val_loss: 0.0344 - val_tp: 102978.0000 - val_fp: 1243.0000 - val_tn: 207287.0000 - val_fn: 1287.0000 - val_accuracy: 0.9919 - val_precision: 0.9881 - val_recall: 0.9877 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0328 - tp: 126725.0000 - fp: 1486.0000 - tn: 255130.0000 - fn: 1583.0000 - accuracy: 0.6675 - precision: 0.9884 - recall: 0.9877 - auc: 0.9997 - val_loss: 0.0317 - val_tp: 102976.0000 - val_fp: 1245.0000 - val_tn: 207285.0000 - val_fn: 1289.0000 - val_accuracy: 0.9919 - val_precision: 0.9881 - val_recall: 0.9876 - val_auc: 0.9996\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0206 - tp: 127509.0000 - fp: 751.0000 - tn: 255865.0000 - fn: 799.0000 - accuracy: 0.6715 - precision: 0.9941 - recall: 0.9938 - auc: 0.9998 - val_loss: 0.0320 - val_tp: 102997.0000 - val_fp: 1224.0000 - val_tn: 207306.0000 - val_fn: 1268.0000 - val_accuracy: 0.9920 - val_precision: 0.9883 - val_recall: 0.9878 - val_auc: 0.9995\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 12s 309ms/step - loss: 0.0144 - tp: 127779.0000 - fp: 485.0000 - tn: 256131.0000 - fn: 529.0000 - accuracy: 0.6727 - precision: 0.9962 - recall: 0.9959 - auc: 0.9999 - val_loss: 0.0329 - val_tp: 102981.0000 - val_fp: 1257.0000 - val_tn: 207273.0000 - val_fn: 1284.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9877 - val_auc: 0.9993\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 12s 307ms/step - loss: 0.0115 - tp: 127850.0000 - fp: 429.0000 - tn: 256187.0000 - fn: 458.0000 - accuracy: 0.6730 - precision: 0.9967 - recall: 0.9964 - auc: 0.9999 - val_loss: 0.0340 - val_tp: 102982.0000 - val_fp: 1249.0000 - val_tn: 207281.0000 - val_fn: 1283.0000 - val_accuracy: 0.9919 - val_precision: 0.9880 - val_recall: 0.9877 - val_auc: 0.9992\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 12s 306ms/step - loss: 0.0099 - tp: 127889.0000 - fp: 384.0000 - tn: 256232.0000 - fn: 419.0000 - accuracy: 0.6732 - precision: 0.9970 - recall: 0.9967 - auc: 0.9999 - val_loss: 0.0353 - val_tp: 102996.0000 - val_fp: 1258.0000 - val_tn: 207272.0000 - val_fn: 1269.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9878 - val_auc: 0.9991\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0085 - tp: 127957.0000 - fp: 323.0000 - tn: 256293.0000 - fn: 351.0000 - accuracy: 0.6734 - precision: 0.9975 - recall: 0.9973 - auc: 0.9999 - val_loss: 0.0364 - val_tp: 103025.0000 - val_fp: 1229.0000 - val_tn: 207301.0000 - val_fn: 1240.0000 - val_accuracy: 0.9921 - val_precision: 0.9882 - val_recall: 0.9881 - val_auc: 0.9990\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 12s 305ms/step - loss: 0.0074 - tp: 127998.0000 - fp: 289.0000 - tn: 256327.0000 - fn: 310.0000 - accuracy: 0.6736 - precision: 0.9977 - recall: 0.9976 - auc: 1.0000 - val_loss: 0.0376 - val_tp: 102983.0000 - val_fp: 1263.0000 - val_tn: 207267.0000 - val_fn: 1282.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9877 - val_auc: 0.9989\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 12s 308ms/step - loss: 0.0067 - tp: 128039.0000 - fp: 252.0000 - tn: 256364.0000 - fn: 269.0000 - accuracy: 0.6738 - precision: 0.9980 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0384 - val_tp: 102980.0000 - val_fp: 1271.0000 - val_tn: 207259.0000 - val_fn: 1285.0000 - val_accuracy: 0.9918 - val_precision: 0.9878 - val_recall: 0.9877 - val_auc: 0.9988\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 12s 306ms/step - loss: 0.0055 - tp: 128107.0000 - fp: 192.0000 - tn: 256424.0000 - fn: 201.0000 - accuracy: 0.6740 - precision: 0.9985 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0399 - val_tp: 102986.0000 - val_fp: 1263.0000 - val_tn: 207267.0000 - val_fn: 1279.0000 - val_accuracy: 0.9919 - val_precision: 0.9879 - val_recall: 0.9877 - val_auc: 0.9988\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0049 - tp: 128116.0000 - fp: 182.0000 - tn: 256434.0000 - fn: 192.0000 - accuracy: 0.6740 - precision: 0.9986 - recall: 0.9985 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 12s 315ms/step - loss: 0.0049 - tp: 128116.0000 - fp: 182.0000 - tn: 256434.0000 - fn: 192.0000 - accuracy: 0.6740 - precision: 0.9986 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0416 - val_tp: 103021.0000 - val_fp: 1240.0000 - val_tn: 207290.0000 - val_fn: 1244.0000 - val_accuracy: 0.9921 - val_precision: 0.9881 - val_recall: 0.9881 - val_auc: 0.9986\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_2class_downsampled/BiLSTM_2class_downsampled_downweight_0.9.h5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGGOp2Avm8g1",
        "outputId": "0ca235f8-370e-4326-d787-0b23c84a5a6a"
      },
      "source": [
        "downweight_models[1.0] = model\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n",
        "    preds = np.argmax(downweight_models[weight].predict(dev_seqs_bo_padded), axis=-1)\n",
        "\n",
        "    dev_seqs_bo['prediction'] = ''\n",
        "    for i in dev_seqs_bo.index:\n",
        "        this_seq_length = len(dev_seqs_bo['token'][i])\n",
        "        dev_seqs_bo['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    dev_long = dev_seqs_bo.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = reverse_bio_from_bo(dev_long['bio_only'])\n",
        "    dev_long['bio_only'] = bio_labs\n",
        "    pred_labs = reverse_bio_from_bo(dev_long['prediction'])\n",
        "    dev_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(dev_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 0.1:\n",
            "Sum of TP and FP = 1684\n",
            "Sum of TP and FN = 805\n",
            "True positives = 325, False positives = 1359, False negatives = 480\n",
            "Precision = 0.193, Recall = 0.404, F1 = 0.261\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 1160\n",
            "Sum of TP and FN = 805\n",
            "True positives = 241, False positives = 919, False negatives = 564\n",
            "Precision = 0.208, Recall = 0.299, F1 = 0.245\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 987\n",
            "Sum of TP and FN = 805\n",
            "True positives = 195, False positives = 792, False negatives = 610\n",
            "Precision = 0.198, Recall = 0.242, F1 = 0.218\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 682\n",
            "Sum of TP and FN = 805\n",
            "True positives = 140, False positives = 542, False negatives = 665\n",
            "Precision = 0.205, Recall = 0.174, F1 = 0.188\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 449\n",
            "Sum of TP and FN = 805\n",
            "True positives = 91, False positives = 358, False negatives = 714\n",
            "Precision = 0.203, Recall = 0.113, F1 = 0.145\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 310\n",
            "Sum of TP and FN = 805\n",
            "True positives = 59, False positives = 251, False negatives = 746\n",
            "Precision = 0.190, Recall = 0.073, F1 = 0.106\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 185\n",
            "Sum of TP and FN = 805\n",
            "True positives = 29, False positives = 156, False negatives = 776\n",
            "Precision = 0.157, Recall = 0.036, F1 = 0.059\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 110\n",
            "Sum of TP and FN = 805\n",
            "True positives = 17, False positives = 93, False negatives = 788\n",
            "Precision = 0.155, Recall = 0.021, F1 = 0.037\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 57\n",
            "Sum of TP and FN = 805\n",
            "True positives = 7, False positives = 50, False negatives = 798\n",
            "Precision = 0.123, Recall = 0.009, F1 = 0.016\n",
            "\n",
            "Weight = 1.0:\n",
            "Sum of TP and FP = 31\n",
            "Sum of TP and FN = 805\n",
            "True positives = 0, False positives = 31, False negatives = 805\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei4V2Wn3k6nw"
      },
      "source": [
        "### 4. Adding PoS feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "TNxPTNw0k45O",
        "outputId": "fea84a95-d890-4ec9-af70-e9eebc745381"
      },
      "source": [
        "# reload training set\n",
        "wnuttrain = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt'\n",
        "train = pd.read_table(wnuttrain, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "# NB: don't drop the empty lines between texts yet, they are needed for sequence splits (they show up as NaN in the data frame)\n",
        "train.head(n=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@paulwalk</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'s</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>AUX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>view</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>where</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>'m</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>living</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>for</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>two</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NUM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>weeks</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Empire</td>\n",
              "      <td>B-location</td>\n",
              "      <td>B</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>State</td>\n",
              "      <td>I-location</td>\n",
              "      <td>I</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Building</td>\n",
              "      <td>I-location</td>\n",
              "      <td>I</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>=</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>SYM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ESB</td>\n",
              "      <td>B-location</td>\n",
              "      <td>B</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Pretty</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>bad</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>storm</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>here</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>last</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>evening</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>From</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Green</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token       label bio_only   upos\n",
              "0   @paulwalk           O        O   NOUN\n",
              "1          It           O        O   PRON\n",
              "2          's           O        O    AUX\n",
              "3         the           O        O    DET\n",
              "4        view           O        O   NOUN\n",
              "5        from           O        O    ADP\n",
              "6       where           O        O    ADV\n",
              "7           I           O        O   PRON\n",
              "8          'm           O        O      X\n",
              "9      living           O        O   NOUN\n",
              "10        for           O        O    ADP\n",
              "11        two           O        O    NUM\n",
              "12      weeks           O        O   NOUN\n",
              "13          .           O        O  PUNCT\n",
              "14     Empire  B-location        B  PROPN\n",
              "15      State  I-location        I  PROPN\n",
              "16   Building  I-location        I  PROPN\n",
              "17          =           O        O    SYM\n",
              "18        ESB  B-location        B  PROPN\n",
              "19          .           O        O  PUNCT\n",
              "20     Pretty           O        O    ADV\n",
              "21        bad           O        O    ADJ\n",
              "22      storm           O        O   NOUN\n",
              "23       here           O        O    ADV\n",
              "24       last           O        O    ADJ\n",
              "25    evening           O        O   NOUN\n",
              "26          .           O        O  PUNCT\n",
              "27        NaN         NaN      NaN    NaN\n",
              "28       From           O        O    ADP\n",
              "29      Green           O        O  PROPN"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "HTtGh0Z_lk31",
        "outputId": "be2dbb79-193f-4ecf-c98a-2f118553def4"
      },
      "source": [
        "# in order to convert word tokens to integers: list the set of token types\n",
        "token_vocab = train.token.unique().tolist()\n",
        "oov = len(token_vocab)  # OOV (out of vocabulary) token as vocab length (because that's max.index + 1)\n",
        "\n",
        "# convert word tokens to integers\n",
        "def token_index(tok):\n",
        "    ind = tok\n",
        "    if not pd.isnull(tok):  # new since last time: deal with the empty lines which we didn't drop yet\n",
        "        if tok in token_vocab:  # if token in vocabulary\n",
        "            ind = token_vocab.index(tok)\n",
        "        else:  # else it's OOV\n",
        "            ind = oov\n",
        "    return ind\n",
        "\n",
        "# training labels: convert BIO to integers\n",
        "def bio_index(bio):\n",
        "    ind = bio\n",
        "    if not pd.isnull(bio):  # deal with empty lines\n",
        "        if bio=='B':\n",
        "            ind = 0\n",
        "        elif bio=='I':\n",
        "            ind = 1\n",
        "        elif bio=='O':\n",
        "            ind = 2\n",
        "    return ind\n",
        "\n",
        "# convert PoS into integers\n",
        "all_pos = train.upos.unique().tolist()\n",
        "oopos = len(all_pos)\n",
        "\n",
        "def pos_index(pos):\n",
        "    ind = pos\n",
        "    if not pd.isnull(pos):\n",
        "        if pos in all_pos:\n",
        "            ind = all_pos.index(pos)\n",
        "        else:\n",
        "            ind = oopos\n",
        "    return ind\n",
        "\n",
        "# pass a data frame through our feature extractor\n",
        "def extract_features_pos(txt,istest=False):\n",
        "    txt_copy = txt.copy()\n",
        "    tokinds = [token_index(u) for u in txt_copy['token']]\n",
        "    txt_copy['token_indices'] = tokinds\n",
        "    posinds = [pos_index(p) for p in txt_copy['upos']]\n",
        "    txt_copy['upos_indices'] = posinds\n",
        "    if not istest:  # can't do this with the test set\n",
        "        bioints = [bio_index(b) for b in txt_copy['bio_only']]\n",
        "        txt_copy['bio_only'] = bioints\n",
        "    return txt_copy\n",
        "\n",
        "train_copy = extract_features_pos(train)\n",
        "train_copy.head(n=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>upos_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@paulwalk</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PRON</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'s</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>AUX</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>DET</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>view</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>where</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PRON</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>'m</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>X</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>living</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>for</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>two</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NUM</td>\n",
              "      <td>11.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>weeks</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Empire</td>\n",
              "      <td>B-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>14.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>State</td>\n",
              "      <td>I-location</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Building</td>\n",
              "      <td>I-location</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>=</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>SYM</td>\n",
              "      <td>17.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ESB</td>\n",
              "      <td>B-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>18.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Pretty</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>19.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>bad</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>20.0</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>storm</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>here</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>22.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>last</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>23.0</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>evening</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>From</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>26.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Green</td>\n",
              "      <td>O</td>\n",
              "      <td>2.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>27.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token       label  bio_only   upos  token_indices  upos_indices\n",
              "0   @paulwalk           O       2.0   NOUN            0.0           0.0\n",
              "1          It           O       2.0   PRON            1.0           1.0\n",
              "2          's           O       2.0    AUX            2.0           2.0\n",
              "3         the           O       2.0    DET            3.0           3.0\n",
              "4        view           O       2.0   NOUN            4.0           0.0\n",
              "5        from           O       2.0    ADP            5.0           4.0\n",
              "6       where           O       2.0    ADV            6.0           5.0\n",
              "7           I           O       2.0   PRON            7.0           1.0\n",
              "8          'm           O       2.0      X            8.0           6.0\n",
              "9      living           O       2.0   NOUN            9.0           0.0\n",
              "10        for           O       2.0    ADP           10.0           4.0\n",
              "11        two           O       2.0    NUM           11.0           7.0\n",
              "12      weeks           O       2.0   NOUN           12.0           0.0\n",
              "13          .           O       2.0  PUNCT           13.0           8.0\n",
              "14     Empire  B-location       0.0  PROPN           14.0           9.0\n",
              "15      State  I-location       1.0  PROPN           15.0           9.0\n",
              "16   Building  I-location       1.0  PROPN           16.0           9.0\n",
              "17          =           O       2.0    SYM           17.0          10.0\n",
              "18        ESB  B-location       0.0  PROPN           18.0           9.0\n",
              "19          .           O       2.0  PUNCT           13.0           8.0\n",
              "20     Pretty           O       2.0    ADV           19.0           5.0\n",
              "21        bad           O       2.0    ADJ           20.0          11.0\n",
              "22      storm           O       2.0   NOUN           21.0           0.0\n",
              "23       here           O       2.0    ADV           22.0           5.0\n",
              "24       last           O       2.0    ADJ           23.0          11.0\n",
              "25    evening           O       2.0   NOUN           24.0           0.0\n",
              "26          .           O       2.0  PUNCT           13.0           8.0\n",
              "27        NaN         NaN       NaN    NaN            NaN           NaN\n",
              "28       From           O       2.0    ADP           26.0           4.0\n",
              "29      Green           O       2.0  PROPN           27.0           9.0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "9n1Q6g4Uoi4e",
        "outputId": "d76e8eb0-3efc-440d-a357-405cf3c82625"
      },
      "source": [
        "def tokens2sequences_pos(txt_in,istest=False):\n",
        "    '''\n",
        "    Takes panda dataframe as input, copies, and adds a sequence index based on full-stops.\n",
        "    Outputs a dataframe with sequences of tokens, named entity labels, token indices, and upos indices as lists.\n",
        "    '''\n",
        "    txt = txt_in.copy()\n",
        "    txt['sequence_num'] = 0\n",
        "    seqcount = 0\n",
        "    for i in txt.index:  # in each row...\n",
        "        txt.loc[i,'sequence_num'] = seqcount  # set the sequence number\n",
        "        if pd.isnull(txt.loc[i,'token']):  # increment sequence counter at empty lines\n",
        "            seqcount += 1\n",
        "    # now drop the empty lines, group by sequence number and output df of sequence lists\n",
        "    txt = txt.dropna()\n",
        "    if istest:  # looking ahead: the test set doesn't have labels\n",
        "        txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'token_indices', 'upos_indices']].agg(lambda x: list(x))\n",
        "    else:  # the dev and training sets do have labels\n",
        "        txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'bio_only', 'token_indices', 'upos_indices']].agg(lambda x: list(x))\n",
        "    return txt_seqs\n",
        "\n",
        "print(\"This cell takes a little while to run: be patient :)\")\n",
        "train_seqs = tokens2sequences_pos(train_copy)\n",
        "train_seqs.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This cell takes a little while to run: be patient :)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>upos_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[@paulwalk, It, 's, the, view, from, where, I,...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 0.0, 4.0, 5.0, 1.0, 6.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[From, Green, Newsfeed, :, AHFA, extends, dead...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....</td>\n",
              "      <td>[4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Pxleyes, Top, 50, Photography, Contest, Pictu...</td>\n",
              "      <td>[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....</td>\n",
              "      <td>[13.0, 11.0, 7.0, 0.0, 9.0, 0.0, 4.0, 9.0, 7.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[today, is, my, last, day, at, the, office, .]</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]</td>\n",
              "      <td>[51.0, 52.0, 53.0, 23.0, 54.0, 55.0, 3.0, 56.0...</td>\n",
              "      <td>[0.0, 2.0, 1.0, 11.0, 0.0, 4.0, 3.0, 0.0, 8.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[4Dbling, 's, place, til, monday, ,, party, pa...</td>\n",
              "      <td>[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...</td>\n",
              "      <td>[0.0, 2.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                       upos_indices\n",
              "0             0  ...  [0.0, 1.0, 2.0, 3.0, 0.0, 4.0, 5.0, 1.0, 6.0, ...\n",
              "1             1  ...  [4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...\n",
              "2             2  ...  [13.0, 11.0, 7.0, 0.0, 9.0, 0.0, 4.0, 9.0, 7.0...\n",
              "3             3  ...     [0.0, 2.0, 1.0, 11.0, 0.0, 4.0, 3.0, 0.0, 8.0]\n",
              "4             4  ...  [0.0, 2.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "hoLm60EwpK47",
        "outputId": "accf8efb-5c29-4114-94ee-c58c31ff82b9"
      },
      "source": [
        "# process the dev set\n",
        "wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'\n",
        "dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "dev_copy = extract_features_pos(dev)\n",
        "dev_seqs = tokens2sequences_pos(dev_copy)\n",
        "dev_seqs.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>upos_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[Stabilized, approach, or, not, ?, That, , s,...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[14801.0, 10361.0, 414.0, 556.0, 131.0, 1740.0...</td>\n",
              "      <td>[9.0, 0.0, 16.0, 15.0, 8.0, 1.0, 10.0, 15.0, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[You, should, ', ve, stayed, on, Redondo, Beac...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 1.0, ...</td>\n",
              "      <td>[151.0, 1018.0, 573.0, 12927.0, 9346.0, 137.0,...</td>\n",
              "      <td>[1.0, 2.0, 8.0, 0.0, 14.0, 4.0, 9.0, 9.0, 9.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[All, I, ', ve, been, doing, is, BINGE, watchi...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[405.0, 7.0, 573.0, 12927.0, 90.0, 848.0, 52.0...</td>\n",
              "      <td>[3.0, 1.0, 8.0, 0.0, 2.0, 14.0, 2.0, 9.0, 14.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[wow, emma, and, kaite, is, so, very, cute, an...</td>\n",
              "      <td>[2.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[4777.0, 14801.0, 113.0, 14801.0, 52.0, 79.0, ...</td>\n",
              "      <td>[13.0, 0.0, 16.0, 0.0, 2.0, 5.0, 5.0, 11.0, 16...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[THIS, IS, SO, GOOD]</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0]</td>\n",
              "      <td>[2239.0, 1567.0, 1089.0, 9176.0]</td>\n",
              "      <td>[3.0, 2.0, 5.0, 11.0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                       upos_indices\n",
              "0             0  ...  [9.0, 0.0, 16.0, 15.0, 8.0, 1.0, 10.0, 15.0, 1...\n",
              "1             1  ...  [1.0, 2.0, 8.0, 0.0, 14.0, 4.0, 9.0, 9.0, 9.0,...\n",
              "2             2  ...  [3.0, 1.0, 8.0, 0.0, 2.0, 14.0, 2.0, 9.0, 14.0...\n",
              "3             3  ...  [13.0, 0.0, 16.0, 0.0, 2.0, 5.0, 5.0, 11.0, 16...\n",
              "4             4  ...                              [3.0, 2.0, 5.0, 11.0]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADYdWXVtp9wX",
        "outputId": "18ad6d06-27c4-4ac8-ec24-48d331d2fe41"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# set maximum sequence length\n",
        "seq_length = 105\n",
        "\n",
        "# a new dummy token index, one more than OOV\n",
        "padtok = oov + 1\n",
        "print('The padding token index is %i' % padtok)\n",
        "\n",
        "padpos = oopos + 1\n",
        "print('The padding pos index is %i' % padpos)\n",
        "\n",
        "# use pad_sequences, padding or truncating at the end of the sequence (default is 'pre')\n",
        "train_seqs_padded = pad_sequences(train_seqs['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                  dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "print('Example of padded token sequence:')\n",
        "print(train_seqs_padded[1])\n",
        "\n",
        "train_pos_padded = pad_sequences(train_seqs['upos_indices'].tolist(), maxlen=seq_length,\n",
        "                                 dtype='int32', padding='post', truncating='post', value=padpos)\n",
        "print('Example of padded pos sequence:')\n",
        "print(train_pos_padded[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The padding token index is 14802\n",
            "The padding pos index is 19\n",
            "Example of padded token sequence:\n",
            "[   26    27    28    29    30    31    32    10    33    34    35    36\n",
            "    13    37    38 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802]\n",
            "Example of padded pos sequence:\n",
            "[ 4  9  9  8  9  0  0  4  9  9  4  9  8  6  6 19 19 19 19 19 19 19 19 19\n",
            " 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19\n",
            " 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19\n",
            " 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19\n",
            " 19 19 19 19 19 19 19 19 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoTzsugfq2og",
        "outputId": "fb5fda58-f004-43d5-a1da-db1bcc748b7a"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# get lists of named entity labels, padded with a null label (=3)\n",
        "padlab = 3\n",
        "train_labs_padded = pad_sequences(train_seqs['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                  dtype='int32', padding='post', truncating='post', value=padlab)\n",
        "\n",
        "# convert those labels to one-hot encoding\n",
        "n_labs = 4  # we have 3 labels: B, I, O (0, 1, 2) + the pad label 3\n",
        "train_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_padded]\n",
        "\n",
        "# follow the print outputs below to see how the labels are transformed\n",
        "print('Example of padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(train_seqs.loc[1])\n",
        "print('Length of input sequence: %i' % len(train_seqs_padded[1]))\n",
        "print('Length of pos sequence: %i' % len(train_pos_padded[1]))\n",
        "print('Length of label sequence: %i' % len(train_labs_onehot[1]))\n",
        "print(train_pos_padded[1][:11])\n",
        "print(train_labs_padded[1][:11])\n",
        "print(train_labs_onehot[1][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     1\n",
            "token            [From, Green, Newsfeed, :, AHFA, extends, dead...\n",
            "bio_only         [2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, ...\n",
            "token_indices    [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
            "upos_indices     [4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...\n",
            "Name: 1, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of pos sequence: 105\n",
            "Length of label sequence: 105\n",
            "[4 9 9 8 9 0 0 4 9 9 4]\n",
            "[2 2 2 2 0 2 2 2 2 2 2]\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNRoNvcBr_eW",
        "outputId": "409adac7-480b-47f9-e8ca-ecbf5ab51898"
      },
      "source": [
        "# now process the dev set in the same way: padding the tokens, pos and labels, and one-hot encoding the labels\n",
        "dev_seqs_padded = pad_sequences(dev_seqs['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "dev_pos_padded = pad_sequences(dev_seqs['upos_indices'].tolist(), maxlen=seq_length,\n",
        "                               dtype='int32', padding='post', truncating='post', value=padpos)\n",
        "dev_labs_padded = pad_sequences(dev_seqs['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                dtype='int32', padding='post', truncating='post', value=padlab)\n",
        "dev_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in dev_labs_padded]\n",
        "\n",
        "print('Dev set padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(dev_seqs.loc[2])\n",
        "print('Length of input sequence: %i' % len(dev_seqs_padded[1]))\n",
        "print('Length of pos sequence: %i' % len(dev_pos_padded[1]))\n",
        "print('Length of label sequence: %i' % len(dev_labs_onehot[1]))\n",
        "print(dev_pos_padded[2][:11])\n",
        "print(dev_labs_padded[2][:11])\n",
        "print(dev_labs_onehot[2][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev set padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     2\n",
            "token            [All, I, ', ve, been, doing, is, BINGE, watchi...\n",
            "bio_only         [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...\n",
            "token_indices    [405.0, 7.0, 573.0, 12927.0, 90.0, 848.0, 52.0...\n",
            "upos_indices     [3.0, 1.0, 8.0, 0.0, 2.0, 14.0, 2.0, 9.0, 14.0...\n",
            "Name: 2, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of pos sequence: 105\n",
            "Length of label sequence: 105\n",
            "[ 3  1  8  0  2 14  2  9 14  9 16]\n",
            "[2 2 2 2 2 2 2 2 2 0 1]\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHE3yqXSsqt6",
        "outputId": "02534c0f-78e7-4dd9-d9c3-165a6b444564"
      },
      "source": [
        "# load Keras and TensorFlow\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# prepare sequences and labels as numpy arrays, check dimensions\n",
        "X_token = np.array(train_seqs_padded)\n",
        "X_pos = np.array(train_pos_padded)\n",
        "y = np.array(train_labs_onehot)\n",
        "print('Input sequence dimensions (n.docs, seq.length):')\n",
        "print(X_token.shape)\n",
        "print('Input pos dimensions (n.docs, seq.length):')\n",
        "print(X_pos.shape)\n",
        "print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n",
        "print(y.shape)\n",
        "\n",
        "# our final vocab size is the padding token + 1 (OR length of vocab + OOV + PAD)\n",
        "vocab_size = padtok + 1\n",
        "print(vocab_size == len(token_vocab) + 2)\n",
        "# our final pos size is the padding token + 1 (OR length of all_pos + OOPoS + PAD)\n",
        "pos_size = padpos + 1  # use this as the pos embedding size\n",
        "print(pos_size == len(all_pos) + 2)\n",
        "token_embed_size = 128  # try an token embedding size of 128 (could tune this)\n",
        "pos_embed_size = 16  # try an token embedding size of 16 (could tune this)\n",
        "\n",
        "#list of metrics to use: true & false positives, negatives, accuracy, precision, recall, area under the curve\n",
        "METRICS = [\n",
        "    keras.metrics.TruePositives(name='tp'),\n",
        "    keras.metrics.FalsePositives(name='fp'),\n",
        "    keras.metrics.TrueNegatives(name='tn'),\n",
        "    keras.metrics.FalseNegatives(name='fn'), \n",
        "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    keras.metrics.Precision(name='precision'),\n",
        "    keras.metrics.Recall(name='recall'),\n",
        "    keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "# our model has the option for an label prediction bias, it's sequential, starts with an embedding layer, then bi-LSTM,\n",
        "# a dropout layer follows for regularisation, and a dense final layer with softmax activation to output class probabilities\n",
        "# we compile with the Adam optimizer at a low learning rate, use categorical cross-entropy as our loss function\n",
        "def make_model(metrics=METRICS, output_bias=None, seed=42):\n",
        "    init_random_seed(seed)\n",
        "    if output_bias is not None:\n",
        "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "    token = keras.layers.Input(shape=(seq_length,), name='input_token')\n",
        "    pos = keras.layers.Input(shape=(seq_length,), name='input_pos')\n",
        "    embed_token = keras.layers.Embedding(input_dim=vocab_size, output_dim=token_embed_size, input_length=seq_length, mask_zero=True, trainable=True, name='token_embedding')(token)\n",
        "    embed_pos = keras.layers.Embedding(input_dim=pos_size, output_dim=pos_embed_size, input_length=seq_length, mask_zero=True, trainable=True, name='pos_embedding')(pos)\n",
        "    concat = keras.layers.Concatenate()([embed_token, embed_pos])\n",
        "    biLSTM = keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(concat)  # 2 directions, 50 units each, concatenated (can change this)\n",
        "    dropout = keras.layers.Dropout(0.5)(biLSTM)\n",
        "    dense = keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias))(dropout)\n",
        "    model = keras.Model(inputs=[token, pos], outputs=dense)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)\n",
        "    return model\n",
        "\n",
        "# early stopping criteria based on area under the curve: will stop if no improvement after 10 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', verbose=1, patience=10, mode='max', restore_best_weights=True)\n",
        "\n",
        "# the number of training epochs we'll use, and the batch size (how many texts are input at once)\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print('**Defining a neural network**')\n",
        "model = make_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence dimensions (n.docs, seq.length):\n",
            "(3375, 105)\n",
            "Input pos dimensions (n.docs, seq.length):\n",
            "(3375, 105)\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):\n",
            "(3375, 105, 4)\n",
            "True\n",
            "True\n",
            "**Defining a neural network**\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 105)]        0           []                               \n",
            "                                                                                                  \n",
            " input_pos (InputLayer)         [(None, 105)]        0           []                               \n",
            "                                                                                                  \n",
            " token_embedding (Embedding)    (None, 105, 128)     1894784     ['input_token[0][0]']            \n",
            "                                                                                                  \n",
            " pos_embedding (Embedding)      (None, 105, 16)      320         ['input_pos[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 105, 144)     0           ['token_embedding[0][0]',        \n",
            "                                                                  'pos_embedding[0][0]']          \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 105, 100)     78000       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 105, 100)     0           ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " time_distributed (TimeDistribu  (None, 105, 4)      404         ['dropout[0][0]']                \n",
            " ted)                                                                                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,973,508\n",
            "Trainable params: 1,973,508\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "2YQ_XC7Z14Lj",
        "outputId": "c364b954-41be-4073-fdff-3bad68741b7c"
      },
      "source": [
        "tf.keras.utils.plot_model(model, to_file='model_pos.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nu2hdE-5IZW",
        "outputId": "917067ba-e3b0-4331-9768-40628e2b69eb"
      },
      "source": [
        "# figure out the label distribution in our fixed-length texts\n",
        "from collections import Counter\n",
        "\n",
        "all_labs = [l for lab in train_labs_padded for l in lab]\n",
        "label_count = Counter(all_labs)\n",
        "total_labs = len(all_labs)\n",
        "print(label_count)\n",
        "print(total_labs)\n",
        "\n",
        "# use this to define an initial model bias\n",
        "initial_bias=[(label_count[0]/total_labs), (label_count[1]/total_labs),\n",
        "              (label_count[2]/total_labs), (label_count[3]/total_labs)]\n",
        "print('Initial bias:')\n",
        "print(initial_bias)\n",
        "\n",
        "# pass the bias to the model and re-evaluate\n",
        "model = make_model(output_bias=initial_bias)\n",
        "results = model.evaluate([X_token, X_pos], y, batch_size=BATCH_SIZE, verbose=0)\n",
        "print(\"Loss: {:0.4f}\".format(results[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({3: 292139, 2: 59095, 0: 1964, 1: 1177})\n",
            "354375\n",
            "Initial bias:\n",
            "[0.005542151675485009, 0.0033213403880070548, 0.1667583774250441, 0.8243781305114638]\n",
            "Loss: 0.9105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydlyQEWf6TDA",
        "outputId": "f29742a1-3319-4962-a028-2554e9773b56"
      },
      "source": [
        "# prepare the dev sequences, pos and labels as numpy arrays\n",
        "dev_X_token = np.array(dev_seqs_padded)\n",
        "dev_X_pos = np.array(dev_pos_padded)\n",
        "dev_y = np.array(dev_labs_onehot)\n",
        "\n",
        "# re-initiate model with bias\n",
        "model = make_model(output_bias=initial_bias)\n",
        "\n",
        "# and fit...\n",
        "model.fit([X_token, X_pos], y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=([dev_X_token, dev_X_pos], dev_y))\n",
        "\n",
        "# save the model\n",
        "model.save('BiLSTM_PoS.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "106/106 [==============================] - 55s 432ms/step - loss: 0.1585 - tp: 315109.0000 - fp: 6825.0000 - tn: 2046357.0000 - fn: 369285.0000 - accuracy: 0.8626 - precision: 0.9788 - recall: 0.4604 - auc: 0.9863 - val_loss: 0.0469 - val_tp: 100511.0000 - val_fp: 1010.0000 - val_tn: 303571.0000 - val_fn: 1016.0000 - val_accuracy: 0.9950 - val_precision: 0.9901 - val_recall: 0.9900 - val_auc: 0.9993\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 44s 413ms/step - loss: 0.0368 - tp: 339466.0000 - fp: 2683.0000 - tn: 1023908.0000 - fn: 2731.0000 - accuracy: 0.9960 - precision: 0.9922 - recall: 0.9920 - auc: 0.9995 - val_loss: 0.0361 - val_tp: 100587.0000 - val_fp: 938.0000 - val_tn: 303643.0000 - val_fn: 940.0000 - val_accuracy: 0.9954 - val_precision: 0.9908 - val_recall: 0.9907 - val_auc: 0.9997\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0256 - tp: 339501.0000 - fp: 2375.0000 - tn: 1024216.0000 - fn: 2696.0000 - accuracy: 0.9963 - precision: 0.9931 - recall: 0.9921 - auc: 0.9998 - val_loss: 0.0298 - val_tp: 100587.0000 - val_fp: 926.0000 - val_tn: 303655.0000 - val_fn: 940.0000 - val_accuracy: 0.9954 - val_precision: 0.9909 - val_recall: 0.9907 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 43s 407ms/step - loss: 0.0169 - tp: 339629.0000 - fp: 1331.0000 - tn: 1025260.0000 - fn: 2568.0000 - accuracy: 0.9972 - precision: 0.9961 - recall: 0.9925 - auc: 0.9999 - val_loss: 0.0272 - val_tp: 100590.0000 - val_fp: 854.0000 - val_tn: 303727.0000 - val_fn: 937.0000 - val_accuracy: 0.9956 - val_precision: 0.9916 - val_recall: 0.9908 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 43s 405ms/step - loss: 0.0120 - tp: 340264.0000 - fp: 864.0000 - tn: 1025727.0000 - fn: 1933.0000 - accuracy: 0.9980 - precision: 0.9975 - recall: 0.9944 - auc: 0.9999 - val_loss: 0.0281 - val_tp: 100598.0000 - val_fp: 824.0000 - val_tn: 303757.0000 - val_fn: 929.0000 - val_accuracy: 0.9957 - val_precision: 0.9919 - val_recall: 0.9908 - val_auc: 0.9993\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0094 - tp: 340787.0000 - fp: 681.0000 - tn: 1025910.0000 - fn: 1410.0000 - accuracy: 0.9985 - precision: 0.9980 - recall: 0.9959 - auc: 0.9999 - val_loss: 0.0297 - val_tp: 100616.0000 - val_fp: 834.0000 - val_tn: 303747.0000 - val_fn: 911.0000 - val_accuracy: 0.9957 - val_precision: 0.9918 - val_recall: 0.9910 - val_auc: 0.9991\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 44s 412ms/step - loss: 0.0077 - tp: 341113.0000 - fp: 573.0000 - tn: 1026018.0000 - fn: 1084.0000 - accuracy: 0.9988 - precision: 0.9983 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0315 - val_tp: 100615.0000 - val_fp: 849.0000 - val_tn: 303732.0000 - val_fn: 912.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9910 - val_auc: 0.9989\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 43s 409ms/step - loss: 0.0065 - tp: 341376.0000 - fp: 518.0000 - tn: 1026073.0000 - fn: 821.0000 - accuracy: 0.9990 - precision: 0.9985 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0349 - val_tp: 100620.0000 - val_fp: 855.0000 - val_tn: 303726.0000 - val_fn: 907.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9911 - val_auc: 0.9986\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0055 - tp: 341503.0000 - fp: 474.0000 - tn: 1026117.0000 - fn: 694.0000 - accuracy: 0.9991 - precision: 0.9986 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0330 - val_tp: 100616.0000 - val_fp: 839.0000 - val_tn: 303742.0000 - val_fn: 911.0000 - val_accuracy: 0.9957 - val_precision: 0.9917 - val_recall: 0.9910 - val_auc: 0.9987\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 44s 416ms/step - loss: 0.0046 - tp: 341645.0000 - fp: 379.0000 - tn: 1026212.0000 - fn: 552.0000 - accuracy: 0.9993 - precision: 0.9989 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0371 - val_tp: 100624.0000 - val_fp: 852.0000 - val_tn: 303729.0000 - val_fn: 903.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9911 - val_auc: 0.9984\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 45s 429ms/step - loss: 0.0039 - tp: 341733.0000 - fp: 344.0000 - tn: 1026247.0000 - fn: 464.0000 - accuracy: 0.9994 - precision: 0.9990 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0363 - val_tp: 100615.0000 - val_fp: 850.0000 - val_tn: 303731.0000 - val_fn: 912.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9910 - val_auc: 0.9984\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0034 - tp: 341808.0000 - fp: 291.0000 - tn: 1026300.0000 - fn: 389.0000 - accuracy: 0.9995 - precision: 0.9991 - recall: 0.9989 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 43s 410ms/step - loss: 0.0034 - tp: 341808.0000 - fp: 291.0000 - tn: 1026300.0000 - fn: 389.0000 - accuracy: 0.9995 - precision: 0.9991 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0393 - val_tp: 100611.0000 - val_fp: 863.0000 - val_tn: 303718.0000 - val_fn: 916.0000 - val_accuracy: 0.9956 - val_precision: 0.9915 - val_recall: 0.9910 - val_auc: 0.9982\n",
            "Epoch 00012: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y0mQIkT80Tg",
        "outputId": "e73dc5e0-5e8c-4433-e6b2-6af9b3a45dfc"
      },
      "source": [
        "# use argmax to figure out the class with highest probability per token\n",
        "preds = np.argmax(model.predict([dev_seqs_padded, dev_pos_padded]), axis=-1)\n",
        "flat_preds = [p for pred in preds for p in pred]\n",
        "print(Counter(flat_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({3: 91627, 2: 12638})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFlY2XVs9TvL"
      },
      "source": [
        "#### Down-weighting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUOBv0_e9W2m"
      },
      "source": [
        "import copy\n",
        "\n",
        "def down_weight(labs_onehot, weight=1):\n",
        "    weights_onehot = copy.deepcopy(labs_onehot)\n",
        "\n",
        "    # our first-pass class weights: normal for named entities (0 and 1), down-weighted for non named entities (2 and 3)\n",
        "    class_wts = [1, 1, weight, weight]\n",
        "\n",
        "    # apply our weights to the label lists\n",
        "    for i, labs in enumerate(weights_onehot):\n",
        "        for j, lablist in enumerate(labs):\n",
        "            lablistaslist = lablist.tolist()\n",
        "            whichismax = lablistaslist.index(max(lablistaslist))\n",
        "            weights_onehot[i][j][whichismax] = class_wts[whichismax]\n",
        "    \n",
        "    return weights_onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0mQ41LT9abd",
        "outputId": "2919ff7e-8f27-4ab4-c72e-286fde66a80a"
      },
      "source": [
        "# now try the weighted one-hot encoding\n",
        "downweight_models = {}\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n",
        "    train_weights_onehot = down_weight(train_labs_onehot, weight)\n",
        "    y = np.array(train_weights_onehot)\n",
        "    print('Down-weighting:', weight)\n",
        "    print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):', np.shape(y))\n",
        "\n",
        "    downweight_model = make_model(output_bias=initial_bias)\n",
        "    downweight_model.fit([X_token, X_pos], y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=([dev_X_token, dev_X_pos], dev_y))\n",
        "\n",
        "    downweight_models[weight] = downweight_model\n",
        "    if weight == 1.0:\n",
        "        downweight_model.save('BiLSTM_PoS.h5')\n",
        "        print(f'Model saved at BiLSTM_PoS.h5')\n",
        "    else:\n",
        "        downweight_model.save(f'BiLSTM_PoS_downweight_{weight}.h5')\n",
        "        print(f'Model saved at BiLSTM_PoS_downweight_{weight}.h5')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Down-weighting: 0.1\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 51s 391ms/step - loss: 0.0309 - tp: 307613.0000 - fp: 5008.0000 - tn: 2048174.0000 - fn: 376781.0000 - accuracy: 0.7482 - precision: 0.9840 - recall: 0.4495 - auc: 0.9865 - val_loss: 0.0577 - val_tp: 100446.0000 - val_fp: 852.0000 - val_tn: 303729.0000 - val_fn: 1081.0000 - val_accuracy: 0.9952 - val_precision: 0.9916 - val_recall: 0.9894 - val_auc: 0.9997\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 39s 368ms/step - loss: 0.0142 - tp: 335676.0000 - fp: 1594.0000 - tn: 1024997.0000 - fn: 6521.0000 - accuracy: 0.7491 - precision: 0.9953 - recall: 0.9809 - auc: 0.9998 - val_loss: 0.0321 - val_tp: 100334.0000 - val_fp: 509.0000 - val_tn: 304072.0000 - val_fn: 1193.0000 - val_accuracy: 0.9958 - val_precision: 0.9950 - val_recall: 0.9882 - val_auc: 0.9999\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 0.0088 - tp: 337794.0000 - fp: 1728.0000 - tn: 1024863.0000 - fn: 4403.0000 - accuracy: 0.7497 - precision: 0.9949 - recall: 0.9871 - auc: 0.9999 - val_loss: 0.0227 - val_tp: 100556.0000 - val_fp: 525.0000 - val_tn: 304056.0000 - val_fn: 971.0000 - val_accuracy: 0.9963 - val_precision: 0.9948 - val_recall: 0.9904 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 39s 369ms/step - loss: 0.0056 - tp: 340028.0000 - fp: 1158.0000 - tn: 1025433.0000 - fn: 2169.0000 - accuracy: 0.7506 - precision: 0.9966 - recall: 0.9937 - auc: 0.9999 - val_loss: 0.0218 - val_tp: 100633.0000 - val_fp: 578.0000 - val_tn: 304003.0000 - val_fn: 894.0000 - val_accuracy: 0.9964 - val_precision: 0.9943 - val_recall: 0.9912 - val_auc: 0.9997\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 39s 368ms/step - loss: 0.0035 - tp: 340858.0000 - fp: 939.0000 - tn: 1025652.0000 - fn: 1339.0000 - accuracy: 0.7510 - precision: 0.9973 - recall: 0.9961 - auc: 0.9999 - val_loss: 0.0224 - val_tp: 100616.0000 - val_fp: 618.0000 - val_tn: 303963.0000 - val_fn: 911.0000 - val_accuracy: 0.9962 - val_precision: 0.9939 - val_recall: 0.9910 - val_auc: 0.9997\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 0.0024 - tp: 341114.0000 - fp: 851.0000 - tn: 1025740.0000 - fn: 1083.0000 - accuracy: 0.7512 - precision: 0.9975 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0242 - val_tp: 100601.0000 - val_fp: 698.0000 - val_tn: 303883.0000 - val_fn: 926.0000 - val_accuracy: 0.9960 - val_precision: 0.9931 - val_recall: 0.9909 - val_auc: 0.9995\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 0.0017 - tp: 341339.0000 - fp: 684.0000 - tn: 1025907.0000 - fn: 858.0000 - accuracy: 0.7513 - precision: 0.9980 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0257 - val_tp: 100593.0000 - val_fp: 720.0000 - val_tn: 303861.0000 - val_fn: 934.0000 - val_accuracy: 0.9959 - val_precision: 0.9929 - val_recall: 0.9908 - val_auc: 0.9994\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 39s 367ms/step - loss: 0.0014 - tp: 341469.0000 - fp: 615.0000 - tn: 1025976.0000 - fn: 728.0000 - accuracy: 0.7514 - precision: 0.9982 - recall: 0.9979 - auc: 0.9999 - val_loss: 0.0261 - val_tp: 100654.0000 - val_fp: 712.0000 - val_tn: 303869.0000 - val_fn: 873.0000 - val_accuracy: 0.9961 - val_precision: 0.9930 - val_recall: 0.9914 - val_auc: 0.9992\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0011 - tp: 341569.0000 - fp: 552.0000 - tn: 1026039.0000 - fn: 628.0000 - accuracy: 0.7515 - precision: 0.9984 - recall: 0.9982 - auc: 0.9999 - val_loss: 0.0274 - val_tp: 100665.0000 - val_fp: 748.0000 - val_tn: 303833.0000 - val_fn: 862.0000 - val_accuracy: 0.9960 - val_precision: 0.9926 - val_recall: 0.9915 - val_auc: 0.9991\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 39s 369ms/step - loss: 8.7248e-04 - tp: 341694.0000 - fp: 446.0000 - tn: 1026145.0000 - fn: 503.0000 - accuracy: 0.7516 - precision: 0.9987 - recall: 0.9985 - auc: 0.9999 - val_loss: 0.0276 - val_tp: 100683.0000 - val_fp: 719.0000 - val_tn: 303862.0000 - val_fn: 844.0000 - val_accuracy: 0.9962 - val_precision: 0.9929 - val_recall: 0.9917 - val_auc: 0.9990\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 7.4396e-04 - tp: 341763.0000 - fp: 373.0000 - tn: 1026218.0000 - fn: 434.0000 - accuracy: 0.7516 - precision: 0.9989 - recall: 0.9987 - auc: 0.9999 - val_loss: 0.0297 - val_tp: 100628.0000 - val_fp: 773.0000 - val_tn: 303808.0000 - val_fn: 899.0000 - val_accuracy: 0.9959 - val_precision: 0.9924 - val_recall: 0.9911 - val_auc: 0.9989\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 6.5918e-04 - tp: 341831.0000 - fp: 327.0000 - tn: 1026264.0000 - fn: 366.0000 - accuracy: 0.7516 - precision: 0.9990 - recall: 0.9989 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 39s 369ms/step - loss: 6.5918e-04 - tp: 341831.0000 - fp: 327.0000 - tn: 1026264.0000 - fn: 366.0000 - accuracy: 0.7516 - precision: 0.9990 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0300 - val_tp: 100680.0000 - val_fp: 753.0000 - val_tn: 303828.0000 - val_fn: 847.0000 - val_accuracy: 0.9961 - val_precision: 0.9926 - val_recall: 0.9917 - val_auc: 0.9988\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_downweight_0.1.h5\n",
            "\n",
            "Down-weighting: 0.2\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 50s 391ms/step - loss: 0.0472 - tp: 413388.0000 - fp: 6779.0000 - tn: 1324393.0000 - fn: 30336.0000 - accuracy: 0.8029 - precision: 0.9839 - recall: 0.9316 - auc: 0.9977 - val_loss: 0.0470 - val_tp: 100541.0000 - val_fp: 937.0000 - val_tn: 303644.0000 - val_fn: 986.0000 - val_accuracy: 0.9953 - val_precision: 0.9908 - val_recall: 0.9903 - val_auc: 0.9996\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0189 - tp: 338332.0000 - fp: 1908.0000 - tn: 1024683.0000 - fn: 3865.0000 - accuracy: 0.7487 - precision: 0.9944 - recall: 0.9887 - auc: 0.9998 - val_loss: 0.0294 - val_tp: 100447.0000 - val_fp: 633.0000 - val_tn: 303948.0000 - val_fn: 1080.0000 - val_accuracy: 0.9958 - val_precision: 0.9937 - val_recall: 0.9894 - val_auc: 0.9999\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 0.0117 - tp: 338645.0000 - fp: 1285.0000 - tn: 1025306.0000 - fn: 3552.0000 - accuracy: 0.7497 - precision: 0.9962 - recall: 0.9896 - auc: 0.9999 - val_loss: 0.0225 - val_tp: 100550.0000 - val_fp: 540.0000 - val_tn: 304041.0000 - val_fn: 977.0000 - val_accuracy: 0.9963 - val_precision: 0.9947 - val_recall: 0.9904 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0076 - tp: 340223.0000 - fp: 908.0000 - tn: 1025683.0000 - fn: 1974.0000 - accuracy: 0.7505 - precision: 0.9973 - recall: 0.9942 - auc: 0.9999 - val_loss: 0.0217 - val_tp: 100629.0000 - val_fp: 542.0000 - val_tn: 304039.0000 - val_fn: 898.0000 - val_accuracy: 0.9965 - val_precision: 0.9946 - val_recall: 0.9912 - val_auc: 0.9997\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0051 - tp: 341017.0000 - fp: 744.0000 - tn: 1025847.0000 - fn: 1180.0000 - accuracy: 0.7510 - precision: 0.9978 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0223 - val_tp: 100633.0000 - val_fp: 587.0000 - val_tn: 303994.0000 - val_fn: 894.0000 - val_accuracy: 0.9964 - val_precision: 0.9942 - val_recall: 0.9912 - val_auc: 0.9996\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 0.0037 - tp: 341243.0000 - fp: 678.0000 - tn: 1025913.0000 - fn: 954.0000 - accuracy: 0.7512 - precision: 0.9980 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0240 - val_tp: 100630.0000 - val_fp: 674.0000 - val_tn: 303907.0000 - val_fn: 897.0000 - val_accuracy: 0.9961 - val_precision: 0.9933 - val_recall: 0.9912 - val_auc: 0.9994\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0027 - tp: 341482.0000 - fp: 525.0000 - tn: 1026066.0000 - fn: 715.0000 - accuracy: 0.7514 - precision: 0.9985 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0252 - val_tp: 100617.0000 - val_fp: 694.0000 - val_tn: 303887.0000 - val_fn: 910.0000 - val_accuracy: 0.9961 - val_precision: 0.9931 - val_recall: 0.9910 - val_auc: 0.9993\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0022 - tp: 341573.0000 - fp: 498.0000 - tn: 1026093.0000 - fn: 624.0000 - accuracy: 0.7514 - precision: 0.9985 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0263 - val_tp: 100675.0000 - val_fp: 698.0000 - val_tn: 303883.0000 - val_fn: 852.0000 - val_accuracy: 0.9962 - val_precision: 0.9931 - val_recall: 0.9916 - val_auc: 0.9991\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0018 - tp: 341661.0000 - fp: 422.0000 - tn: 1026169.0000 - fn: 536.0000 - accuracy: 0.7515 - precision: 0.9988 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0272 - val_tp: 100657.0000 - val_fp: 745.0000 - val_tn: 303836.0000 - val_fn: 870.0000 - val_accuracy: 0.9960 - val_precision: 0.9927 - val_recall: 0.9914 - val_auc: 0.9991\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0014 - tp: 341764.0000 - fp: 371.0000 - tn: 1026220.0000 - fn: 433.0000 - accuracy: 0.7516 - precision: 0.9989 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0277 - val_tp: 100679.0000 - val_fp: 727.0000 - val_tn: 303854.0000 - val_fn: 848.0000 - val_accuracy: 0.9961 - val_precision: 0.9928 - val_recall: 0.9916 - val_auc: 0.9990\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0012 - tp: 341821.0000 - fp: 329.0000 - tn: 1026262.0000 - fn: 376.0000 - accuracy: 0.7516 - precision: 0.9990 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0297 - val_tp: 100682.0000 - val_fp: 753.0000 - val_tn: 303828.0000 - val_fn: 845.0000 - val_accuracy: 0.9961 - val_precision: 0.9926 - val_recall: 0.9917 - val_auc: 0.9988\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0010 - tp: 341895.0000 - fp: 265.0000 - tn: 1026326.0000 - fn: 302.0000 - accuracy: 0.7517 - precision: 0.9992 - recall: 0.9991 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 39s 369ms/step - loss: 0.0010 - tp: 341895.0000 - fp: 265.0000 - tn: 1026326.0000 - fn: 302.0000 - accuracy: 0.7517 - precision: 0.9992 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0310 - val_tp: 100710.0000 - val_fp: 734.0000 - val_tn: 303847.0000 - val_fn: 817.0000 - val_accuracy: 0.9962 - val_precision: 0.9928 - val_recall: 0.9920 - val_auc: 0.9988\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_downweight_0.2.h5\n",
            "\n",
            "Down-weighting: 0.3\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 50s 388ms/step - loss: 0.0622 - tp: 414642.0000 - fp: 7054.0000 - tn: 1324118.0000 - fn: 29082.0000 - accuracy: 0.8028 - precision: 0.9833 - recall: 0.9345 - auc: 0.9976 - val_loss: 0.0442 - val_tp: 100558.0000 - val_fp: 942.0000 - val_tn: 303639.0000 - val_fn: 969.0000 - val_accuracy: 0.9953 - val_precision: 0.9907 - val_recall: 0.9905 - val_auc: 0.9996\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0224 - tp: 339015.0000 - fp: 2289.0000 - tn: 1024302.0000 - fn: 3182.0000 - accuracy: 0.7483 - precision: 0.9933 - recall: 0.9907 - auc: 0.9998 - val_loss: 0.0298 - val_tp: 100530.0000 - val_fp: 837.0000 - val_tn: 303744.0000 - val_fn: 997.0000 - val_accuracy: 0.9955 - val_precision: 0.9917 - val_recall: 0.9902 - val_auc: 0.9998\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 39s 368ms/step - loss: 0.0139 - tp: 338984.0000 - fp: 1238.0000 - tn: 1025353.0000 - fn: 3213.0000 - accuracy: 0.7495 - precision: 0.9964 - recall: 0.9906 - auc: 0.9999 - val_loss: 0.0232 - val_tp: 100533.0000 - val_fp: 618.0000 - val_tn: 303963.0000 - val_fn: 994.0000 - val_accuracy: 0.9960 - val_precision: 0.9939 - val_recall: 0.9902 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0091 - tp: 340154.0000 - fp: 887.0000 - tn: 1025704.0000 - fn: 2043.0000 - accuracy: 0.7503 - precision: 0.9974 - recall: 0.9940 - auc: 0.9999 - val_loss: 0.0224 - val_tp: 100587.0000 - val_fp: 571.0000 - val_tn: 304010.0000 - val_fn: 940.0000 - val_accuracy: 0.9963 - val_precision: 0.9944 - val_recall: 0.9907 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 0.0064 - tp: 340984.0000 - fp: 714.0000 - tn: 1025877.0000 - fn: 1213.0000 - accuracy: 0.7509 - precision: 0.9979 - recall: 0.9965 - auc: 0.9999 - val_loss: 0.0228 - val_tp: 100625.0000 - val_fp: 591.0000 - val_tn: 303990.0000 - val_fn: 902.0000 - val_accuracy: 0.9963 - val_precision: 0.9942 - val_recall: 0.9911 - val_auc: 0.9995\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 39s 369ms/step - loss: 0.0047 - tp: 341267.0000 - fp: 615.0000 - tn: 1025976.0000 - fn: 930.0000 - accuracy: 0.7511 - precision: 0.9982 - recall: 0.9973 - auc: 1.0000 - val_loss: 0.0248 - val_tp: 100635.0000 - val_fp: 704.0000 - val_tn: 303877.0000 - val_fn: 892.0000 - val_accuracy: 0.9961 - val_precision: 0.9931 - val_recall: 0.9912 - val_auc: 0.9993\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 39s 368ms/step - loss: 0.0036 - tp: 341488.0000 - fp: 505.0000 - tn: 1026086.0000 - fn: 709.0000 - accuracy: 0.7513 - precision: 0.9985 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0261 - val_tp: 100645.0000 - val_fp: 728.0000 - val_tn: 303853.0000 - val_fn: 882.0000 - val_accuracy: 0.9960 - val_precision: 0.9928 - val_recall: 0.9913 - val_auc: 0.9992\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 39s 369ms/step - loss: 0.0030 - tp: 341578.0000 - fp: 465.0000 - tn: 1026126.0000 - fn: 619.0000 - accuracy: 0.7514 - precision: 0.9986 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0272 - val_tp: 100685.0000 - val_fp: 720.0000 - val_tn: 303861.0000 - val_fn: 842.0000 - val_accuracy: 0.9962 - val_precision: 0.9929 - val_recall: 0.9917 - val_auc: 0.9990\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 39s 369ms/step - loss: 0.0025 - tp: 341664.0000 - fp: 422.0000 - tn: 1026169.0000 - fn: 533.0000 - accuracy: 0.7515 - precision: 0.9988 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0284 - val_tp: 100661.0000 - val_fp: 753.0000 - val_tn: 303828.0000 - val_fn: 866.0000 - val_accuracy: 0.9960 - val_precision: 0.9926 - val_recall: 0.9915 - val_auc: 0.9990\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 39s 373ms/step - loss: 0.0020 - tp: 341761.0000 - fp: 347.0000 - tn: 1026244.0000 - fn: 436.0000 - accuracy: 0.7516 - precision: 0.9990 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0304 - val_tp: 100671.0000 - val_fp: 765.0000 - val_tn: 303816.0000 - val_fn: 856.0000 - val_accuracy: 0.9960 - val_precision: 0.9925 - val_recall: 0.9916 - val_auc: 0.9988\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0017 - tp: 341830.0000 - fp: 302.0000 - tn: 1026289.0000 - fn: 367.0000 - accuracy: 0.7516 - precision: 0.9991 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0310 - val_tp: 100668.0000 - val_fp: 769.0000 - val_tn: 303812.0000 - val_fn: 859.0000 - val_accuracy: 0.9960 - val_precision: 0.9924 - val_recall: 0.9915 - val_auc: 0.9988\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0014 - tp: 341892.0000 - fp: 266.0000 - tn: 1026325.0000 - fn: 305.0000 - accuracy: 0.7517 - precision: 0.9992 - recall: 0.9991 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 39s 373ms/step - loss: 0.0014 - tp: 341892.0000 - fp: 266.0000 - tn: 1026325.0000 - fn: 305.0000 - accuracy: 0.7517 - precision: 0.9992 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0320 - val_tp: 100677.0000 - val_fp: 769.0000 - val_tn: 303812.0000 - val_fn: 850.0000 - val_accuracy: 0.9960 - val_precision: 0.9924 - val_recall: 0.9916 - val_auc: 0.9987\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_downweight_0.3.h5\n",
            "\n",
            "Down-weighting: 0.4\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 51s 393ms/step - loss: 0.0766 - tp: 415115.0000 - fp: 7215.0000 - tn: 1323957.0000 - fn: 28609.0000 - accuracy: 0.8027 - precision: 0.9829 - recall: 0.9355 - auc: 0.9976 - val_loss: 0.0434 - val_tp: 100566.0000 - val_fp: 948.0000 - val_tn: 303633.0000 - val_fn: 961.0000 - val_accuracy: 0.9953 - val_precision: 0.9907 - val_recall: 0.9905 - val_auc: 0.9995\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0253 - tp: 339302.0000 - fp: 2456.0000 - tn: 1024135.0000 - fn: 2895.0000 - accuracy: 0.7482 - precision: 0.9928 - recall: 0.9915 - auc: 0.9997 - val_loss: 0.0308 - val_tp: 100560.0000 - val_fp: 918.0000 - val_tn: 303663.0000 - val_fn: 967.0000 - val_accuracy: 0.9954 - val_precision: 0.9910 - val_recall: 0.9905 - val_auc: 0.9998\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0160 - tp: 339175.0000 - fp: 1415.0000 - tn: 1025176.0000 - fn: 3022.0000 - accuracy: 0.7492 - precision: 0.9958 - recall: 0.9912 - auc: 0.9999 - val_loss: 0.0242 - val_tp: 100532.0000 - val_fp: 729.0000 - val_tn: 303852.0000 - val_fn: 995.0000 - val_accuracy: 0.9958 - val_precision: 0.9928 - val_recall: 0.9902 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0105 - tp: 340047.0000 - fp: 874.0000 - tn: 1025717.0000 - fn: 2150.0000 - accuracy: 0.7501 - precision: 0.9974 - recall: 0.9937 - auc: 0.9999 - val_loss: 0.0231 - val_tp: 100548.0000 - val_fp: 623.0000 - val_tn: 303958.0000 - val_fn: 979.0000 - val_accuracy: 0.9961 - val_precision: 0.9938 - val_recall: 0.9904 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0074 - tp: 340842.0000 - fp: 691.0000 - tn: 1025900.0000 - fn: 1355.0000 - accuracy: 0.7507 - precision: 0.9980 - recall: 0.9960 - auc: 0.9999 - val_loss: 0.0237 - val_tp: 100601.0000 - val_fp: 662.0000 - val_tn: 303919.0000 - val_fn: 926.0000 - val_accuracy: 0.9961 - val_precision: 0.9935 - val_recall: 0.9909 - val_auc: 0.9994\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0056 - tp: 341250.0000 - fp: 613.0000 - tn: 1025978.0000 - fn: 947.0000 - accuracy: 0.7510 - precision: 0.9982 - recall: 0.9972 - auc: 1.0000 - val_loss: 0.0255 - val_tp: 100635.0000 - val_fp: 735.0000 - val_tn: 303846.0000 - val_fn: 892.0000 - val_accuracy: 0.9960 - val_precision: 0.9927 - val_recall: 0.9912 - val_auc: 0.9993\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0043 - tp: 341466.0000 - fp: 518.0000 - tn: 1026073.0000 - fn: 731.0000 - accuracy: 0.7513 - precision: 0.9985 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0274 - val_tp: 100634.0000 - val_fp: 768.0000 - val_tn: 303813.0000 - val_fn: 893.0000 - val_accuracy: 0.9959 - val_precision: 0.9924 - val_recall: 0.9912 - val_auc: 0.9992\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0036 - tp: 341582.0000 - fp: 456.0000 - tn: 1026135.0000 - fn: 615.0000 - accuracy: 0.7514 - precision: 0.9987 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0287 - val_tp: 100669.0000 - val_fp: 760.0000 - val_tn: 303821.0000 - val_fn: 858.0000 - val_accuracy: 0.9960 - val_precision: 0.9925 - val_recall: 0.9915 - val_auc: 0.9989\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0031 - tp: 341642.0000 - fp: 432.0000 - tn: 1026159.0000 - fn: 555.0000 - accuracy: 0.7514 - precision: 0.9987 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0292 - val_tp: 100658.0000 - val_fp: 760.0000 - val_tn: 303821.0000 - val_fn: 869.0000 - val_accuracy: 0.9960 - val_precision: 0.9925 - val_recall: 0.9914 - val_auc: 0.9989\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 39s 369ms/step - loss: 0.0025 - tp: 341759.0000 - fp: 340.0000 - tn: 1026251.0000 - fn: 438.0000 - accuracy: 0.7515 - precision: 0.9990 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0334 - val_tp: 100652.0000 - val_fp: 813.0000 - val_tn: 303768.0000 - val_fn: 875.0000 - val_accuracy: 0.9958 - val_precision: 0.9920 - val_recall: 0.9914 - val_auc: 0.9986\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0021 - tp: 341816.0000 - fp: 303.0000 - tn: 1026288.0000 - fn: 381.0000 - accuracy: 0.7516 - precision: 0.9991 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0310 - val_tp: 100651.0000 - val_fp: 764.0000 - val_tn: 303817.0000 - val_fn: 876.0000 - val_accuracy: 0.9960 - val_precision: 0.9925 - val_recall: 0.9914 - val_auc: 0.9988\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0018 - tp: 341890.0000 - fp: 252.0000 - tn: 1026339.0000 - fn: 307.0000 - accuracy: 0.7516 - precision: 0.9993 - recall: 0.9991 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0018 - tp: 341890.0000 - fp: 252.0000 - tn: 1026339.0000 - fn: 307.0000 - accuracy: 0.7516 - precision: 0.9993 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0333 - val_tp: 100670.0000 - val_fp: 787.0000 - val_tn: 303794.0000 - val_fn: 857.0000 - val_accuracy: 0.9960 - val_precision: 0.9922 - val_recall: 0.9916 - val_auc: 0.9986\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_downweight_0.4.h5\n",
            "\n",
            "Down-weighting: 0.5\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 50s 390ms/step - loss: 0.0906 - tp: 415353.0000 - fp: 7346.0000 - tn: 1323826.0000 - fn: 28371.0000 - accuracy: 0.8026 - precision: 0.9826 - recall: 0.9361 - auc: 0.9975 - val_loss: 0.0436 - val_tp: 100527.0000 - val_fp: 970.0000 - val_tn: 303611.0000 - val_fn: 1000.0000 - val_accuracy: 0.9951 - val_precision: 0.9904 - val_recall: 0.9902 - val_auc: 0.9995\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0278 - tp: 339391.0000 - fp: 2576.0000 - tn: 1024015.0000 - fn: 2806.0000 - accuracy: 0.7481 - precision: 0.9925 - recall: 0.9918 - auc: 0.9997 - val_loss: 0.0318 - val_tp: 100575.0000 - val_fp: 931.0000 - val_tn: 303650.0000 - val_fn: 952.0000 - val_accuracy: 0.9954 - val_precision: 0.9908 - val_recall: 0.9906 - val_auc: 0.9998\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 0.0179 - tp: 339293.0000 - fp: 1625.0000 - tn: 1024966.0000 - fn: 2904.0000 - accuracy: 0.7489 - precision: 0.9952 - recall: 0.9915 - auc: 0.9999 - val_loss: 0.0251 - val_tp: 100545.0000 - val_fp: 789.0000 - val_tn: 303792.0000 - val_fn: 982.0000 - val_accuracy: 0.9956 - val_precision: 0.9922 - val_recall: 0.9903 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 39s 369ms/step - loss: 0.0117 - tp: 339984.0000 - fp: 896.0000 - tn: 1025695.0000 - fn: 2213.0000 - accuracy: 0.7500 - precision: 0.9974 - recall: 0.9935 - auc: 0.9999 - val_loss: 0.0237 - val_tp: 100564.0000 - val_fp: 683.0000 - val_tn: 303898.0000 - val_fn: 963.0000 - val_accuracy: 0.9959 - val_precision: 0.9933 - val_recall: 0.9905 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0083 - tp: 340698.0000 - fp: 706.0000 - tn: 1025885.0000 - fn: 1499.0000 - accuracy: 0.7506 - precision: 0.9979 - recall: 0.9956 - auc: 0.9999 - val_loss: 0.0246 - val_tp: 100594.0000 - val_fp: 711.0000 - val_tn: 303870.0000 - val_fn: 933.0000 - val_accuracy: 0.9960 - val_precision: 0.9930 - val_recall: 0.9908 - val_auc: 0.9994\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0064 - tp: 341179.0000 - fp: 604.0000 - tn: 1025987.0000 - fn: 1018.0000 - accuracy: 0.7510 - precision: 0.9982 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0264 - val_tp: 100616.0000 - val_fp: 748.0000 - val_tn: 303833.0000 - val_fn: 911.0000 - val_accuracy: 0.9959 - val_precision: 0.9926 - val_recall: 0.9910 - val_auc: 0.9992\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 0.0050 - tp: 341425.0000 - fp: 515.0000 - tn: 1026076.0000 - fn: 772.0000 - accuracy: 0.7512 - precision: 0.9985 - recall: 0.9977 - auc: 1.0000 - val_loss: 0.0283 - val_tp: 100610.0000 - val_fp: 801.0000 - val_tn: 303780.0000 - val_fn: 917.0000 - val_accuracy: 0.9958 - val_precision: 0.9921 - val_recall: 0.9910 - val_auc: 0.9991\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0042 - tp: 341563.0000 - fp: 451.0000 - tn: 1026140.0000 - fn: 634.0000 - accuracy: 0.7513 - precision: 0.9987 - recall: 0.9981 - auc: 1.0000 - val_loss: 0.0296 - val_tp: 100649.0000 - val_fp: 788.0000 - val_tn: 303793.0000 - val_fn: 878.0000 - val_accuracy: 0.9959 - val_precision: 0.9922 - val_recall: 0.9914 - val_auc: 0.9989\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0036 - tp: 341627.0000 - fp: 429.0000 - tn: 1026162.0000 - fn: 570.0000 - accuracy: 0.7514 - precision: 0.9987 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0300 - val_tp: 100653.0000 - val_fp: 785.0000 - val_tn: 303796.0000 - val_fn: 874.0000 - val_accuracy: 0.9959 - val_precision: 0.9923 - val_recall: 0.9914 - val_auc: 0.9989\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0029 - tp: 341742.0000 - fp: 349.0000 - tn: 1026242.0000 - fn: 455.0000 - accuracy: 0.7515 - precision: 0.9990 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0347 - val_tp: 100651.0000 - val_fp: 826.0000 - val_tn: 303755.0000 - val_fn: 876.0000 - val_accuracy: 0.9958 - val_precision: 0.9919 - val_recall: 0.9914 - val_auc: 0.9985\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0025 - tp: 341811.0000 - fp: 296.0000 - tn: 1026295.0000 - fn: 386.0000 - accuracy: 0.7516 - precision: 0.9991 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0316 - val_tp: 100663.0000 - val_fp: 779.0000 - val_tn: 303802.0000 - val_fn: 864.0000 - val_accuracy: 0.9960 - val_precision: 0.9923 - val_recall: 0.9915 - val_auc: 0.9988\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0021 - tp: 341882.0000 - fp: 250.0000 - tn: 1026341.0000 - fn: 315.0000 - accuracy: 0.7516 - precision: 0.9993 - recall: 0.9991 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0021 - tp: 341882.0000 - fp: 250.0000 - tn: 1026341.0000 - fn: 315.0000 - accuracy: 0.7516 - precision: 0.9993 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0350 - val_tp: 100656.0000 - val_fp: 809.0000 - val_tn: 303772.0000 - val_fn: 871.0000 - val_accuracy: 0.9959 - val_precision: 0.9920 - val_recall: 0.9914 - val_auc: 0.9986\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_downweight_0.5.h5\n",
            "\n",
            "Down-weighting: 0.6\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 51s 396ms/step - loss: 0.1044 - tp: 415476.0000 - fp: 7455.0000 - tn: 1323717.0000 - fn: 28248.0000 - accuracy: 0.8025 - precision: 0.9824 - recall: 0.9363 - auc: 0.9974 - val_loss: 0.0443 - val_tp: 100522.0000 - val_fp: 991.0000 - val_tn: 303590.0000 - val_fn: 1005.0000 - val_accuracy: 0.9951 - val_precision: 0.9902 - val_recall: 0.9901 - val_auc: 0.9995\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0300 - tp: 339428.0000 - fp: 2630.0000 - tn: 1023961.0000 - fn: 2769.0000 - accuracy: 0.7481 - precision: 0.9923 - recall: 0.9919 - auc: 0.9996 - val_loss: 0.0328 - val_tp: 100576.0000 - val_fp: 937.0000 - val_tn: 303644.0000 - val_fn: 951.0000 - val_accuracy: 0.9954 - val_precision: 0.9908 - val_recall: 0.9906 - val_auc: 0.9998\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0197 - tp: 339344.0000 - fp: 1845.0000 - tn: 1024746.0000 - fn: 2853.0000 - accuracy: 0.7487 - precision: 0.9946 - recall: 0.9917 - auc: 0.9999 - val_loss: 0.0261 - val_tp: 100559.0000 - val_fp: 842.0000 - val_tn: 303739.0000 - val_fn: 968.0000 - val_accuracy: 0.9955 - val_precision: 0.9917 - val_recall: 0.9905 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0128 - tp: 339909.0000 - fp: 954.0000 - tn: 1025637.0000 - fn: 2288.0000 - accuracy: 0.7498 - precision: 0.9972 - recall: 0.9933 - auc: 0.9999 - val_loss: 0.0244 - val_tp: 100564.0000 - val_fp: 733.0000 - val_tn: 303848.0000 - val_fn: 963.0000 - val_accuracy: 0.9958 - val_precision: 0.9928 - val_recall: 0.9905 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0092 - tp: 340615.0000 - fp: 746.0000 - tn: 1025845.0000 - fn: 1582.0000 - accuracy: 0.7504 - precision: 0.9978 - recall: 0.9954 - auc: 0.9999 - val_loss: 0.0254 - val_tp: 100586.0000 - val_fp: 748.0000 - val_tn: 303833.0000 - val_fn: 941.0000 - val_accuracy: 0.9958 - val_precision: 0.9926 - val_recall: 0.9907 - val_auc: 0.9993\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0071 - tp: 341137.0000 - fp: 626.0000 - tn: 1025965.0000 - fn: 1060.0000 - accuracy: 0.7509 - precision: 0.9982 - recall: 0.9969 - auc: 0.9999 - val_loss: 0.0273 - val_tp: 100607.0000 - val_fp: 776.0000 - val_tn: 303805.0000 - val_fn: 920.0000 - val_accuracy: 0.9958 - val_precision: 0.9923 - val_recall: 0.9909 - val_auc: 0.9992\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0056 - tp: 341374.0000 - fp: 523.0000 - tn: 1026068.0000 - fn: 823.0000 - accuracy: 0.7511 - precision: 0.9985 - recall: 0.9976 - auc: 1.0000 - val_loss: 0.0293 - val_tp: 100611.0000 - val_fp: 820.0000 - val_tn: 303761.0000 - val_fn: 916.0000 - val_accuracy: 0.9957 - val_precision: 0.9919 - val_recall: 0.9910 - val_auc: 0.9991\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0047 - tp: 341538.0000 - fp: 462.0000 - tn: 1026129.0000 - fn: 659.0000 - accuracy: 0.7513 - precision: 0.9986 - recall: 0.9981 - auc: 1.0000 - val_loss: 0.0305 - val_tp: 100639.0000 - val_fp: 805.0000 - val_tn: 303776.0000 - val_fn: 888.0000 - val_accuracy: 0.9958 - val_precision: 0.9921 - val_recall: 0.9913 - val_auc: 0.9988\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0041 - tp: 341610.0000 - fp: 425.0000 - tn: 1026166.0000 - fn: 587.0000 - accuracy: 0.7513 - precision: 0.9988 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0307 - val_tp: 100633.0000 - val_fp: 807.0000 - val_tn: 303774.0000 - val_fn: 894.0000 - val_accuracy: 0.9958 - val_precision: 0.9920 - val_recall: 0.9912 - val_auc: 0.9988\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0033 - tp: 341727.0000 - fp: 351.0000 - tn: 1026240.0000 - fn: 470.0000 - accuracy: 0.7515 - precision: 0.9990 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0355 - val_tp: 100638.0000 - val_fp: 841.0000 - val_tn: 303740.0000 - val_fn: 889.0000 - val_accuracy: 0.9957 - val_precision: 0.9917 - val_recall: 0.9912 - val_auc: 0.9985\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0028 - tp: 341798.0000 - fp: 289.0000 - tn: 1026302.0000 - fn: 399.0000 - accuracy: 0.7515 - precision: 0.9992 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0328 - val_tp: 100653.0000 - val_fp: 801.0000 - val_tn: 303780.0000 - val_fn: 874.0000 - val_accuracy: 0.9959 - val_precision: 0.9921 - val_recall: 0.9914 - val_auc: 0.9986\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0024 - tp: 341860.0000 - fp: 256.0000 - tn: 1026335.0000 - fn: 337.0000 - accuracy: 0.7516 - precision: 0.9993 - recall: 0.9990 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0024 - tp: 341860.0000 - fp: 256.0000 - tn: 1026335.0000 - fn: 337.0000 - accuracy: 0.7516 - precision: 0.9993 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0357 - val_tp: 100650.0000 - val_fp: 819.0000 - val_tn: 303762.0000 - val_fn: 877.0000 - val_accuracy: 0.9958 - val_precision: 0.9919 - val_recall: 0.9914 - val_auc: 0.9985\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_downweight_0.6.h5\n",
            "\n",
            "Down-weighting: 0.7\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 51s 393ms/step - loss: 0.1181 - tp: 415611.0000 - fp: 7516.0000 - tn: 1323656.0000 - fn: 28113.0000 - accuracy: 0.8025 - precision: 0.9822 - recall: 0.9366 - auc: 0.9973 - val_loss: 0.0449 - val_tp: 100516.0000 - val_fp: 997.0000 - val_tn: 303584.0000 - val_fn: 1011.0000 - val_accuracy: 0.9951 - val_precision: 0.9902 - val_recall: 0.9900 - val_auc: 0.9994\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0319 - tp: 339444.0000 - fp: 2659.0000 - tn: 1023932.0000 - fn: 2753.0000 - accuracy: 0.7481 - precision: 0.9922 - recall: 0.9920 - auc: 0.9996 - val_loss: 0.0337 - val_tp: 100585.0000 - val_fp: 938.0000 - val_tn: 303643.0000 - val_fn: 942.0000 - val_accuracy: 0.9954 - val_precision: 0.9908 - val_recall: 0.9907 - val_auc: 0.9998\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0213 - tp: 339401.0000 - fp: 2003.0000 - tn: 1024588.0000 - fn: 2796.0000 - accuracy: 0.7486 - precision: 0.9941 - recall: 0.9918 - auc: 0.9999 - val_loss: 0.0271 - val_tp: 100568.0000 - val_fp: 882.0000 - val_tn: 303699.0000 - val_fn: 959.0000 - val_accuracy: 0.9955 - val_precision: 0.9913 - val_recall: 0.9906 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 39s 373ms/step - loss: 0.0139 - tp: 339830.0000 - fp: 1028.0000 - tn: 1025563.0000 - fn: 2367.0000 - accuracy: 0.7497 - precision: 0.9970 - recall: 0.9931 - auc: 0.9999 - val_loss: 0.0251 - val_tp: 100582.0000 - val_fp: 771.0000 - val_tn: 303810.0000 - val_fn: 945.0000 - val_accuracy: 0.9958 - val_precision: 0.9924 - val_recall: 0.9907 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0100 - tp: 340510.0000 - fp: 768.0000 - tn: 1025823.0000 - fn: 1687.0000 - accuracy: 0.7503 - precision: 0.9977 - recall: 0.9951 - auc: 0.9999 - val_loss: 0.0262 - val_tp: 100586.0000 - val_fp: 776.0000 - val_tn: 303805.0000 - val_fn: 941.0000 - val_accuracy: 0.9958 - val_precision: 0.9923 - val_recall: 0.9907 - val_auc: 0.9993\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0077 - tp: 341040.0000 - fp: 626.0000 - tn: 1025965.0000 - fn: 1157.0000 - accuracy: 0.7508 - precision: 0.9982 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0279 - val_tp: 100604.0000 - val_fp: 798.0000 - val_tn: 303783.0000 - val_fn: 923.0000 - val_accuracy: 0.9958 - val_precision: 0.9921 - val_recall: 0.9909 - val_auc: 0.9992\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0062 - tp: 341310.0000 - fp: 531.0000 - tn: 1026060.0000 - fn: 887.0000 - accuracy: 0.7511 - precision: 0.9984 - recall: 0.9974 - auc: 0.9999 - val_loss: 0.0299 - val_tp: 100613.0000 - val_fp: 830.0000 - val_tn: 303751.0000 - val_fn: 914.0000 - val_accuracy: 0.9957 - val_precision: 0.9918 - val_recall: 0.9910 - val_auc: 0.9990\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0052 - tp: 341519.0000 - fp: 473.0000 - tn: 1026118.0000 - fn: 678.0000 - accuracy: 0.7512 - precision: 0.9986 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0324 - val_tp: 100631.0000 - val_fp: 836.0000 - val_tn: 303745.0000 - val_fn: 896.0000 - val_accuracy: 0.9957 - val_precision: 0.9918 - val_recall: 0.9912 - val_auc: 0.9987\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0045 - tp: 341599.0000 - fp: 442.0000 - tn: 1026149.0000 - fn: 598.0000 - accuracy: 0.7513 - precision: 0.9987 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0308 - val_tp: 100629.0000 - val_fp: 809.0000 - val_tn: 303772.0000 - val_fn: 898.0000 - val_accuracy: 0.9958 - val_precision: 0.9920 - val_recall: 0.9912 - val_auc: 0.9988\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0037 - tp: 341699.0000 - fp: 349.0000 - tn: 1026242.0000 - fn: 498.0000 - accuracy: 0.7514 - precision: 0.9990 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0359 - val_tp: 100632.0000 - val_fp: 851.0000 - val_tn: 303730.0000 - val_fn: 895.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9912 - val_auc: 0.9984\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0031 - tp: 341783.0000 - fp: 318.0000 - tn: 1026273.0000 - fn: 414.0000 - accuracy: 0.7515 - precision: 0.9991 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0335 - val_tp: 100629.0000 - val_fp: 831.0000 - val_tn: 303750.0000 - val_fn: 898.0000 - val_accuracy: 0.9957 - val_precision: 0.9918 - val_recall: 0.9912 - val_auc: 0.9986\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0026 - tp: 341857.0000 - fp: 262.0000 - tn: 1026329.0000 - fn: 340.0000 - accuracy: 0.7516 - precision: 0.9992 - recall: 0.9990 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0026 - tp: 341857.0000 - fp: 262.0000 - tn: 1026329.0000 - fn: 340.0000 - accuracy: 0.7516 - precision: 0.9992 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0362 - val_tp: 100642.0000 - val_fp: 828.0000 - val_tn: 303753.0000 - val_fn: 885.0000 - val_accuracy: 0.9958 - val_precision: 0.9918 - val_recall: 0.9913 - val_auc: 0.9984\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_downweight_0.7.h5\n",
            "\n",
            "Down-weighting: 0.8\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 51s 392ms/step - loss: 0.1317 - tp: 415673.0000 - fp: 7582.0000 - tn: 1323590.0000 - fn: 28051.0000 - accuracy: 0.8024 - precision: 0.9821 - recall: 0.9368 - auc: 0.9973 - val_loss: 0.0457 - val_tp: 100510.0000 - val_fp: 1002.0000 - val_tn: 303579.0000 - val_fn: 1017.0000 - val_accuracy: 0.9950 - val_precision: 0.9901 - val_recall: 0.9900 - val_auc: 0.9994\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0337 - tp: 339455.0000 - fp: 2670.0000 - tn: 1023921.0000 - fn: 2742.0000 - accuracy: 0.7480 - precision: 0.9922 - recall: 0.9920 - auc: 0.9996 - val_loss: 0.0346 - val_tp: 100585.0000 - val_fp: 937.0000 - val_tn: 303644.0000 - val_fn: 942.0000 - val_accuracy: 0.9954 - val_precision: 0.9908 - val_recall: 0.9907 - val_auc: 0.9997\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0228 - tp: 339430.0000 - fp: 2154.0000 - tn: 1024437.0000 - fn: 2767.0000 - accuracy: 0.7484 - precision: 0.9937 - recall: 0.9919 - auc: 0.9999 - val_loss: 0.0280 - val_tp: 100576.0000 - val_fp: 907.0000 - val_tn: 303674.0000 - val_fn: 951.0000 - val_accuracy: 0.9954 - val_precision: 0.9911 - val_recall: 0.9906 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0149 - tp: 339760.0000 - fp: 1112.0000 - tn: 1025479.0000 - fn: 2437.0000 - accuracy: 0.7495 - precision: 0.9967 - recall: 0.9929 - auc: 0.9999 - val_loss: 0.0258 - val_tp: 100585.0000 - val_fp: 800.0000 - val_tn: 303781.0000 - val_fn: 942.0000 - val_accuracy: 0.9957 - val_precision: 0.9921 - val_recall: 0.9907 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0107 - tp: 340413.0000 - fp: 799.0000 - tn: 1025792.0000 - fn: 1784.0000 - accuracy: 0.7502 - precision: 0.9977 - recall: 0.9948 - auc: 0.9999 - val_loss: 0.0270 - val_tp: 100599.0000 - val_fp: 795.0000 - val_tn: 303786.0000 - val_fn: 928.0000 - val_accuracy: 0.9958 - val_precision: 0.9922 - val_recall: 0.9909 - val_auc: 0.9993\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0083 - tp: 340941.0000 - fp: 631.0000 - tn: 1025960.0000 - fn: 1256.0000 - accuracy: 0.7507 - precision: 0.9982 - recall: 0.9963 - auc: 0.9999 - val_loss: 0.0285 - val_tp: 100609.0000 - val_fp: 804.0000 - val_tn: 303777.0000 - val_fn: 918.0000 - val_accuracy: 0.9958 - val_precision: 0.9921 - val_recall: 0.9910 - val_auc: 0.9991\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0067 - tp: 341226.0000 - fp: 543.0000 - tn: 1026048.0000 - fn: 971.0000 - accuracy: 0.7510 - precision: 0.9984 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0304 - val_tp: 100610.0000 - val_fp: 835.0000 - val_tn: 303746.0000 - val_fn: 917.0000 - val_accuracy: 0.9957 - val_precision: 0.9918 - val_recall: 0.9910 - val_auc: 0.9990\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0057 - tp: 341465.0000 - fp: 499.0000 - tn: 1026092.0000 - fn: 732.0000 - accuracy: 0.7512 - precision: 0.9985 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0335 - val_tp: 100623.0000 - val_fp: 848.0000 - val_tn: 303733.0000 - val_fn: 904.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9911 - val_auc: 0.9987\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0049 - tp: 341553.0000 - fp: 454.0000 - tn: 1026137.0000 - fn: 644.0000 - accuracy: 0.7513 - precision: 0.9987 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0315 - val_tp: 100624.0000 - val_fp: 823.0000 - val_tn: 303758.0000 - val_fn: 903.0000 - val_accuracy: 0.9957 - val_precision: 0.9919 - val_recall: 0.9911 - val_auc: 0.9988\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0040 - tp: 341680.0000 - fp: 350.0000 - tn: 1026241.0000 - fn: 517.0000 - accuracy: 0.7514 - precision: 0.9990 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0363 - val_tp: 100630.0000 - val_fp: 844.0000 - val_tn: 303737.0000 - val_fn: 897.0000 - val_accuracy: 0.9957 - val_precision: 0.9917 - val_recall: 0.9912 - val_auc: 0.9984\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0034 - tp: 341771.0000 - fp: 329.0000 - tn: 1026262.0000 - fn: 426.0000 - accuracy: 0.7515 - precision: 0.9990 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0353 - val_tp: 100624.0000 - val_fp: 849.0000 - val_tn: 303732.0000 - val_fn: 903.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9911 - val_auc: 0.9985\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0029 - tp: 341840.0000 - fp: 271.0000 - tn: 1026320.0000 - fn: 357.0000 - accuracy: 0.7515 - precision: 0.9992 - recall: 0.9990 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0029 - tp: 341840.0000 - fp: 271.0000 - tn: 1026320.0000 - fn: 357.0000 - accuracy: 0.7515 - precision: 0.9992 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0375 - val_tp: 100629.0000 - val_fp: 837.0000 - val_tn: 303744.0000 - val_fn: 898.0000 - val_accuracy: 0.9957 - val_precision: 0.9918 - val_recall: 0.9912 - val_auc: 0.9984\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_downweight_0.8.h5\n",
            "\n",
            "Down-weighting: 0.9\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 50s 393ms/step - loss: 0.1451 - tp: 415695.0000 - fp: 7630.0000 - tn: 1323542.0000 - fn: 28029.0000 - accuracy: 0.8024 - precision: 0.9820 - recall: 0.9368 - auc: 0.9972 - val_loss: 0.0465 - val_tp: 100504.0000 - val_fp: 1012.0000 - val_tn: 303569.0000 - val_fn: 1023.0000 - val_accuracy: 0.9950 - val_precision: 0.9900 - val_recall: 0.9899 - val_auc: 0.9994\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0353 - tp: 339458.0000 - fp: 2682.0000 - tn: 1023909.0000 - fn: 2739.0000 - accuracy: 0.7480 - precision: 0.9922 - recall: 0.9920 - auc: 0.9995 - val_loss: 0.0354 - val_tp: 100585.0000 - val_fp: 939.0000 - val_tn: 303642.0000 - val_fn: 942.0000 - val_accuracy: 0.9954 - val_precision: 0.9908 - val_recall: 0.9907 - val_auc: 0.9997\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0243 - tp: 339474.0000 - fp: 2281.0000 - tn: 1024310.0000 - fn: 2723.0000 - accuracy: 0.7483 - precision: 0.9933 - recall: 0.9920 - auc: 0.9998 - val_loss: 0.0289 - val_tp: 100580.0000 - val_fp: 923.0000 - val_tn: 303658.0000 - val_fn: 947.0000 - val_accuracy: 0.9954 - val_precision: 0.9909 - val_recall: 0.9907 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0159 - tp: 339686.0000 - fp: 1207.0000 - tn: 1025384.0000 - fn: 2511.0000 - accuracy: 0.7494 - precision: 0.9965 - recall: 0.9927 - auc: 0.9999 - val_loss: 0.0265 - val_tp: 100588.0000 - val_fp: 829.0000 - val_tn: 303752.0000 - val_fn: 939.0000 - val_accuracy: 0.9956 - val_precision: 0.9918 - val_recall: 0.9908 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0113 - tp: 340342.0000 - fp: 823.0000 - tn: 1025768.0000 - fn: 1855.0000 - accuracy: 0.7501 - precision: 0.9976 - recall: 0.9946 - auc: 0.9999 - val_loss: 0.0276 - val_tp: 100599.0000 - val_fp: 810.0000 - val_tn: 303771.0000 - val_fn: 928.0000 - val_accuracy: 0.9957 - val_precision: 0.9920 - val_recall: 0.9909 - val_auc: 0.9993\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0089 - tp: 340860.0000 - fp: 645.0000 - tn: 1025946.0000 - fn: 1337.0000 - accuracy: 0.7506 - precision: 0.9981 - recall: 0.9961 - auc: 0.9999 - val_loss: 0.0291 - val_tp: 100613.0000 - val_fp: 817.0000 - val_tn: 303764.0000 - val_fn: 914.0000 - val_accuracy: 0.9957 - val_precision: 0.9919 - val_recall: 0.9910 - val_auc: 0.9991\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0072 - tp: 341177.0000 - fp: 562.0000 - tn: 1026029.0000 - fn: 1020.0000 - accuracy: 0.7509 - precision: 0.9984 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0310 - val_tp: 100620.0000 - val_fp: 843.0000 - val_tn: 303738.0000 - val_fn: 907.0000 - val_accuracy: 0.9957 - val_precision: 0.9917 - val_recall: 0.9911 - val_auc: 0.9990\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0061 - tp: 341439.0000 - fp: 506.0000 - tn: 1026085.0000 - fn: 758.0000 - accuracy: 0.7511 - precision: 0.9985 - recall: 0.9978 - auc: 0.9999 - val_loss: 0.0342 - val_tp: 100625.0000 - val_fp: 850.0000 - val_tn: 303731.0000 - val_fn: 902.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9911 - val_auc: 0.9986\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0052 - tp: 341527.0000 - fp: 464.0000 - tn: 1026127.0000 - fn: 670.0000 - accuracy: 0.7512 - precision: 0.9986 - recall: 0.9980 - auc: 0.9999 - val_loss: 0.0321 - val_tp: 100614.0000 - val_fp: 825.0000 - val_tn: 303756.0000 - val_fn: 913.0000 - val_accuracy: 0.9957 - val_precision: 0.9919 - val_recall: 0.9910 - val_auc: 0.9988\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0044 - tp: 341661.0000 - fp: 363.0000 - tn: 1026228.0000 - fn: 536.0000 - accuracy: 0.7514 - precision: 0.9989 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0369 - val_tp: 100631.0000 - val_fp: 851.0000 - val_tn: 303730.0000 - val_fn: 896.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9912 - val_auc: 0.9984\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0037 - tp: 341742.0000 - fp: 341.0000 - tn: 1026250.0000 - fn: 455.0000 - accuracy: 0.7514 - precision: 0.9990 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0355 - val_tp: 100619.0000 - val_fp: 845.0000 - val_tn: 303736.0000 - val_fn: 908.0000 - val_accuracy: 0.9957 - val_precision: 0.9917 - val_recall: 0.9911 - val_auc: 0.9984\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0032 - tp: 341828.0000 - fp: 278.0000 - tn: 1026313.0000 - fn: 369.0000 - accuracy: 0.7515 - precision: 0.9992 - recall: 0.9989 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0032 - tp: 341828.0000 - fp: 278.0000 - tn: 1026313.0000 - fn: 369.0000 - accuracy: 0.7515 - precision: 0.9992 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0386 - val_tp: 100618.0000 - val_fp: 851.0000 - val_tn: 303730.0000 - val_fn: 909.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9910 - val_auc: 0.9983\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_downweight_0.9.h5\n",
            "\n",
            "Down-weighting: 1.0\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (3375, 105, 4)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 52s 395ms/step - loss: 0.1585 - tp: 415718.0000 - fp: 7688.0000 - tn: 1323484.0000 - fn: 28006.0000 - accuracy: 0.9799 - precision: 0.9818 - recall: 0.9369 - auc: 0.9971 - val_loss: 0.0473 - val_tp: 100489.0000 - val_fp: 1026.0000 - val_tn: 303555.0000 - val_fn: 1038.0000 - val_accuracy: 0.9949 - val_precision: 0.9899 - val_recall: 0.9898 - val_auc: 0.9993\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0368 - tp: 339458.0000 - fp: 2684.0000 - tn: 1023907.0000 - fn: 2739.0000 - accuracy: 0.9960 - precision: 0.9922 - recall: 0.9920 - auc: 0.9995 - val_loss: 0.0361 - val_tp: 100587.0000 - val_fp: 938.0000 - val_tn: 303643.0000 - val_fn: 940.0000 - val_accuracy: 0.9954 - val_precision: 0.9908 - val_recall: 0.9907 - val_auc: 0.9997\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0256 - tp: 339503.0000 - fp: 2370.0000 - tn: 1024221.0000 - fn: 2694.0000 - accuracy: 0.9963 - precision: 0.9931 - recall: 0.9921 - auc: 0.9998 - val_loss: 0.0298 - val_tp: 100585.0000 - val_fp: 926.0000 - val_tn: 303655.0000 - val_fn: 942.0000 - val_accuracy: 0.9954 - val_precision: 0.9909 - val_recall: 0.9907 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0169 - tp: 339629.0000 - fp: 1327.0000 - tn: 1025264.0000 - fn: 2568.0000 - accuracy: 0.9972 - precision: 0.9961 - recall: 0.9925 - auc: 0.9999 - val_loss: 0.0272 - val_tp: 100591.0000 - val_fp: 851.0000 - val_tn: 303730.0000 - val_fn: 936.0000 - val_accuracy: 0.9956 - val_precision: 0.9916 - val_recall: 0.9908 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 41s 382ms/step - loss: 0.0120 - tp: 340256.0000 - fp: 861.0000 - tn: 1025730.0000 - fn: 1941.0000 - accuracy: 0.9980 - precision: 0.9975 - recall: 0.9943 - auc: 0.9999 - val_loss: 0.0282 - val_tp: 100601.0000 - val_fp: 827.0000 - val_tn: 303754.0000 - val_fn: 926.0000 - val_accuracy: 0.9957 - val_precision: 0.9918 - val_recall: 0.9909 - val_auc: 0.9993\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 41s 383ms/step - loss: 0.0094 - tp: 340786.0000 - fp: 677.0000 - tn: 1025914.0000 - fn: 1411.0000 - accuracy: 0.9985 - precision: 0.9980 - recall: 0.9959 - auc: 0.9999 - val_loss: 0.0296 - val_tp: 100617.0000 - val_fp: 831.0000 - val_tn: 303750.0000 - val_fn: 910.0000 - val_accuracy: 0.9957 - val_precision: 0.9918 - val_recall: 0.9910 - val_auc: 0.9991\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0077 - tp: 341114.0000 - fp: 576.0000 - tn: 1026015.0000 - fn: 1083.0000 - accuracy: 0.9988 - precision: 0.9983 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0316 - val_tp: 100615.0000 - val_fp: 848.0000 - val_tn: 303733.0000 - val_fn: 912.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9910 - val_auc: 0.9989\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0065 - tp: 341376.0000 - fp: 527.0000 - tn: 1026064.0000 - fn: 821.0000 - accuracy: 0.9990 - precision: 0.9985 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0351 - val_tp: 100617.0000 - val_fp: 861.0000 - val_tn: 303720.0000 - val_fn: 910.0000 - val_accuracy: 0.9956 - val_precision: 0.9915 - val_recall: 0.9910 - val_auc: 0.9985\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0055 - tp: 341497.0000 - fp: 470.0000 - tn: 1026121.0000 - fn: 700.0000 - accuracy: 0.9991 - precision: 0.9986 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0326 - val_tp: 100615.0000 - val_fp: 826.0000 - val_tn: 303755.0000 - val_fn: 912.0000 - val_accuracy: 0.9957 - val_precision: 0.9919 - val_recall: 0.9910 - val_auc: 0.9987\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0046 - tp: 341640.0000 - fp: 384.0000 - tn: 1026207.0000 - fn: 557.0000 - accuracy: 0.9993 - precision: 0.9989 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0371 - val_tp: 100628.0000 - val_fp: 860.0000 - val_tn: 303721.0000 - val_fn: 899.0000 - val_accuracy: 0.9957 - val_precision: 0.9915 - val_recall: 0.9911 - val_auc: 0.9984\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0039 - tp: 341728.0000 - fp: 347.0000 - tn: 1026244.0000 - fn: 469.0000 - accuracy: 0.9994 - precision: 0.9990 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0361 - val_tp: 100614.0000 - val_fp: 852.0000 - val_tn: 303729.0000 - val_fn: 913.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9910 - val_auc: 0.9984\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0034 - tp: 341809.0000 - fp: 291.0000 - tn: 1026300.0000 - fn: 388.0000 - accuracy: 0.9995 - precision: 0.9991 - recall: 0.9989 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 41s 384ms/step - loss: 0.0034 - tp: 341809.0000 - fp: 291.0000 - tn: 1026300.0000 - fn: 388.0000 - accuracy: 0.9995 - precision: 0.9991 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0390 - val_tp: 100618.0000 - val_fp: 851.0000 - val_tn: 303730.0000 - val_fn: 909.0000 - val_accuracy: 0.9957 - val_precision: 0.9916 - val_recall: 0.9910 - val_auc: 0.9983\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_downweight_1.0.h5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0NImnAWzo_j",
        "outputId": "8b649aae-7e86-4bf3-9b1b-0e95dd349329"
      },
      "source": [
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n",
        "    preds = np.argmax(downweight_models[weight].predict([dev_seqs_padded, dev_pos_padded]), axis=-1)\n",
        "    flat_preds = [p for pred in preds for p in pred]\n",
        "    print(f'Weight = {weight}:', Counter(flat_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 0.1: Counter({3: 91649, 2: 11784, 0: 832})\n",
            "Weight = 0.2: Counter({3: 91646, 2: 12400, 0: 219})\n",
            "Weight = 0.3: Counter({3: 91645, 2: 12585, 0: 35})\n",
            "Weight = 0.4: Counter({3: 91643, 2: 12622})\n",
            "Weight = 0.5: Counter({3: 91633, 2: 12632})\n",
            "Weight = 0.6: Counter({3: 91631, 2: 12634})\n",
            "Weight = 0.7: Counter({3: 91631, 2: 12634})\n",
            "Weight = 0.8: Counter({3: 91630, 2: 12635})\n",
            "Weight = 0.9: Counter({3: 91629, 2: 12636})\n",
            "Weight = 1.0: Counter({3: 91627, 2: 12638})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXz8kDs1v6pY",
        "outputId": "c42f1f10-cb78-4c42-ec32-c468888a08dc"
      },
      "source": [
        "def reverse_bio(ind):\n",
        "    bio = 'O'  # for any pad=3 predictions\n",
        "    if ind==0:\n",
        "        bio = 'B'\n",
        "    elif ind==1:\n",
        "        bio = 'I'\n",
        "    elif ind==2:\n",
        "        bio = 'O'\n",
        "    return bio\n",
        "\n",
        "def wnut_evaluate(txt):\n",
        "    '''entity evaluation: we evaluate by whole named entities'''\n",
        "    npred = 0; ngold = 0; tp = 0\n",
        "    nrows = len(txt)\n",
        "    for i in txt.index:\n",
        "        if txt['prediction'][i]=='B' and txt['bio_only'][i]=='B':\n",
        "            npred += 1\n",
        "            ngold += 1\n",
        "            for predfindbo in range((i+1),nrows):\n",
        "                if txt['prediction'][predfindbo]=='O' or txt['prediction'][predfindbo]=='B':\n",
        "                    break  # find index of first O (end of entity) or B (new entity)\n",
        "            for goldfindbo in range((i+1),nrows):\n",
        "                if txt['bio_only'][goldfindbo]=='O' or txt['bio_only'][goldfindbo]=='B':\n",
        "                    break  # find index of first O (end of entity) or B (new entity)\n",
        "            if predfindbo==goldfindbo:  # only count a true positive if the whole entity phrase matches\n",
        "                tp += 1\n",
        "        elif txt['prediction'][i]=='B':\n",
        "            npred += 1\n",
        "        elif txt['bio_only'][i]=='B':\n",
        "            ngold += 1\n",
        "  \n",
        "    fp = npred - tp  # n false predictions\n",
        "    fn = ngold - tp  # n missing gold entities\n",
        "    prec = tp / (tp+fp+1e-7)\n",
        "    rec = tp / (tp+fn+1e-7)\n",
        "    f1 = (2*(prec*rec)) / (prec+rec+1e-7)\n",
        "    print('Sum of TP and FP = %i' % (tp+fp))\n",
        "    print('Sum of TP and FN = %i' % (tp+fn))\n",
        "    print('True positives = %i, False positives = %i, False negatives = %i' % (tp, fp, fn))\n",
        "    print('Precision = %.3f, Recall = %.3f, F1 = %.3f' % (prec, rec, f1))\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n",
        "    preds = np.argmax(downweight_models[weight].predict([dev_seqs_padded, dev_pos_padded]), axis=-1)\n",
        "\n",
        "    dev_seqs['prediction'] = ''\n",
        "    for i in dev_seqs.index:\n",
        "        this_seq_length = len(dev_seqs['token'][i])\n",
        "        dev_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    dev_long = dev_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = [reverse_bio(b) for b in dev_long['bio_only']]\n",
        "    dev_long['bio_only'] = bio_labs\n",
        "    pred_labs = [reverse_bio(b) for b in dev_long['prediction']]\n",
        "    dev_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(dev_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 0.1:\n",
            "Sum of TP and FP = 832\n",
            "Sum of TP and FN = 826\n",
            "True positives = 262, False positives = 570, False negatives = 564\n",
            "Precision = 0.315, Recall = 0.317, F1 = 0.316\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 219\n",
            "Sum of TP and FN = 826\n",
            "True positives = 49, False positives = 170, False negatives = 777\n",
            "Precision = 0.224, Recall = 0.059, F1 = 0.094\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 35\n",
            "Sum of TP and FN = 826\n",
            "True positives = 9, False positives = 26, False negatives = 817\n",
            "Precision = 0.257, Recall = 0.011, F1 = 0.021\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 0, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 0, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 0, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 0, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 0, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 0, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 1.0:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 0, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLyhy2siCg0f"
      },
      "source": [
        "#### Down-sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "RsOsYDpPCgEI",
        "outputId": "490d1714-d9d0-48cf-83c6-09e4fd0591c8"
      },
      "source": [
        "mask = train_seqs.bio_only.apply(lambda labels: 0.0 in labels)\n",
        "train_seqs_downsampled = train_seqs[mask]\n",
        "train_seqs_downsampled.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>upos_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[@paulwalk, It, 's, the, view, from, where, I,...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 0.0, 4.0, 5.0, 1.0, 6.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[From, Green, Newsfeed, :, AHFA, extends, dead...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....</td>\n",
              "      <td>[4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Pxleyes, Top, 50, Photography, Contest, Pictu...</td>\n",
              "      <td>[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....</td>\n",
              "      <td>[13.0, 11.0, 7.0, 0.0, 9.0, 0.0, 4.0, 9.0, 7.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[4Dbling, 's, place, til, monday, ,, party, pa...</td>\n",
              "      <td>[0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...</td>\n",
              "      <td>[0.0, 2.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>[watching, the, VMA, pre-show, again, lol, it,...</td>\n",
              "      <td>[2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[65.0, 3.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0...</td>\n",
              "      <td>[14.0, 3.0, 9.0, 5.0, 5.0, 13.0, 1.0, 2.0, 15....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                       upos_indices\n",
              "0             0  ...  [0.0, 1.0, 2.0, 3.0, 0.0, 4.0, 5.0, 1.0, 6.0, ...\n",
              "1             1  ...  [4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...\n",
              "2             2  ...  [13.0, 11.0, 7.0, 0.0, 9.0, 0.0, 4.0, 9.0, 7.0...\n",
              "4             4  ...  [0.0, 2.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, ...\n",
              "5             5  ...  [14.0, 3.0, 9.0, 5.0, 5.0, 13.0, 1.0, 2.0, 15....\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbpPsbN1Cn-1",
        "outputId": "5e2cb51c-9c92-4b83-9a6e-f4df3720de95"
      },
      "source": [
        "train_seqs_downsampled_padded = pad_sequences(train_seqs_downsampled['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                              dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "train_pos_downsampled_padded = pad_sequences(train_seqs_downsampled['upos_indices'].tolist(), maxlen=seq_length,\n",
        "                                             dtype='int32', padding='post', truncating='post', value=padpos)\n",
        "train_labs_downsampled_padded = pad_sequences(train_seqs_downsampled['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                              dtype='int32', padding='post', truncating='post', value=padlab)\n",
        "train_labs_downsampled_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_downsampled_padded]\n",
        "\n",
        "# follow the print outputs below to see how the labels are transformed\n",
        "print('Example of padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(train_seqs_downsampled.loc[1])\n",
        "print('Length of input sequence: %i' % len(train_seqs_downsampled_padded[1]))\n",
        "print('Length of pos sequence: %i' % len(train_pos_downsampled_padded[1]))\n",
        "print('Length of label sequence: %i' % len(train_labs_downsampled_onehot[1]))\n",
        "print(train_labs_downsampled_padded[1][:11])\n",
        "print(train_pos_downsampled_padded[1][:11])\n",
        "print(train_labs_downsampled_onehot[1][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     1\n",
            "token            [From, Green, Newsfeed, :, AHFA, extends, dead...\n",
            "bio_only         [2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, ...\n",
            "token_indices    [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
            "upos_indices     [4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...\n",
            "Name: 1, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of pos sequence: 105\n",
            "Length of label sequence: 105\n",
            "[2 2 2 2 0 2 2 2 2 2 2]\n",
            "[4 9 9 8 9 0 0 4 9 9 4]\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhlpNqw8DLnh",
        "outputId": "753703f6-c027-4170-cacf-5960cc4d0fc5"
      },
      "source": [
        "# figure out the label distribution in our downsampled fixed-length texts\n",
        "all_labs = [l for lab in train_labs_downsampled_padded for l in lab]\n",
        "label_count = Counter(all_labs)\n",
        "total_labs = len(all_labs)\n",
        "print(label_count)\n",
        "print(total_labs)\n",
        "\n",
        "# use this to define an initial model bias\n",
        "initial_bias=[(label_count[0]/total_labs), (label_count[1]/total_labs),\n",
        "              (label_count[2]/total_labs), (label_count[3]/total_labs)]\n",
        "print('Initial bias:')\n",
        "print(initial_bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({3: 103017, 2: 22152, 0: 1964, 1: 1177})\n",
            "128310\n",
            "Initial bias:\n",
            "[0.015306679136466371, 0.00917309640713896, 0.17264437689969606, 0.8028758475566986]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY6D4o80DQSJ",
        "outputId": "3e0a7365-ac29-4099-8a95-8f0098b013f9"
      },
      "source": [
        "# prepare sequences and labels as numpy arrays, check dimensions\n",
        "X_token = np.array(train_seqs_downsampled_padded)\n",
        "X_pos = np.array(train_pos_downsampled_padded)\n",
        "y = np.array(train_labs_downsampled_onehot)\n",
        "print('Input token sequence dimensions (n.docs, seq.length):')\n",
        "print(X_token.shape)\n",
        "print('Input pos sequence dimensions (n.docs, seq.length):')\n",
        "print(X_pos.shape)\n",
        "print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input token sequence dimensions (n.docs, seq.length):\n",
            "(1222, 105)\n",
            "Input pos sequence dimensions (n.docs, seq.length):\n",
            "(1222, 105)\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):\n",
            "(1222, 105, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u5f09JcDyqX",
        "outputId": "41d0e452-3917-4df1-f24f-43e89632475a"
      },
      "source": [
        "# now try the weighted one-hot encoding\n",
        "downweight_models = {}\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n",
        "    train_weights_onehot = down_weight(train_labs_downsampled_onehot, weight)\n",
        "    y = np.array(train_weights_onehot)\n",
        "    print('Down-weighting:', weight)\n",
        "    print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):', np.shape(y))\n",
        "\n",
        "    downweight_model = make_model(output_bias=initial_bias)\n",
        "    downweight_model.fit([X_token, X_pos], y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=([dev_X_token, dev_X_pos], dev_y))\n",
        "\n",
        "    downweight_models[weight] = downweight_model\n",
        "    if weight == 1.0:\n",
        "        downweight_model.save('BiLSTM_PoS_downsampled.h5')\n",
        "        print(f'Model saved at BiLSTM_PoS_downsampled.h5')\n",
        "    else:\n",
        "        downweight_model.save(f'BiLSTM_PoS_downsampled_downweight_{weight}.h5')\n",
        "        print(f'Model saved at BiLSTM_PoS_downsampled_downweight_{weight}.h5')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Down-weighting: 0.1\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 32s 476ms/step - loss: 0.0715 - tp: 266672.0000 - fp: 5790.0000 - tn: 980592.0000 - fn: 62122.0000 - accuracy: 0.8220 - precision: 0.9787 - recall: 0.8111 - auc: 0.9904 - val_loss: 0.1505 - val_tp: 87339.0000 - val_fp: 10.0000 - val_tn: 304571.0000 - val_fn: 14188.0000 - val_accuracy: 0.9650 - val_precision: 0.9999 - val_recall: 0.8603 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 417ms/step - loss: 0.0410 - tp: 105693.0000 - fp: 1081.0000 - tn: 369842.0000 - fn: 17948.0000 - accuracy: 0.7480 - precision: 0.9899 - recall: 0.8548 - auc: 0.9962 - val_loss: 0.1146 - val_tp: 90228.0000 - val_fp: 18.0000 - val_tn: 304563.0000 - val_fn: 11299.0000 - val_accuracy: 0.9721 - val_precision: 0.9998 - val_recall: 0.8887 - val_auc: 0.9988\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 412ms/step - loss: 0.0354 - tp: 110234.0000 - fp: 1498.0000 - tn: 369425.0000 - fn: 13407.0000 - accuracy: 0.7477 - precision: 0.9866 - recall: 0.8916 - auc: 0.9972 - val_loss: 0.0802 - val_tp: 97466.0000 - val_fp: 91.0000 - val_tn: 304490.0000 - val_fn: 4061.0000 - val_accuracy: 0.9898 - val_precision: 0.9991 - val_recall: 0.9600 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0285 - tp: 116263.0000 - fp: 1278.0000 - tn: 369645.0000 - fn: 7378.0000 - accuracy: 0.7490 - precision: 0.9891 - recall: 0.9403 - auc: 0.9987 - val_loss: 0.0510 - val_tp: 99215.0000 - val_fp: 282.0000 - val_tn: 304299.0000 - val_fn: 2312.0000 - val_accuracy: 0.9936 - val_precision: 0.9972 - val_recall: 0.9772 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0208 - tp: 119600.0000 - fp: 1295.0000 - tn: 369628.0000 - fn: 4041.0000 - accuracy: 0.7504 - precision: 0.9893 - recall: 0.9673 - auc: 0.9993 - val_loss: 0.0374 - val_tp: 99932.0000 - val_fp: 511.0000 - val_tn: 304070.0000 - val_fn: 1595.0000 - val_accuracy: 0.9948 - val_precision: 0.9949 - val_recall: 0.9843 - val_auc: 0.9998\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0142 - tp: 121336.0000 - fp: 1022.0000 - tn: 369901.0000 - fn: 2305.0000 - accuracy: 0.7521 - precision: 0.9916 - recall: 0.9814 - auc: 0.9996 - val_loss: 0.0287 - val_tp: 100310.0000 - val_fp: 571.0000 - val_tn: 304010.0000 - val_fn: 1217.0000 - val_accuracy: 0.9956 - val_precision: 0.9943 - val_recall: 0.9880 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 406ms/step - loss: 0.0094 - tp: 122267.0000 - fp: 820.0000 - tn: 370103.0000 - fn: 1374.0000 - accuracy: 0.7530 - precision: 0.9933 - recall: 0.9889 - auc: 0.9998 - val_loss: 0.0292 - val_tp: 100286.0000 - val_fp: 703.0000 - val_tn: 303878.0000 - val_fn: 1241.0000 - val_accuracy: 0.9952 - val_precision: 0.9930 - val_recall: 0.9878 - val_auc: 0.9998\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0066 - tp: 122610.0000 - fp: 706.0000 - tn: 370217.0000 - fn: 1031.0000 - accuracy: 0.7535 - precision: 0.9943 - recall: 0.9917 - auc: 0.9998 - val_loss: 0.0295 - val_tp: 100271.0000 - val_fp: 806.0000 - val_tn: 303775.0000 - val_fn: 1256.0000 - val_accuracy: 0.9949 - val_precision: 0.9920 - val_recall: 0.9876 - val_auc: 0.9997\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0047 - tp: 122836.0000 - fp: 641.0000 - tn: 370282.0000 - fn: 805.0000 - accuracy: 0.7537 - precision: 0.9948 - recall: 0.9935 - auc: 0.9999 - val_loss: 0.0281 - val_tp: 100360.0000 - val_fp: 807.0000 - val_tn: 303774.0000 - val_fn: 1167.0000 - val_accuracy: 0.9951 - val_precision: 0.9920 - val_recall: 0.9885 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 413ms/step - loss: 0.0038 - tp: 122964.0000 - fp: 575.0000 - tn: 370348.0000 - fn: 677.0000 - accuracy: 0.7540 - precision: 0.9953 - recall: 0.9945 - auc: 0.9999 - val_loss: 0.0273 - val_tp: 100441.0000 - val_fp: 781.0000 - val_tn: 303800.0000 - val_fn: 1086.0000 - val_accuracy: 0.9954 - val_precision: 0.9923 - val_recall: 0.9893 - val_auc: 0.9996\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0032 - tp: 123067.0000 - fp: 491.0000 - tn: 370432.0000 - fn: 574.0000 - accuracy: 0.7542 - precision: 0.9960 - recall: 0.9954 - auc: 0.9999 - val_loss: 0.0286 - val_tp: 100395.0000 - val_fp: 831.0000 - val_tn: 303750.0000 - val_fn: 1132.0000 - val_accuracy: 0.9952 - val_precision: 0.9918 - val_recall: 0.9889 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 408ms/step - loss: 0.0026 - tp: 123111.0000 - fp: 459.0000 - tn: 370464.0000 - fn: 530.0000 - accuracy: 0.7543 - precision: 0.9963 - recall: 0.9957 - auc: 0.9999 - val_loss: 0.0279 - val_tp: 100478.0000 - val_fp: 815.0000 - val_tn: 303766.0000 - val_fn: 1049.0000 - val_accuracy: 0.9954 - val_precision: 0.9920 - val_recall: 0.9897 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.0024 - tp: 123163.0000 - fp: 413.0000 - tn: 370510.0000 - fn: 478.0000 - accuracy: 0.7544 - precision: 0.9967 - recall: 0.9961 - auc: 0.9999 - val_loss: 0.0285 - val_tp: 100435.0000 - val_fp: 832.0000 - val_tn: 303749.0000 - val_fn: 1092.0000 - val_accuracy: 0.9953 - val_precision: 0.9918 - val_recall: 0.9892 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 440ms/step - loss: 0.0021 - tp: 123228.0000 - fp: 375.0000 - tn: 370548.0000 - fn: 413.0000 - accuracy: 0.7545 - precision: 0.9970 - recall: 0.9967 - auc: 0.9999 - val_loss: 0.0281 - val_tp: 100467.0000 - val_fp: 817.0000 - val_tn: 303764.0000 - val_fn: 1060.0000 - val_accuracy: 0.9954 - val_precision: 0.9919 - val_recall: 0.9896 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0019 - tp: 123263.0000 - fp: 346.0000 - tn: 370577.0000 - fn: 378.0000 - accuracy: 0.7545 - precision: 0.9972 - recall: 0.9969 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0019 - tp: 123263.0000 - fp: 346.0000 - tn: 370577.0000 - fn: 378.0000 - accuracy: 0.7545 - precision: 0.9972 - recall: 0.9969 - auc: 0.9999 - val_loss: 0.0291 - val_tp: 100458.0000 - val_fp: 842.0000 - val_tn: 303739.0000 - val_fn: 1069.0000 - val_accuracy: 0.9953 - val_precision: 0.9917 - val_recall: 0.9895 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_downsampled_downweight_0.1.h5\n",
            "\n",
            "Down-weighting: 0.2\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 28s 468ms/step - loss: 0.1077 - tp: 192775.0000 - fp: 3777.0000 - tn: 671727.0000 - fn: 32393.0000 - accuracy: 0.8573 - precision: 0.9808 - recall: 0.8561 - auc: 0.9940 - val_loss: 0.1194 - val_tp: 94109.0000 - val_fp: 432.0000 - val_tn: 304149.0000 - val_fn: 7418.0000 - val_accuracy: 0.9807 - val_precision: 0.9954 - val_recall: 0.9269 - val_auc: 0.9990\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 15s 398ms/step - loss: 0.0533 - tp: 116138.0000 - fp: 1806.0000 - tn: 369117.0000 - fn: 7503.0000 - accuracy: 0.7464 - precision: 0.9847 - recall: 0.9393 - auc: 0.9982 - val_loss: 0.0794 - val_tp: 99346.0000 - val_fp: 604.0000 - val_tn: 303977.0000 - val_fn: 2181.0000 - val_accuracy: 0.9931 - val_precision: 0.9940 - val_recall: 0.9785 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0445 - tp: 117657.0000 - fp: 1336.0000 - tn: 369587.0000 - fn: 5984.0000 - accuracy: 0.7474 - precision: 0.9888 - recall: 0.9516 - auc: 0.9989 - val_loss: 0.0556 - val_tp: 99933.0000 - val_fp: 529.0000 - val_tn: 304052.0000 - val_fn: 1594.0000 - val_accuracy: 0.9948 - val_precision: 0.9947 - val_recall: 0.9843 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0354 - tp: 119083.0000 - fp: 1017.0000 - tn: 369906.0000 - fn: 4558.0000 - accuracy: 0.7488 - precision: 0.9915 - recall: 0.9631 - auc: 0.9994 - val_loss: 0.0376 - val_tp: 100000.0000 - val_fp: 347.0000 - val_tn: 304234.0000 - val_fn: 1527.0000 - val_accuracy: 0.9954 - val_precision: 0.9965 - val_recall: 0.9850 - val_auc: 0.9999\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0261 - tp: 120547.0000 - fp: 948.0000 - tn: 369975.0000 - fn: 3094.0000 - accuracy: 0.7503 - precision: 0.9922 - recall: 0.9750 - auc: 0.9996 - val_loss: 0.0290 - val_tp: 100323.0000 - val_fp: 463.0000 - val_tn: 304118.0000 - val_fn: 1204.0000 - val_accuracy: 0.9959 - val_precision: 0.9954 - val_recall: 0.9881 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 408ms/step - loss: 0.0192 - tp: 121717.0000 - fp: 770.0000 - tn: 370153.0000 - fn: 1924.0000 - accuracy: 0.7518 - precision: 0.9937 - recall: 0.9844 - auc: 0.9998 - val_loss: 0.0250 - val_tp: 100439.0000 - val_fp: 486.0000 - val_tn: 304095.0000 - val_fn: 1088.0000 - val_accuracy: 0.9961 - val_precision: 0.9952 - val_recall: 0.9893 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.0140 - tp: 122424.0000 - fp: 648.0000 - tn: 370275.0000 - fn: 1217.0000 - accuracy: 0.7528 - precision: 0.9947 - recall: 0.9902 - auc: 0.9998 - val_loss: 0.0247 - val_tp: 100452.0000 - val_fp: 552.0000 - val_tn: 304029.0000 - val_fn: 1075.0000 - val_accuracy: 0.9960 - val_precision: 0.9945 - val_recall: 0.9894 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0105 - tp: 122773.0000 - fp: 566.0000 - tn: 370357.0000 - fn: 868.0000 - accuracy: 0.7535 - precision: 0.9954 - recall: 0.9930 - auc: 0.9999 - val_loss: 0.0245 - val_tp: 100417.0000 - val_fp: 586.0000 - val_tn: 303995.0000 - val_fn: 1110.0000 - val_accuracy: 0.9958 - val_precision: 0.9942 - val_recall: 0.9891 - val_auc: 0.9997\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 408ms/step - loss: 0.0077 - tp: 122951.0000 - fp: 479.0000 - tn: 370444.0000 - fn: 690.0000 - accuracy: 0.7538 - precision: 0.9961 - recall: 0.9944 - auc: 0.9999 - val_loss: 0.0248 - val_tp: 100454.0000 - val_fp: 649.0000 - val_tn: 303932.0000 - val_fn: 1073.0000 - val_accuracy: 0.9958 - val_precision: 0.9936 - val_recall: 0.9894 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 409ms/step - loss: 0.0063 - tp: 123069.0000 - fp: 438.0000 - tn: 370485.0000 - fn: 572.0000 - accuracy: 0.7541 - precision: 0.9965 - recall: 0.9954 - auc: 0.9999 - val_loss: 0.0243 - val_tp: 100503.0000 - val_fp: 668.0000 - val_tn: 303913.0000 - val_fn: 1024.0000 - val_accuracy: 0.9958 - val_precision: 0.9934 - val_recall: 0.9899 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0053 - tp: 123126.0000 - fp: 400.0000 - tn: 370523.0000 - fn: 515.0000 - accuracy: 0.7542 - precision: 0.9968 - recall: 0.9958 - auc: 0.9999 - val_loss: 0.0254 - val_tp: 100470.0000 - val_fp: 711.0000 - val_tn: 303870.0000 - val_fn: 1057.0000 - val_accuracy: 0.9956 - val_precision: 0.9930 - val_recall: 0.9896 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0044 - tp: 123200.0000 - fp: 365.0000 - tn: 370558.0000 - fn: 441.0000 - accuracy: 0.7543 - precision: 0.9970 - recall: 0.9964 - auc: 0.9999 - val_loss: 0.0254 - val_tp: 100510.0000 - val_fp: 716.0000 - val_tn: 303865.0000 - val_fn: 1017.0000 - val_accuracy: 0.9957 - val_precision: 0.9929 - val_recall: 0.9900 - val_auc: 0.9995\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 402ms/step - loss: 0.0038 - tp: 123217.0000 - fp: 337.0000 - tn: 370586.0000 - fn: 424.0000 - accuracy: 0.7544 - precision: 0.9973 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0256 - val_tp: 100527.0000 - val_fp: 733.0000 - val_tn: 303848.0000 - val_fn: 1000.0000 - val_accuracy: 0.9957 - val_precision: 0.9928 - val_recall: 0.9902 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 400ms/step - loss: 0.0033 - tp: 123284.0000 - fp: 304.0000 - tn: 370619.0000 - fn: 357.0000 - accuracy: 0.7545 - precision: 0.9975 - recall: 0.9971 - auc: 1.0000 - val_loss: 0.0265 - val_tp: 100521.0000 - val_fp: 747.0000 - val_tn: 303834.0000 - val_fn: 1006.0000 - val_accuracy: 0.9957 - val_precision: 0.9926 - val_recall: 0.9901 - val_auc: 0.9994\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0030 - tp: 123345.0000 - fp: 265.0000 - tn: 370658.0000 - fn: 296.0000 - accuracy: 0.7547 - precision: 0.9979 - recall: 0.9976 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0030 - tp: 123345.0000 - fp: 265.0000 - tn: 370658.0000 - fn: 296.0000 - accuracy: 0.7547 - precision: 0.9979 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0274 - val_tp: 100498.0000 - val_fp: 779.0000 - val_tn: 303802.0000 - val_fn: 1029.0000 - val_accuracy: 0.9955 - val_precision: 0.9923 - val_recall: 0.9899 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_downsampled_downweight_0.2.h5\n",
            "\n",
            "Down-weighting: 0.3\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 468ms/step - loss: 0.1424 - tp: 193937.0000 - fp: 4265.0000 - tn: 671239.0000 - fn: 31231.0000 - accuracy: 0.8568 - precision: 0.9785 - recall: 0.8613 - auc: 0.9942 - val_loss: 0.1002 - val_tp: 97629.0000 - val_fp: 1101.0000 - val_tn: 303480.0000 - val_fn: 3898.0000 - val_accuracy: 0.9877 - val_precision: 0.9888 - val_recall: 0.9616 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 409ms/step - loss: 0.0626 - tp: 118998.0000 - fp: 2432.0000 - tn: 368491.0000 - fn: 4643.0000 - accuracy: 0.7451 - precision: 0.9800 - recall: 0.9624 - auc: 0.9981 - val_loss: 0.0670 - val_tp: 100202.0000 - val_fp: 857.0000 - val_tn: 303724.0000 - val_fn: 1325.0000 - val_accuracy: 0.9946 - val_precision: 0.9915 - val_recall: 0.9869 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0520 - tp: 119780.0000 - fp: 2028.0000 - tn: 368895.0000 - fn: 3861.0000 - accuracy: 0.7459 - precision: 0.9834 - recall: 0.9688 - auc: 0.9989 - val_loss: 0.0506 - val_tp: 100360.0000 - val_fp: 791.0000 - val_tn: 303790.0000 - val_fn: 1167.0000 - val_accuracy: 0.9952 - val_precision: 0.9922 - val_recall: 0.9885 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0424 - tp: 119888.0000 - fp: 1284.0000 - tn: 369639.0000 - fn: 3753.0000 - accuracy: 0.7477 - precision: 0.9894 - recall: 0.9696 - auc: 0.9994 - val_loss: 0.0360 - val_tp: 100228.0000 - val_fp: 543.0000 - val_tn: 304038.0000 - val_fn: 1299.0000 - val_accuracy: 0.9955 - val_precision: 0.9946 - val_recall: 0.9872 - val_auc: 0.9999\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.0316 - tp: 120644.0000 - fp: 909.0000 - tn: 370014.0000 - fn: 2997.0000 - accuracy: 0.7496 - precision: 0.9925 - recall: 0.9758 - auc: 0.9997 - val_loss: 0.0275 - val_tp: 100306.0000 - val_fp: 408.0000 - val_tn: 304173.0000 - val_fn: 1221.0000 - val_accuracy: 0.9960 - val_precision: 0.9959 - val_recall: 0.9880 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0234 - tp: 121614.0000 - fp: 760.0000 - tn: 370163.0000 - fn: 2027.0000 - accuracy: 0.7512 - precision: 0.9938 - recall: 0.9836 - auc: 0.9998 - val_loss: 0.0241 - val_tp: 100406.0000 - val_fp: 446.0000 - val_tn: 304135.0000 - val_fn: 1121.0000 - val_accuracy: 0.9961 - val_precision: 0.9956 - val_recall: 0.9890 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 400ms/step - loss: 0.0177 - tp: 122332.0000 - fp: 634.0000 - tn: 370289.0000 - fn: 1309.0000 - accuracy: 0.7524 - precision: 0.9948 - recall: 0.9894 - auc: 0.9998 - val_loss: 0.0238 - val_tp: 100472.0000 - val_fp: 489.0000 - val_tn: 304092.0000 - val_fn: 1055.0000 - val_accuracy: 0.9962 - val_precision: 0.9952 - val_recall: 0.9896 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 404ms/step - loss: 0.0137 - tp: 122720.0000 - fp: 536.0000 - tn: 370387.0000 - fn: 921.0000 - accuracy: 0.7532 - precision: 0.9957 - recall: 0.9926 - auc: 0.9999 - val_loss: 0.0238 - val_tp: 100489.0000 - val_fp: 535.0000 - val_tn: 304046.0000 - val_fn: 1038.0000 - val_accuracy: 0.9961 - val_precision: 0.9947 - val_recall: 0.9898 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0104 - tp: 122933.0000 - fp: 455.0000 - tn: 370468.0000 - fn: 708.0000 - accuracy: 0.7537 - precision: 0.9963 - recall: 0.9943 - auc: 0.9999 - val_loss: 0.0235 - val_tp: 100523.0000 - val_fp: 574.0000 - val_tn: 304007.0000 - val_fn: 1004.0000 - val_accuracy: 0.9961 - val_precision: 0.9943 - val_recall: 0.9901 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 409ms/step - loss: 0.0083 - tp: 123095.0000 - fp: 375.0000 - tn: 370548.0000 - fn: 546.0000 - accuracy: 0.7541 - precision: 0.9970 - recall: 0.9956 - auc: 0.9999 - val_loss: 0.0235 - val_tp: 100555.0000 - val_fp: 609.0000 - val_tn: 303972.0000 - val_fn: 972.0000 - val_accuracy: 0.9961 - val_precision: 0.9940 - val_recall: 0.9904 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 404ms/step - loss: 0.0068 - tp: 123160.0000 - fp: 348.0000 - tn: 370575.0000 - fn: 481.0000 - accuracy: 0.7542 - precision: 0.9972 - recall: 0.9961 - auc: 0.9999 - val_loss: 0.0241 - val_tp: 100546.0000 - val_fp: 633.0000 - val_tn: 303948.0000 - val_fn: 981.0000 - val_accuracy: 0.9960 - val_precision: 0.9937 - val_recall: 0.9903 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0057 - tp: 123208.0000 - fp: 326.0000 - tn: 370597.0000 - fn: 433.0000 - accuracy: 0.7543 - precision: 0.9974 - recall: 0.9965 - auc: 0.9999 - val_loss: 0.0250 - val_tp: 100546.0000 - val_fp: 679.0000 - val_tn: 303902.0000 - val_fn: 981.0000 - val_accuracy: 0.9959 - val_precision: 0.9933 - val_recall: 0.9903 - val_auc: 0.9995\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0047 - tp: 123274.0000 - fp: 280.0000 - tn: 370643.0000 - fn: 367.0000 - accuracy: 0.7545 - precision: 0.9977 - recall: 0.9970 - auc: 1.0000 - val_loss: 0.0250 - val_tp: 100574.0000 - val_fp: 688.0000 - val_tn: 303893.0000 - val_fn: 953.0000 - val_accuracy: 0.9960 - val_precision: 0.9932 - val_recall: 0.9906 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0040 - tp: 123315.0000 - fp: 268.0000 - tn: 370655.0000 - fn: 326.0000 - accuracy: 0.7546 - precision: 0.9978 - recall: 0.9974 - auc: 1.0000 - val_loss: 0.0255 - val_tp: 100550.0000 - val_fp: 710.0000 - val_tn: 303871.0000 - val_fn: 977.0000 - val_accuracy: 0.9958 - val_precision: 0.9930 - val_recall: 0.9904 - val_auc: 0.9994\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0036 - tp: 123369.0000 - fp: 226.0000 - tn: 370697.0000 - fn: 272.0000 - accuracy: 0.7547 - precision: 0.9982 - recall: 0.9978 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 404ms/step - loss: 0.0036 - tp: 123369.0000 - fp: 226.0000 - tn: 370697.0000 - fn: 272.0000 - accuracy: 0.7547 - precision: 0.9982 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0267 - val_tp: 100542.0000 - val_fp: 759.0000 - val_tn: 303822.0000 - val_fn: 985.0000 - val_accuracy: 0.9957 - val_precision: 0.9925 - val_recall: 0.9903 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_downsampled_downweight_0.3.h5\n",
            "\n",
            "Down-weighting: 0.4\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 459ms/step - loss: 0.1764 - tp: 194772.0000 - fp: 4616.0000 - tn: 670888.0000 - fn: 30396.0000 - accuracy: 0.8565 - precision: 0.9768 - recall: 0.8650 - auc: 0.9942 - val_loss: 0.0888 - val_tp: 98957.0000 - val_fp: 1212.0000 - val_tn: 303369.0000 - val_fn: 2570.0000 - val_accuracy: 0.9907 - val_precision: 0.9879 - val_recall: 0.9747 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 400ms/step - loss: 0.0704 - tp: 119766.0000 - fp: 2601.0000 - tn: 368322.0000 - fn: 3875.0000 - accuracy: 0.7447 - precision: 0.9787 - recall: 0.9687 - auc: 0.9979 - val_loss: 0.0603 - val_tp: 100299.0000 - val_fp: 888.0000 - val_tn: 303693.0000 - val_fn: 1228.0000 - val_accuracy: 0.9948 - val_precision: 0.9912 - val_recall: 0.9879 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 404ms/step - loss: 0.0583 - tp: 120428.0000 - fp: 2405.0000 - tn: 368518.0000 - fn: 3213.0000 - accuracy: 0.7451 - precision: 0.9804 - recall: 0.9740 - auc: 0.9988 - val_loss: 0.0480 - val_tp: 100479.0000 - val_fp: 904.0000 - val_tn: 303677.0000 - val_fn: 1048.0000 - val_accuracy: 0.9952 - val_precision: 0.9911 - val_recall: 0.9897 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0486 - tp: 120399.0000 - fp: 1709.0000 - tn: 369214.0000 - fn: 3242.0000 - accuracy: 0.7466 - precision: 0.9860 - recall: 0.9738 - auc: 0.9993 - val_loss: 0.0357 - val_tp: 100402.0000 - val_fp: 767.0000 - val_tn: 303814.0000 - val_fn: 1125.0000 - val_accuracy: 0.9953 - val_precision: 0.9924 - val_recall: 0.9889 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 405ms/step - loss: 0.0367 - tp: 120678.0000 - fp: 959.0000 - tn: 369964.0000 - fn: 2963.0000 - accuracy: 0.7489 - precision: 0.9921 - recall: 0.9760 - auc: 0.9996 - val_loss: 0.0276 - val_tp: 100314.0000 - val_fp: 476.0000 - val_tn: 304105.0000 - val_fn: 1213.0000 - val_accuracy: 0.9958 - val_precision: 0.9953 - val_recall: 0.9881 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0272 - tp: 121494.0000 - fp: 761.0000 - tn: 370162.0000 - fn: 2147.0000 - accuracy: 0.7506 - precision: 0.9938 - recall: 0.9826 - auc: 0.9998 - val_loss: 0.0241 - val_tp: 100387.0000 - val_fp: 463.0000 - val_tn: 304118.0000 - val_fn: 1140.0000 - val_accuracy: 0.9961 - val_precision: 0.9954 - val_recall: 0.9888 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 405ms/step - loss: 0.0207 - tp: 122177.0000 - fp: 671.0000 - tn: 370252.0000 - fn: 1464.0000 - accuracy: 0.7518 - precision: 0.9945 - recall: 0.9882 - auc: 0.9998 - val_loss: 0.0234 - val_tp: 100471.0000 - val_fp: 477.0000 - val_tn: 304104.0000 - val_fn: 1056.0000 - val_accuracy: 0.9962 - val_precision: 0.9953 - val_recall: 0.9896 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0164 - tp: 122614.0000 - fp: 526.0000 - tn: 370397.0000 - fn: 1027.0000 - accuracy: 0.7528 - precision: 0.9957 - recall: 0.9917 - auc: 0.9999 - val_loss: 0.0234 - val_tp: 100488.0000 - val_fp: 519.0000 - val_tn: 304062.0000 - val_fn: 1039.0000 - val_accuracy: 0.9962 - val_precision: 0.9949 - val_recall: 0.9898 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0130 - tp: 122851.0000 - fp: 463.0000 - tn: 370460.0000 - fn: 790.0000 - accuracy: 0.7534 - precision: 0.9962 - recall: 0.9936 - auc: 0.9999 - val_loss: 0.0232 - val_tp: 100534.0000 - val_fp: 540.0000 - val_tn: 304041.0000 - val_fn: 993.0000 - val_accuracy: 0.9962 - val_precision: 0.9947 - val_recall: 0.9902 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 408ms/step - loss: 0.0105 - tp: 123036.0000 - fp: 385.0000 - tn: 370538.0000 - fn: 605.0000 - accuracy: 0.7538 - precision: 0.9969 - recall: 0.9951 - auc: 0.9999 - val_loss: 0.0239 - val_tp: 100537.0000 - val_fp: 599.0000 - val_tn: 303982.0000 - val_fn: 990.0000 - val_accuracy: 0.9961 - val_precision: 0.9941 - val_recall: 0.9902 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 400ms/step - loss: 0.0089 - tp: 123123.0000 - fp: 356.0000 - tn: 370567.0000 - fn: 518.0000 - accuracy: 0.7540 - precision: 0.9971 - recall: 0.9958 - auc: 0.9999 - val_loss: 0.0241 - val_tp: 100550.0000 - val_fp: 603.0000 - val_tn: 303978.0000 - val_fn: 977.0000 - val_accuracy: 0.9961 - val_precision: 0.9940 - val_recall: 0.9904 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0075 - tp: 123173.0000 - fp: 326.0000 - tn: 370597.0000 - fn: 468.0000 - accuracy: 0.7542 - precision: 0.9974 - recall: 0.9962 - auc: 0.9999 - val_loss: 0.0248 - val_tp: 100536.0000 - val_fp: 660.0000 - val_tn: 303921.0000 - val_fn: 991.0000 - val_accuracy: 0.9959 - val_precision: 0.9935 - val_recall: 0.9902 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 409ms/step - loss: 0.0063 - tp: 123243.0000 - fp: 292.0000 - tn: 370631.0000 - fn: 398.0000 - accuracy: 0.7544 - precision: 0.9976 - recall: 0.9968 - auc: 1.0000 - val_loss: 0.0251 - val_tp: 100583.0000 - val_fp: 701.0000 - val_tn: 303880.0000 - val_fn: 944.0000 - val_accuracy: 0.9959 - val_precision: 0.9931 - val_recall: 0.9907 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0057 - tp: 123273.0000 - fp: 283.0000 - tn: 370640.0000 - fn: 368.0000 - accuracy: 0.7544 - precision: 0.9977 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0260 - val_tp: 100547.0000 - val_fp: 730.0000 - val_tn: 303851.0000 - val_fn: 980.0000 - val_accuracy: 0.9958 - val_precision: 0.9928 - val_recall: 0.9903 - val_auc: 0.9994\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0049 - tp: 123337.0000 - fp: 245.0000 - tn: 370678.0000 - fn: 304.0000 - accuracy: 0.7546 - precision: 0.9980 - recall: 0.9975 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0049 - tp: 123337.0000 - fp: 245.0000 - tn: 370678.0000 - fn: 304.0000 - accuracy: 0.7546 - precision: 0.9980 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0273 - val_tp: 100517.0000 - val_fp: 776.0000 - val_tn: 303805.0000 - val_fn: 1010.0000 - val_accuracy: 0.9956 - val_precision: 0.9923 - val_recall: 0.9901 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_downsampled_downweight_0.4.h5\n",
            "\n",
            "Down-weighting: 0.5\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 26s 453ms/step - loss: 0.2100 - tp: 195284.0000 - fp: 4912.0000 - tn: 670592.0000 - fn: 29884.0000 - accuracy: 0.8561 - precision: 0.9755 - recall: 0.8673 - auc: 0.9941 - val_loss: 0.0831 - val_tp: 99541.0000 - val_fp: 1222.0000 - val_tn: 303359.0000 - val_fn: 1986.0000 - val_accuracy: 0.9921 - val_precision: 0.9879 - val_recall: 0.9804 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 399ms/step - loss: 0.0773 - tp: 120092.0000 - fp: 2673.0000 - tn: 368250.0000 - fn: 3549.0000 - accuracy: 0.7446 - precision: 0.9782 - recall: 0.9713 - auc: 0.9978 - val_loss: 0.0560 - val_tp: 100364.0000 - val_fp: 908.0000 - val_tn: 303673.0000 - val_fn: 1163.0000 - val_accuracy: 0.9949 - val_precision: 0.9910 - val_recall: 0.9885 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 408ms/step - loss: 0.0636 - tp: 120654.0000 - fp: 2554.0000 - tn: 368369.0000 - fn: 2987.0000 - accuracy: 0.7448 - precision: 0.9793 - recall: 0.9758 - auc: 0.9987 - val_loss: 0.0460 - val_tp: 100521.0000 - val_fp: 925.0000 - val_tn: 303656.0000 - val_fn: 1006.0000 - val_accuracy: 0.9952 - val_precision: 0.9909 - val_recall: 0.9901 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 404ms/step - loss: 0.0539 - tp: 120666.0000 - fp: 2100.0000 - tn: 368823.0000 - fn: 2975.0000 - accuracy: 0.7458 - precision: 0.9829 - recall: 0.9759 - auc: 0.9992 - val_loss: 0.0359 - val_tp: 100476.0000 - val_fp: 866.0000 - val_tn: 303715.0000 - val_fn: 1051.0000 - val_accuracy: 0.9953 - val_precision: 0.9915 - val_recall: 0.9896 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 402ms/step - loss: 0.0415 - tp: 120665.0000 - fp: 1141.0000 - tn: 369782.0000 - fn: 2976.0000 - accuracy: 0.7481 - precision: 0.9906 - recall: 0.9759 - auc: 0.9996 - val_loss: 0.0280 - val_tp: 100380.0000 - val_fp: 623.0000 - val_tn: 303958.0000 - val_fn: 1147.0000 - val_accuracy: 0.9956 - val_precision: 0.9938 - val_recall: 0.9887 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0308 - tp: 121352.0000 - fp: 814.0000 - tn: 370109.0000 - fn: 2289.0000 - accuracy: 0.7500 - precision: 0.9933 - recall: 0.9815 - auc: 0.9997 - val_loss: 0.0244 - val_tp: 100402.0000 - val_fp: 518.0000 - val_tn: 304063.0000 - val_fn: 1125.0000 - val_accuracy: 0.9960 - val_precision: 0.9949 - val_recall: 0.9889 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 405ms/step - loss: 0.0235 - tp: 122009.0000 - fp: 683.0000 - tn: 370240.0000 - fn: 1632.0000 - accuracy: 0.7514 - precision: 0.9944 - recall: 0.9868 - auc: 0.9998 - val_loss: 0.0235 - val_tp: 100449.0000 - val_fp: 488.0000 - val_tn: 304093.0000 - val_fn: 1078.0000 - val_accuracy: 0.9961 - val_precision: 0.9952 - val_recall: 0.9894 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 405ms/step - loss: 0.0188 - tp: 122496.0000 - fp: 569.0000 - tn: 370354.0000 - fn: 1145.0000 - accuracy: 0.7524 - precision: 0.9954 - recall: 0.9907 - auc: 0.9998 - val_loss: 0.0235 - val_tp: 100492.0000 - val_fp: 522.0000 - val_tn: 304059.0000 - val_fn: 1035.0000 - val_accuracy: 0.9962 - val_precision: 0.9948 - val_recall: 0.9898 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 404ms/step - loss: 0.0153 - tp: 122775.0000 - fp: 490.0000 - tn: 370433.0000 - fn: 866.0000 - accuracy: 0.7531 - precision: 0.9960 - recall: 0.9930 - auc: 0.9999 - val_loss: 0.0233 - val_tp: 100538.0000 - val_fp: 520.0000 - val_tn: 304061.0000 - val_fn: 989.0000 - val_accuracy: 0.9963 - val_precision: 0.9949 - val_recall: 0.9903 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0126 - tp: 122978.0000 - fp: 417.0000 - tn: 370506.0000 - fn: 663.0000 - accuracy: 0.7536 - precision: 0.9966 - recall: 0.9946 - auc: 0.9999 - val_loss: 0.0240 - val_tp: 100536.0000 - val_fp: 611.0000 - val_tn: 303970.0000 - val_fn: 991.0000 - val_accuracy: 0.9961 - val_precision: 0.9940 - val_recall: 0.9902 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0107 - tp: 123071.0000 - fp: 364.0000 - tn: 370559.0000 - fn: 570.0000 - accuracy: 0.7538 - precision: 0.9971 - recall: 0.9954 - auc: 0.9999 - val_loss: 0.0240 - val_tp: 100552.0000 - val_fp: 602.0000 - val_tn: 303979.0000 - val_fn: 975.0000 - val_accuracy: 0.9961 - val_precision: 0.9940 - val_recall: 0.9904 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0090 - tp: 123154.0000 - fp: 342.0000 - tn: 370581.0000 - fn: 487.0000 - accuracy: 0.7540 - precision: 0.9972 - recall: 0.9961 - auc: 0.9999 - val_loss: 0.0250 - val_tp: 100536.0000 - val_fp: 675.0000 - val_tn: 303906.0000 - val_fn: 991.0000 - val_accuracy: 0.9959 - val_precision: 0.9933 - val_recall: 0.9902 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 402ms/step - loss: 0.0076 - tp: 123220.0000 - fp: 291.0000 - tn: 370632.0000 - fn: 421.0000 - accuracy: 0.7543 - precision: 0.9976 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0254 - val_tp: 100597.0000 - val_fp: 711.0000 - val_tn: 303870.0000 - val_fn: 930.0000 - val_accuracy: 0.9960 - val_precision: 0.9930 - val_recall: 0.9908 - val_auc: 0.9993\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0069 - tp: 123266.0000 - fp: 282.0000 - tn: 370641.0000 - fn: 375.0000 - accuracy: 0.7543 - precision: 0.9977 - recall: 0.9970 - auc: 1.0000 - val_loss: 0.0261 - val_tp: 100563.0000 - val_fp: 741.0000 - val_tn: 303840.0000 - val_fn: 964.0000 - val_accuracy: 0.9958 - val_precision: 0.9927 - val_recall: 0.9905 - val_auc: 0.9994\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0059 - tp: 123340.0000 - fp: 250.0000 - tn: 370673.0000 - fn: 301.0000 - accuracy: 0.7545 - precision: 0.9980 - recall: 0.9976 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0059 - tp: 123340.0000 - fp: 250.0000 - tn: 370673.0000 - fn: 301.0000 - accuracy: 0.7545 - precision: 0.9980 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0277 - val_tp: 100524.0000 - val_fp: 799.0000 - val_tn: 303782.0000 - val_fn: 1003.0000 - val_accuracy: 0.9956 - val_precision: 0.9921 - val_recall: 0.9901 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_downsampled_downweight_0.5.h5\n",
            "\n",
            "Down-weighting: 0.6\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 28s 462ms/step - loss: 0.2433 - tp: 195650.0000 - fp: 5110.0000 - tn: 670394.0000 - fn: 29518.0000 - accuracy: 0.8559 - precision: 0.9745 - recall: 0.8689 - auc: 0.9940 - val_loss: 0.0801 - val_tp: 99805.0000 - val_fp: 1201.0000 - val_tn: 303380.0000 - val_fn: 1722.0000 - val_accuracy: 0.9928 - val_precision: 0.9881 - val_recall: 0.9830 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 403ms/step - loss: 0.0835 - tp: 120237.0000 - fp: 2719.0000 - tn: 368204.0000 - fn: 3404.0000 - accuracy: 0.7445 - precision: 0.9779 - recall: 0.9725 - auc: 0.9976 - val_loss: 0.0532 - val_tp: 100418.0000 - val_fp: 922.0000 - val_tn: 303659.0000 - val_fn: 1109.0000 - val_accuracy: 0.9950 - val_precision: 0.9909 - val_recall: 0.9891 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 404ms/step - loss: 0.0684 - tp: 120757.0000 - fp: 2591.0000 - tn: 368332.0000 - fn: 2884.0000 - accuracy: 0.7448 - precision: 0.9790 - recall: 0.9767 - auc: 0.9986 - val_loss: 0.0444 - val_tp: 100535.0000 - val_fp: 929.0000 - val_tn: 303652.0000 - val_fn: 992.0000 - val_accuracy: 0.9953 - val_precision: 0.9908 - val_recall: 0.9902 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 405ms/step - loss: 0.0586 - tp: 120796.0000 - fp: 2342.0000 - tn: 368581.0000 - fn: 2845.0000 - accuracy: 0.7453 - precision: 0.9810 - recall: 0.9770 - auc: 0.9992 - val_loss: 0.0361 - val_tp: 100515.0000 - val_fp: 914.0000 - val_tn: 303667.0000 - val_fn: 1012.0000 - val_accuracy: 0.9953 - val_precision: 0.9910 - val_recall: 0.9900 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 405ms/step - loss: 0.0459 - tp: 120732.0000 - fp: 1459.0000 - tn: 369464.0000 - fn: 2909.0000 - accuracy: 0.7472 - precision: 0.9881 - recall: 0.9765 - auc: 0.9995 - val_loss: 0.0287 - val_tp: 100456.0000 - val_fp: 763.0000 - val_tn: 303818.0000 - val_fn: 1071.0000 - val_accuracy: 0.9955 - val_precision: 0.9925 - val_recall: 0.9895 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 405ms/step - loss: 0.0343 - tp: 121229.0000 - fp: 881.0000 - tn: 370042.0000 - fn: 2412.0000 - accuracy: 0.7494 - precision: 0.9928 - recall: 0.9805 - auc: 0.9997 - val_loss: 0.0248 - val_tp: 100413.0000 - val_fp: 606.0000 - val_tn: 303975.0000 - val_fn: 1114.0000 - val_accuracy: 0.9958 - val_precision: 0.9940 - val_recall: 0.9890 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 404ms/step - loss: 0.0261 - tp: 121865.0000 - fp: 711.0000 - tn: 370212.0000 - fn: 1776.0000 - accuracy: 0.7510 - precision: 0.9942 - recall: 0.9856 - auc: 0.9998 - val_loss: 0.0237 - val_tp: 100440.0000 - val_fp: 518.0000 - val_tn: 304063.0000 - val_fn: 1087.0000 - val_accuracy: 0.9960 - val_precision: 0.9949 - val_recall: 0.9893 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 402ms/step - loss: 0.0209 - tp: 122371.0000 - fp: 581.0000 - tn: 370342.0000 - fn: 1270.0000 - accuracy: 0.7521 - precision: 0.9953 - recall: 0.9897 - auc: 0.9998 - val_loss: 0.0237 - val_tp: 100484.0000 - val_fp: 552.0000 - val_tn: 304029.0000 - val_fn: 1043.0000 - val_accuracy: 0.9961 - val_precision: 0.9945 - val_recall: 0.9897 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 406ms/step - loss: 0.0172 - tp: 122684.0000 - fp: 513.0000 - tn: 370410.0000 - fn: 957.0000 - accuracy: 0.7528 - precision: 0.9958 - recall: 0.9923 - auc: 0.9999 - val_loss: 0.0234 - val_tp: 100538.0000 - val_fp: 529.0000 - val_tn: 304052.0000 - val_fn: 989.0000 - val_accuracy: 0.9963 - val_precision: 0.9948 - val_recall: 0.9903 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 404ms/step - loss: 0.0143 - tp: 122915.0000 - fp: 445.0000 - tn: 370478.0000 - fn: 726.0000 - accuracy: 0.7533 - precision: 0.9964 - recall: 0.9941 - auc: 0.9999 - val_loss: 0.0241 - val_tp: 100537.0000 - val_fp: 636.0000 - val_tn: 303945.0000 - val_fn: 990.0000 - val_accuracy: 0.9960 - val_precision: 0.9937 - val_recall: 0.9902 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 406ms/step - loss: 0.0123 - tp: 123018.0000 - fp: 405.0000 - tn: 370518.0000 - fn: 623.0000 - accuracy: 0.7536 - precision: 0.9967 - recall: 0.9950 - auc: 0.9999 - val_loss: 0.0241 - val_tp: 100569.0000 - val_fp: 630.0000 - val_tn: 303951.0000 - val_fn: 958.0000 - val_accuracy: 0.9961 - val_precision: 0.9938 - val_recall: 0.9906 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 406ms/step - loss: 0.0104 - tp: 123121.0000 - fp: 352.0000 - tn: 370571.0000 - fn: 520.0000 - accuracy: 0.7539 - precision: 0.9971 - recall: 0.9958 - auc: 0.9999 - val_loss: 0.0251 - val_tp: 100546.0000 - val_fp: 699.0000 - val_tn: 303882.0000 - val_fn: 981.0000 - val_accuracy: 0.9959 - val_precision: 0.9931 - val_recall: 0.9903 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 409ms/step - loss: 0.0088 - tp: 123195.0000 - fp: 296.0000 - tn: 370627.0000 - fn: 446.0000 - accuracy: 0.7541 - precision: 0.9976 - recall: 0.9964 - auc: 0.9999 - val_loss: 0.0257 - val_tp: 100615.0000 - val_fp: 729.0000 - val_tn: 303852.0000 - val_fn: 912.0000 - val_accuracy: 0.9960 - val_precision: 0.9928 - val_recall: 0.9910 - val_auc: 0.9993\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 406ms/step - loss: 0.0080 - tp: 123240.0000 - fp: 296.0000 - tn: 370627.0000 - fn: 401.0000 - accuracy: 0.7542 - precision: 0.9976 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0262 - val_tp: 100562.0000 - val_fp: 754.0000 - val_tn: 303827.0000 - val_fn: 965.0000 - val_accuracy: 0.9958 - val_precision: 0.9926 - val_recall: 0.9905 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0069 - tp: 123324.0000 - fp: 259.0000 - tn: 370664.0000 - fn: 317.0000 - accuracy: 0.7544 - precision: 0.9979 - recall: 0.9974 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0069 - tp: 123324.0000 - fp: 259.0000 - tn: 370664.0000 - fn: 317.0000 - accuracy: 0.7544 - precision: 0.9979 - recall: 0.9974 - auc: 0.9999 - val_loss: 0.0279 - val_tp: 100534.0000 - val_fp: 797.0000 - val_tn: 303784.0000 - val_fn: 993.0000 - val_accuracy: 0.9956 - val_precision: 0.9921 - val_recall: 0.9902 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_downsampled_downweight_0.6.h5\n",
            "\n",
            "Down-weighting: 0.7\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 465ms/step - loss: 0.2763 - tp: 195888.0000 - fp: 5279.0000 - tn: 670225.0000 - fn: 29280.0000 - accuracy: 0.8558 - precision: 0.9738 - recall: 0.8700 - auc: 0.9940 - val_loss: 0.0786 - val_tp: 99906.0000 - val_fp: 1200.0000 - val_tn: 303381.0000 - val_fn: 1621.0000 - val_accuracy: 0.9931 - val_precision: 0.9881 - val_recall: 0.9840 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 409ms/step - loss: 0.0891 - tp: 120328.0000 - fp: 2743.0000 - tn: 368180.0000 - fn: 3313.0000 - accuracy: 0.7445 - precision: 0.9777 - recall: 0.9732 - auc: 0.9975 - val_loss: 0.0513 - val_tp: 100463.0000 - val_fp: 928.0000 - val_tn: 303653.0000 - val_fn: 1064.0000 - val_accuracy: 0.9951 - val_precision: 0.9908 - val_recall: 0.9895 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 409ms/step - loss: 0.0726 - tp: 120799.0000 - fp: 2618.0000 - tn: 368305.0000 - fn: 2842.0000 - accuracy: 0.7447 - precision: 0.9788 - recall: 0.9770 - auc: 0.9985 - val_loss: 0.0433 - val_tp: 100540.0000 - val_fp: 934.0000 - val_tn: 303647.0000 - val_fn: 987.0000 - val_accuracy: 0.9953 - val_precision: 0.9908 - val_recall: 0.9903 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0628 - tp: 120879.0000 - fp: 2473.0000 - tn: 368450.0000 - fn: 2762.0000 - accuracy: 0.7450 - precision: 0.9800 - recall: 0.9777 - auc: 0.9991 - val_loss: 0.0363 - val_tp: 100541.0000 - val_fp: 929.0000 - val_tn: 303652.0000 - val_fn: 986.0000 - val_accuracy: 0.9953 - val_precision: 0.9908 - val_recall: 0.9903 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 406ms/step - loss: 0.0500 - tp: 120799.0000 - fp: 1759.0000 - tn: 369164.0000 - fn: 2842.0000 - accuracy: 0.7465 - precision: 0.9856 - recall: 0.9770 - auc: 0.9995 - val_loss: 0.0294 - val_tp: 100504.0000 - val_fp: 850.0000 - val_tn: 303731.0000 - val_fn: 1023.0000 - val_accuracy: 0.9954 - val_precision: 0.9916 - val_recall: 0.9899 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 404ms/step - loss: 0.0377 - tp: 121113.0000 - fp: 1017.0000 - tn: 369906.0000 - fn: 2528.0000 - accuracy: 0.7487 - precision: 0.9917 - recall: 0.9796 - auc: 0.9997 - val_loss: 0.0253 - val_tp: 100447.0000 - val_fp: 698.0000 - val_tn: 303883.0000 - val_fn: 1080.0000 - val_accuracy: 0.9956 - val_precision: 0.9931 - val_recall: 0.9894 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0286 - tp: 121737.0000 - fp: 753.0000 - tn: 370170.0000 - fn: 1904.0000 - accuracy: 0.7505 - precision: 0.9939 - recall: 0.9846 - auc: 0.9998 - val_loss: 0.0239 - val_tp: 100448.0000 - val_fp: 559.0000 - val_tn: 304022.0000 - val_fn: 1079.0000 - val_accuracy: 0.9960 - val_precision: 0.9945 - val_recall: 0.9894 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 402ms/step - loss: 0.0229 - tp: 122235.0000 - fp: 613.0000 - tn: 370310.0000 - fn: 1406.0000 - accuracy: 0.7517 - precision: 0.9950 - recall: 0.9886 - auc: 0.9998 - val_loss: 0.0239 - val_tp: 100482.0000 - val_fp: 588.0000 - val_tn: 303993.0000 - val_fn: 1045.0000 - val_accuracy: 0.9960 - val_precision: 0.9942 - val_recall: 0.9897 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0191 - tp: 122584.0000 - fp: 528.0000 - tn: 370395.0000 - fn: 1057.0000 - accuracy: 0.7525 - precision: 0.9957 - recall: 0.9915 - auc: 0.9998 - val_loss: 0.0236 - val_tp: 100516.0000 - val_fp: 547.0000 - val_tn: 304034.0000 - val_fn: 1011.0000 - val_accuracy: 0.9962 - val_precision: 0.9946 - val_recall: 0.9900 - val_auc: 0.9995\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 401ms/step - loss: 0.0160 - tp: 122819.0000 - fp: 472.0000 - tn: 370451.0000 - fn: 822.0000 - accuracy: 0.7530 - precision: 0.9962 - recall: 0.9934 - auc: 0.9999 - val_loss: 0.0243 - val_tp: 100542.0000 - val_fp: 650.0000 - val_tn: 303931.0000 - val_fn: 985.0000 - val_accuracy: 0.9960 - val_precision: 0.9936 - val_recall: 0.9903 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 402ms/step - loss: 0.0139 - tp: 122970.0000 - fp: 436.0000 - tn: 370487.0000 - fn: 671.0000 - accuracy: 0.7534 - precision: 0.9965 - recall: 0.9946 - auc: 0.9999 - val_loss: 0.0242 - val_tp: 100553.0000 - val_fp: 640.0000 - val_tn: 303941.0000 - val_fn: 974.0000 - val_accuracy: 0.9960 - val_precision: 0.9937 - val_recall: 0.9904 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.0118 - tp: 123090.0000 - fp: 368.0000 - tn: 370555.0000 - fn: 551.0000 - accuracy: 0.7538 - precision: 0.9970 - recall: 0.9955 - auc: 0.9999 - val_loss: 0.0253 - val_tp: 100558.0000 - val_fp: 709.0000 - val_tn: 303872.0000 - val_fn: 969.0000 - val_accuracy: 0.9959 - val_precision: 0.9930 - val_recall: 0.9905 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 406ms/step - loss: 0.0101 - tp: 123168.0000 - fp: 310.0000 - tn: 370613.0000 - fn: 473.0000 - accuracy: 0.7540 - precision: 0.9975 - recall: 0.9962 - auc: 0.9999 - val_loss: 0.0261 - val_tp: 100605.0000 - val_fp: 741.0000 - val_tn: 303840.0000 - val_fn: 922.0000 - val_accuracy: 0.9959 - val_precision: 0.9927 - val_recall: 0.9909 - val_auc: 0.9993\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 404ms/step - loss: 0.0091 - tp: 123222.0000 - fp: 301.0000 - tn: 370622.0000 - fn: 419.0000 - accuracy: 0.7541 - precision: 0.9976 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0263 - val_tp: 100580.0000 - val_fp: 744.0000 - val_tn: 303837.0000 - val_fn: 947.0000 - val_accuracy: 0.9958 - val_precision: 0.9927 - val_recall: 0.9907 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0078 - tp: 123297.0000 - fp: 272.0000 - tn: 370651.0000 - fn: 344.0000 - accuracy: 0.7543 - precision: 0.9978 - recall: 0.9972 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.0078 - tp: 123297.0000 - fp: 272.0000 - tn: 370651.0000 - fn: 344.0000 - accuracy: 0.7543 - precision: 0.9978 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0280 - val_tp: 100553.0000 - val_fp: 794.0000 - val_tn: 303787.0000 - val_fn: 974.0000 - val_accuracy: 0.9956 - val_precision: 0.9922 - val_recall: 0.9904 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_downsampled_downweight_0.7.h5\n",
            "\n",
            "Down-weighting: 0.8\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 28s 492ms/step - loss: 0.3093 - tp: 196120.0000 - fp: 5382.0000 - tn: 670122.0000 - fn: 29048.0000 - accuracy: 0.8557 - precision: 0.9733 - recall: 0.8710 - auc: 0.9939 - val_loss: 0.0778 - val_tp: 99954.0000 - val_fp: 1185.0000 - val_tn: 303396.0000 - val_fn: 1573.0000 - val_accuracy: 0.9932 - val_precision: 0.9883 - val_recall: 0.9845 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 405ms/step - loss: 0.0944 - tp: 120381.0000 - fp: 2764.0000 - tn: 368159.0000 - fn: 3260.0000 - accuracy: 0.7444 - precision: 0.9776 - recall: 0.9736 - auc: 0.9974 - val_loss: 0.0500 - val_tp: 100496.0000 - val_fp: 933.0000 - val_tn: 303648.0000 - val_fn: 1031.0000 - val_accuracy: 0.9952 - val_precision: 0.9908 - val_recall: 0.9898 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 406ms/step - loss: 0.0765 - tp: 120826.0000 - fp: 2627.0000 - tn: 368296.0000 - fn: 2815.0000 - accuracy: 0.7447 - precision: 0.9787 - recall: 0.9772 - auc: 0.9985 - val_loss: 0.0425 - val_tp: 100547.0000 - val_fp: 937.0000 - val_tn: 303644.0000 - val_fn: 980.0000 - val_accuracy: 0.9953 - val_precision: 0.9908 - val_recall: 0.9903 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0666 - tp: 120920.0000 - fp: 2552.0000 - tn: 368371.0000 - fn: 2721.0000 - accuracy: 0.7448 - precision: 0.9793 - recall: 0.9780 - auc: 0.9990 - val_loss: 0.0364 - val_tp: 100554.0000 - val_fp: 935.0000 - val_tn: 303646.0000 - val_fn: 973.0000 - val_accuracy: 0.9953 - val_precision: 0.9908 - val_recall: 0.9904 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 405ms/step - loss: 0.0539 - tp: 120871.0000 - fp: 2007.0000 - tn: 368916.0000 - fn: 2770.0000 - accuracy: 0.7460 - precision: 0.9837 - recall: 0.9776 - auc: 0.9995 - val_loss: 0.0301 - val_tp: 100534.0000 - val_fp: 898.0000 - val_tn: 303683.0000 - val_fn: 993.0000 - val_accuracy: 0.9953 - val_precision: 0.9911 - val_recall: 0.9902 - val_auc: 0.9998\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.0411 - tp: 121012.0000 - fp: 1147.0000 - tn: 369776.0000 - fn: 2629.0000 - accuracy: 0.7481 - precision: 0.9906 - recall: 0.9787 - auc: 0.9996 - val_loss: 0.0259 - val_tp: 100489.0000 - val_fp: 769.0000 - val_tn: 303812.0000 - val_fn: 1038.0000 - val_accuracy: 0.9956 - val_precision: 0.9924 - val_recall: 0.9898 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0311 - tp: 121636.0000 - fp: 784.0000 - tn: 370139.0000 - fn: 2005.0000 - accuracy: 0.7502 - precision: 0.9936 - recall: 0.9838 - auc: 0.9997 - val_loss: 0.0243 - val_tp: 100470.0000 - val_fp: 633.0000 - val_tn: 303948.0000 - val_fn: 1057.0000 - val_accuracy: 0.9958 - val_precision: 0.9937 - val_recall: 0.9896 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 405ms/step - loss: 0.0247 - tp: 122116.0000 - fp: 649.0000 - tn: 370274.0000 - fn: 1525.0000 - accuracy: 0.7513 - precision: 0.9947 - recall: 0.9877 - auc: 0.9998 - val_loss: 0.0241 - val_tp: 100487.0000 - val_fp: 624.0000 - val_tn: 303957.0000 - val_fn: 1040.0000 - val_accuracy: 0.9959 - val_precision: 0.9938 - val_recall: 0.9898 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 402ms/step - loss: 0.0207 - tp: 122446.0000 - fp: 554.0000 - tn: 370369.0000 - fn: 1195.0000 - accuracy: 0.7521 - precision: 0.9955 - recall: 0.9903 - auc: 0.9998 - val_loss: 0.0239 - val_tp: 100514.0000 - val_fp: 595.0000 - val_tn: 303986.0000 - val_fn: 1013.0000 - val_accuracy: 0.9960 - val_precision: 0.9941 - val_recall: 0.9900 - val_auc: 0.9995\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 406ms/step - loss: 0.0175 - tp: 122728.0000 - fp: 489.0000 - tn: 370434.0000 - fn: 913.0000 - accuracy: 0.7528 - precision: 0.9960 - recall: 0.9926 - auc: 0.9998 - val_loss: 0.0246 - val_tp: 100535.0000 - val_fp: 669.0000 - val_tn: 303912.0000 - val_fn: 992.0000 - val_accuracy: 0.9959 - val_precision: 0.9934 - val_recall: 0.9902 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 410ms/step - loss: 0.0152 - tp: 122911.0000 - fp: 462.0000 - tn: 370461.0000 - fn: 730.0000 - accuracy: 0.7532 - precision: 0.9963 - recall: 0.9941 - auc: 0.9998 - val_loss: 0.0244 - val_tp: 100566.0000 - val_fp: 655.0000 - val_tn: 303926.0000 - val_fn: 961.0000 - val_accuracy: 0.9960 - val_precision: 0.9935 - val_recall: 0.9905 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0131 - tp: 123033.0000 - fp: 393.0000 - tn: 370530.0000 - fn: 608.0000 - accuracy: 0.7536 - precision: 0.9968 - recall: 0.9951 - auc: 0.9999 - val_loss: 0.0255 - val_tp: 100560.0000 - val_fp: 727.0000 - val_tn: 303854.0000 - val_fn: 967.0000 - val_accuracy: 0.9958 - val_precision: 0.9928 - val_recall: 0.9905 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 18s 463ms/step - loss: 0.0112 - tp: 123126.0000 - fp: 334.0000 - tn: 370589.0000 - fn: 515.0000 - accuracy: 0.7539 - precision: 0.9973 - recall: 0.9958 - auc: 0.9999 - val_loss: 0.0265 - val_tp: 100595.0000 - val_fp: 763.0000 - val_tn: 303818.0000 - val_fn: 932.0000 - val_accuracy: 0.9958 - val_precision: 0.9925 - val_recall: 0.9908 - val_auc: 0.9993\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 18s 451ms/step - loss: 0.0101 - tp: 123186.0000 - fp: 306.0000 - tn: 370617.0000 - fn: 455.0000 - accuracy: 0.7540 - precision: 0.9975 - recall: 0.9963 - auc: 0.9999 - val_loss: 0.0265 - val_tp: 100588.0000 - val_fp: 755.0000 - val_tn: 303826.0000 - val_fn: 939.0000 - val_accuracy: 0.9958 - val_precision: 0.9926 - val_recall: 0.9908 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0087 - tp: 123272.0000 - fp: 275.0000 - tn: 370648.0000 - fn: 369.0000 - accuracy: 0.7542 - precision: 0.9978 - recall: 0.9970 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0087 - tp: 123272.0000 - fp: 275.0000 - tn: 370648.0000 - fn: 369.0000 - accuracy: 0.7542 - precision: 0.9978 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0281 - val_tp: 100557.0000 - val_fp: 807.0000 - val_tn: 303774.0000 - val_fn: 970.0000 - val_accuracy: 0.9956 - val_precision: 0.9920 - val_recall: 0.9904 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_downsampled_downweight_0.8.h5\n",
            "\n",
            "Down-weighting: 0.9\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 29s 488ms/step - loss: 0.3421 - tp: 196241.0000 - fp: 5471.0000 - tn: 670033.0000 - fn: 28927.0000 - accuracy: 0.8556 - precision: 0.9729 - recall: 0.8715 - auc: 0.9939 - val_loss: 0.0774 - val_tp: 99992.0000 - val_fp: 1179.0000 - val_tn: 303402.0000 - val_fn: 1535.0000 - val_accuracy: 0.9933 - val_precision: 0.9883 - val_recall: 0.9849 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0993 - tp: 120433.0000 - fp: 2774.0000 - tn: 368149.0000 - fn: 3208.0000 - accuracy: 0.7444 - precision: 0.9775 - recall: 0.9741 - auc: 0.9973 - val_loss: 0.0492 - val_tp: 100516.0000 - val_fp: 939.0000 - val_tn: 303642.0000 - val_fn: 1011.0000 - val_accuracy: 0.9952 - val_precision: 0.9907 - val_recall: 0.9900 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0801 - tp: 120845.0000 - fp: 2630.0000 - tn: 368293.0000 - fn: 2796.0000 - accuracy: 0.7447 - precision: 0.9787 - recall: 0.9774 - auc: 0.9984 - val_loss: 0.0420 - val_tp: 100550.0000 - val_fp: 940.0000 - val_tn: 303641.0000 - val_fn: 977.0000 - val_accuracy: 0.9953 - val_precision: 0.9907 - val_recall: 0.9904 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0701 - tp: 120935.0000 - fp: 2596.0000 - tn: 368327.0000 - fn: 2706.0000 - accuracy: 0.7448 - precision: 0.9790 - recall: 0.9781 - auc: 0.9990 - val_loss: 0.0365 - val_tp: 100555.0000 - val_fp: 937.0000 - val_tn: 303644.0000 - val_fn: 972.0000 - val_accuracy: 0.9953 - val_precision: 0.9908 - val_recall: 0.9904 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 412ms/step - loss: 0.0576 - tp: 120919.0000 - fp: 2222.0000 - tn: 368701.0000 - fn: 2722.0000 - accuracy: 0.7455 - precision: 0.9820 - recall: 0.9780 - auc: 0.9994 - val_loss: 0.0308 - val_tp: 100545.0000 - val_fp: 922.0000 - val_tn: 303659.0000 - val_fn: 982.0000 - val_accuracy: 0.9953 - val_precision: 0.9909 - val_recall: 0.9903 - val_auc: 0.9998\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 409ms/step - loss: 0.0444 - tp: 120961.0000 - fp: 1328.0000 - tn: 369595.0000 - fn: 2680.0000 - accuracy: 0.7476 - precision: 0.9891 - recall: 0.9783 - auc: 0.9996 - val_loss: 0.0266 - val_tp: 100506.0000 - val_fp: 845.0000 - val_tn: 303736.0000 - val_fn: 1021.0000 - val_accuracy: 0.9954 - val_precision: 0.9917 - val_recall: 0.9899 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.0335 - tp: 121506.0000 - fp: 851.0000 - tn: 370072.0000 - fn: 2135.0000 - accuracy: 0.7497 - precision: 0.9930 - recall: 0.9827 - auc: 0.9997 - val_loss: 0.0246 - val_tp: 100491.0000 - val_fp: 685.0000 - val_tn: 303896.0000 - val_fn: 1036.0000 - val_accuracy: 0.9958 - val_precision: 0.9932 - val_recall: 0.9898 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 18s 462ms/step - loss: 0.0265 - tp: 122008.0000 - fp: 667.0000 - tn: 370256.0000 - fn: 1633.0000 - accuracy: 0.7510 - precision: 0.9946 - recall: 0.9868 - auc: 0.9998 - val_loss: 0.0244 - val_tp: 100513.0000 - val_fp: 672.0000 - val_tn: 303909.0000 - val_fn: 1014.0000 - val_accuracy: 0.9958 - val_precision: 0.9934 - val_recall: 0.9900 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 449ms/step - loss: 0.0223 - tp: 122328.0000 - fp: 575.0000 - tn: 370348.0000 - fn: 1313.0000 - accuracy: 0.7518 - precision: 0.9953 - recall: 0.9894 - auc: 0.9998 - val_loss: 0.0241 - val_tp: 100524.0000 - val_fp: 635.0000 - val_tn: 303946.0000 - val_fn: 1003.0000 - val_accuracy: 0.9960 - val_precision: 0.9937 - val_recall: 0.9901 - val_auc: 0.9995\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 443ms/step - loss: 0.0189 - tp: 122655.0000 - fp: 529.0000 - tn: 370394.0000 - fn: 986.0000 - accuracy: 0.7525 - precision: 0.9957 - recall: 0.9920 - auc: 0.9998 - val_loss: 0.0248 - val_tp: 100537.0000 - val_fp: 690.0000 - val_tn: 303891.0000 - val_fn: 990.0000 - val_accuracy: 0.9959 - val_precision: 0.9932 - val_recall: 0.9902 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 431ms/step - loss: 0.0165 - tp: 122848.0000 - fp: 475.0000 - tn: 370448.0000 - fn: 793.0000 - accuracy: 0.7530 - precision: 0.9961 - recall: 0.9936 - auc: 0.9998 - val_loss: 0.0247 - val_tp: 100564.0000 - val_fp: 667.0000 - val_tn: 303914.0000 - val_fn: 963.0000 - val_accuracy: 0.9960 - val_precision: 0.9934 - val_recall: 0.9905 - val_auc: 0.9994\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 409ms/step - loss: 0.0142 - tp: 122990.0000 - fp: 408.0000 - tn: 370515.0000 - fn: 651.0000 - accuracy: 0.7534 - precision: 0.9967 - recall: 0.9947 - auc: 0.9999 - val_loss: 0.0257 - val_tp: 100548.0000 - val_fp: 742.0000 - val_tn: 303839.0000 - val_fn: 979.0000 - val_accuracy: 0.9958 - val_precision: 0.9927 - val_recall: 0.9904 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 410ms/step - loss: 0.0123 - tp: 123096.0000 - fp: 360.0000 - tn: 370563.0000 - fn: 545.0000 - accuracy: 0.7537 - precision: 0.9971 - recall: 0.9956 - auc: 0.9999 - val_loss: 0.0268 - val_tp: 100597.0000 - val_fp: 772.0000 - val_tn: 303809.0000 - val_fn: 930.0000 - val_accuracy: 0.9958 - val_precision: 0.9924 - val_recall: 0.9908 - val_auc: 0.9992\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 412ms/step - loss: 0.0111 - tp: 123153.0000 - fp: 329.0000 - tn: 370594.0000 - fn: 488.0000 - accuracy: 0.7539 - precision: 0.9973 - recall: 0.9961 - auc: 0.9999 - val_loss: 0.0268 - val_tp: 100584.0000 - val_fp: 764.0000 - val_tn: 303817.0000 - val_fn: 943.0000 - val_accuracy: 0.9958 - val_precision: 0.9925 - val_recall: 0.9907 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0096 - tp: 123246.0000 - fp: 288.0000 - tn: 370635.0000 - fn: 395.0000 - accuracy: 0.7541 - precision: 0.9977 - recall: 0.9968 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 415ms/step - loss: 0.0096 - tp: 123246.0000 - fp: 288.0000 - tn: 370635.0000 - fn: 395.0000 - accuracy: 0.7541 - precision: 0.9977 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0284 - val_tp: 100566.0000 - val_fp: 816.0000 - val_tn: 303765.0000 - val_fn: 961.0000 - val_accuracy: 0.9956 - val_precision: 0.9920 - val_recall: 0.9905 - val_auc: 0.9992\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_downsampled_downweight_0.9.h5\n",
            "\n",
            "Down-weighting: 1.0\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels): (1222, 105, 4)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 466ms/step - loss: 0.3749 - tp: 196375.0000 - fp: 5549.0000 - tn: 669955.0000 - fn: 28793.0000 - accuracy: 0.9619 - precision: 0.9725 - recall: 0.8721 - auc: 0.9939 - val_loss: 0.0771 - val_tp: 100006.0000 - val_fp: 1185.0000 - val_tn: 303396.0000 - val_fn: 1521.0000 - val_accuracy: 0.9933 - val_precision: 0.9883 - val_recall: 0.9850 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 409ms/step - loss: 0.1039 - tp: 120462.0000 - fp: 2785.0000 - tn: 368138.0000 - fn: 3179.0000 - accuracy: 0.9879 - precision: 0.9774 - recall: 0.9743 - auc: 0.9973 - val_loss: 0.0487 - val_tp: 100534.0000 - val_fp: 939.0000 - val_tn: 303642.0000 - val_fn: 993.0000 - val_accuracy: 0.9952 - val_precision: 0.9907 - val_recall: 0.9902 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 412ms/step - loss: 0.0834 - tp: 120860.0000 - fp: 2635.0000 - tn: 368288.0000 - fn: 2781.0000 - accuracy: 0.9890 - precision: 0.9787 - recall: 0.9775 - auc: 0.9983 - val_loss: 0.0416 - val_tp: 100554.0000 - val_fp: 940.0000 - val_tn: 303641.0000 - val_fn: 973.0000 - val_accuracy: 0.9953 - val_precision: 0.9907 - val_recall: 0.9904 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 409ms/step - loss: 0.0733 - tp: 120943.0000 - fp: 2610.0000 - tn: 368313.0000 - fn: 2698.0000 - accuracy: 0.9893 - precision: 0.9789 - recall: 0.9782 - auc: 0.9989 - val_loss: 0.0366 - val_tp: 100559.0000 - val_fp: 941.0000 - val_tn: 303640.0000 - val_fn: 968.0000 - val_accuracy: 0.9953 - val_precision: 0.9907 - val_recall: 0.9905 - val_auc: 0.9997\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.0609 - tp: 120944.0000 - fp: 2346.0000 - tn: 368577.0000 - fn: 2697.0000 - accuracy: 0.9898 - precision: 0.9810 - recall: 0.9782 - auc: 0.9994 - val_loss: 0.0314 - val_tp: 100559.0000 - val_fp: 937.0000 - val_tn: 303644.0000 - val_fn: 968.0000 - val_accuracy: 0.9953 - val_precision: 0.9908 - val_recall: 0.9905 - val_auc: 0.9998\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 412ms/step - loss: 0.0477 - tp: 120944.0000 - fp: 1543.0000 - tn: 369380.0000 - fn: 2697.0000 - accuracy: 0.9914 - precision: 0.9874 - recall: 0.9782 - auc: 0.9996 - val_loss: 0.0272 - val_tp: 100527.0000 - val_fp: 871.0000 - val_tn: 303710.0000 - val_fn: 1000.0000 - val_accuracy: 0.9954 - val_precision: 0.9914 - val_recall: 0.9902 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.0360 - tp: 121360.0000 - fp: 900.0000 - tn: 370023.0000 - fn: 2281.0000 - accuracy: 0.9936 - precision: 0.9926 - recall: 0.9816 - auc: 0.9997 - val_loss: 0.0250 - val_tp: 100500.0000 - val_fp: 737.0000 - val_tn: 303844.0000 - val_fn: 1027.0000 - val_accuracy: 0.9957 - val_precision: 0.9927 - val_recall: 0.9899 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 407ms/step - loss: 0.0283 - tp: 121899.0000 - fp: 688.0000 - tn: 370235.0000 - fn: 1742.0000 - accuracy: 0.9951 - precision: 0.9944 - recall: 0.9859 - auc: 0.9998 - val_loss: 0.0247 - val_tp: 100525.0000 - val_fp: 715.0000 - val_tn: 303866.0000 - val_fn: 1002.0000 - val_accuracy: 0.9958 - val_precision: 0.9929 - val_recall: 0.9901 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0237 - tp: 122225.0000 - fp: 607.0000 - tn: 370316.0000 - fn: 1416.0000 - accuracy: 0.9959 - precision: 0.9951 - recall: 0.9885 - auc: 0.9998 - val_loss: 0.0244 - val_tp: 100542.0000 - val_fp: 675.0000 - val_tn: 303906.0000 - val_fn: 985.0000 - val_accuracy: 0.9959 - val_precision: 0.9933 - val_recall: 0.9903 - val_auc: 0.9995\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 417ms/step - loss: 0.0202 - tp: 122565.0000 - fp: 542.0000 - tn: 370381.0000 - fn: 1076.0000 - accuracy: 0.9967 - precision: 0.9956 - recall: 0.9913 - auc: 0.9998 - val_loss: 0.0251 - val_tp: 100542.0000 - val_fp: 711.0000 - val_tn: 303870.0000 - val_fn: 985.0000 - val_accuracy: 0.9958 - val_precision: 0.9930 - val_recall: 0.9903 - val_auc: 0.9994\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 414ms/step - loss: 0.0177 - tp: 122794.0000 - fp: 491.0000 - tn: 370432.0000 - fn: 847.0000 - accuracy: 0.9973 - precision: 0.9960 - recall: 0.9931 - auc: 0.9998 - val_loss: 0.0250 - val_tp: 100567.0000 - val_fp: 691.0000 - val_tn: 303890.0000 - val_fn: 960.0000 - val_accuracy: 0.9959 - val_precision: 0.9932 - val_recall: 0.9905 - val_auc: 0.9994\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 410ms/step - loss: 0.0153 - tp: 122949.0000 - fp: 425.0000 - tn: 370498.0000 - fn: 692.0000 - accuracy: 0.9977 - precision: 0.9966 - recall: 0.9944 - auc: 0.9999 - val_loss: 0.0259 - val_tp: 100554.0000 - val_fp: 748.0000 - val_tn: 303833.0000 - val_fn: 973.0000 - val_accuracy: 0.9958 - val_precision: 0.9926 - val_recall: 0.9904 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 410ms/step - loss: 0.0133 - tp: 123058.0000 - fp: 373.0000 - tn: 370550.0000 - fn: 583.0000 - accuracy: 0.9981 - precision: 0.9970 - recall: 0.9953 - auc: 0.9999 - val_loss: 0.0272 - val_tp: 100592.0000 - val_fp: 776.0000 - val_tn: 303805.0000 - val_fn: 935.0000 - val_accuracy: 0.9958 - val_precision: 0.9923 - val_recall: 0.9908 - val_auc: 0.9992\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 408ms/step - loss: 0.0120 - tp: 123127.0000 - fp: 334.0000 - tn: 370589.0000 - fn: 514.0000 - accuracy: 0.9983 - precision: 0.9973 - recall: 0.9958 - auc: 0.9999 - val_loss: 0.0271 - val_tp: 100577.0000 - val_fp: 784.0000 - val_tn: 303797.0000 - val_fn: 950.0000 - val_accuracy: 0.9957 - val_precision: 0.9923 - val_recall: 0.9906 - val_auc: 0.9992\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0104 - tp: 123220.0000 - fp: 299.0000 - tn: 370624.0000 - fn: 421.0000 - accuracy: 0.9985 - precision: 0.9976 - recall: 0.9966 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 411ms/step - loss: 0.0104 - tp: 123220.0000 - fp: 299.0000 - tn: 370624.0000 - fn: 421.0000 - accuracy: 0.9985 - precision: 0.9976 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0287 - val_tp: 100555.0000 - val_fp: 825.0000 - val_tn: 303756.0000 - val_fn: 972.0000 - val_accuracy: 0.9956 - val_precision: 0.9919 - val_recall: 0.9904 - val_auc: 0.9992\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_downsampled.h5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFIEtdEhIPBg",
        "outputId": "7e664bbc-8413-475d-9d99-a2d7c8d97bbe"
      },
      "source": [
        "for weight in [1.0, .9, .8, .7, .6, .5, .4, .3, .2, .1]:\n",
        "    preds = np.argmax(downweight_models[weight].predict([dev_seqs_padded, dev_pos_padded]), axis=-1)\n",
        "\n",
        "    dev_seqs['prediction'] = ''\n",
        "    for i in dev_seqs.index:\n",
        "        this_seq_length = len(dev_seqs['token'][i])\n",
        "        dev_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    dev_long = dev_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = [reverse_bio(b) for b in dev_long['bio_only']]\n",
        "    dev_long['bio_only'] = bio_labs\n",
        "    pred_labs = [reverse_bio(b) for b in dev_long['prediction']]\n",
        "    dev_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(dev_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 1.0:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 0, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 0, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 3\n",
            "Sum of TP and FN = 826\n",
            "True positives = 0, False positives = 3, False negatives = 826\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 26\n",
            "Sum of TP and FN = 826\n",
            "True positives = 5, False positives = 21, False negatives = 821\n",
            "Precision = 0.192, Recall = 0.006, F1 = 0.012\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 73\n",
            "Sum of TP and FN = 826\n",
            "True positives = 9, False positives = 64, False negatives = 817\n",
            "Precision = 0.123, Recall = 0.011, F1 = 0.020\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 156\n",
            "Sum of TP and FN = 826\n",
            "True positives = 25, False positives = 131, False negatives = 801\n",
            "Precision = 0.160, Recall = 0.030, F1 = 0.051\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 385\n",
            "Sum of TP and FN = 826\n",
            "True positives = 92, False positives = 293, False negatives = 734\n",
            "Precision = 0.239, Recall = 0.111, F1 = 0.152\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 684\n",
            "Sum of TP and FN = 826\n",
            "True positives = 229, False positives = 455, False negatives = 597\n",
            "Precision = 0.335, Recall = 0.277, F1 = 0.303\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 872\n",
            "Sum of TP and FN = 826\n",
            "True positives = 289, False positives = 583, False negatives = 537\n",
            "Precision = 0.331, Recall = 0.350, F1 = 0.340\n",
            "\n",
            "Weight = 0.1:\n",
            "Sum of TP and FP = 1149\n",
            "Sum of TP and FN = 826\n",
            "True positives = 362, False positives = 787, False negatives = 464\n",
            "Precision = 0.315, Recall = 0.438, F1 = 0.367\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397D4kdVVPQl"
      },
      "source": [
        "#### Merging B and I labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "TMG6GBI1Vcyt",
        "outputId": "df68df5f-0f81-40af-f5cb-68433c929968"
      },
      "source": [
        "# training labels: convert BIO to integers, this time merging B and I\n",
        "def bo_index(bio):\n",
        "    ind = bio\n",
        "    if not pd.isnull(bio):  # deal with empty lines\n",
        "        if bio == 'B' or bio =='I':\n",
        "            ind = 0\n",
        "        elif bio == 'O':\n",
        "            ind = 1\n",
        "    return ind\n",
        "\n",
        "# pass a data frame through the new feature extractor\n",
        "def extract_features_pos_bo(txt, istest=False):\n",
        "    txt_copy = txt.copy()\n",
        "    tokinds = [token_index(u) for u in txt_copy['token']]\n",
        "    txt_copy['token_indices'] = tokinds\n",
        "    posinds = [pos_index(p) for p in txt_copy['upos']]\n",
        "    txt_copy['upos_indices'] = posinds\n",
        "    if not istest:  # can't do this with the test set\n",
        "        boints = [bo_index(b) for b in txt_copy['bio_only']]\n",
        "        txt_copy['bio_only'] = boints\n",
        "    return txt_copy\n",
        "\n",
        "train_copy_bo = extract_features_pos_bo(train)\n",
        "train_copy_bo.head(n=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>upos_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@paulwalk</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PRON</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'s</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>AUX</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>DET</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>view</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>where</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PRON</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>'m</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>X</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>living</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>for</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>two</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NUM</td>\n",
              "      <td>11.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>weeks</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Empire</td>\n",
              "      <td>B-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>14.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>State</td>\n",
              "      <td>I-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Building</td>\n",
              "      <td>I-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>=</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>SYM</td>\n",
              "      <td>17.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ESB</td>\n",
              "      <td>B-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>18.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Pretty</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>19.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>bad</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>20.0</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>storm</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>here</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>22.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>last</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>23.0</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>evening</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>From</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>26.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Green</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>27.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token       label  bio_only   upos  token_indices  upos_indices\n",
              "0   @paulwalk           O       1.0   NOUN            0.0           0.0\n",
              "1          It           O       1.0   PRON            1.0           1.0\n",
              "2          's           O       1.0    AUX            2.0           2.0\n",
              "3         the           O       1.0    DET            3.0           3.0\n",
              "4        view           O       1.0   NOUN            4.0           0.0\n",
              "5        from           O       1.0    ADP            5.0           4.0\n",
              "6       where           O       1.0    ADV            6.0           5.0\n",
              "7           I           O       1.0   PRON            7.0           1.0\n",
              "8          'm           O       1.0      X            8.0           6.0\n",
              "9      living           O       1.0   NOUN            9.0           0.0\n",
              "10        for           O       1.0    ADP           10.0           4.0\n",
              "11        two           O       1.0    NUM           11.0           7.0\n",
              "12      weeks           O       1.0   NOUN           12.0           0.0\n",
              "13          .           O       1.0  PUNCT           13.0           8.0\n",
              "14     Empire  B-location       0.0  PROPN           14.0           9.0\n",
              "15      State  I-location       0.0  PROPN           15.0           9.0\n",
              "16   Building  I-location       0.0  PROPN           16.0           9.0\n",
              "17          =           O       1.0    SYM           17.0          10.0\n",
              "18        ESB  B-location       0.0  PROPN           18.0           9.0\n",
              "19          .           O       1.0  PUNCT           13.0           8.0\n",
              "20     Pretty           O       1.0    ADV           19.0           5.0\n",
              "21        bad           O       1.0    ADJ           20.0          11.0\n",
              "22      storm           O       1.0   NOUN           21.0           0.0\n",
              "23       here           O       1.0    ADV           22.0           5.0\n",
              "24       last           O       1.0    ADJ           23.0          11.0\n",
              "25    evening           O       1.0   NOUN           24.0           0.0\n",
              "26          .           O       1.0  PUNCT           13.0           8.0\n",
              "27        NaN         NaN       NaN    NaN            NaN           NaN\n",
              "28       From           O       1.0    ADP           26.0           4.0\n",
              "29      Green           O       1.0  PROPN           27.0           9.0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "B0xjuLCOV0s8",
        "outputId": "722bc8f0-7b3a-4981-8c95-b2f5721331f9"
      },
      "source": [
        "print(\"This cell takes a little while to run: be patient :)\")\n",
        "train_seqs_bo = tokens2sequences_pos(train_copy_bo)\n",
        "train_seqs_bo.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This cell takes a little while to run: be patient :)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>upos_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[@paulwalk, It, 's, the, view, from, where, I,...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 0.0, 4.0, 5.0, 1.0, 6.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[From, Green, Newsfeed, :, AHFA, extends, dead...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....</td>\n",
              "      <td>[4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Pxleyes, Top, 50, Photography, Contest, Pictu...</td>\n",
              "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....</td>\n",
              "      <td>[13.0, 11.0, 7.0, 0.0, 9.0, 0.0, 4.0, 9.0, 7.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[today, is, my, last, day, at, the, office, .]</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
              "      <td>[51.0, 52.0, 53.0, 23.0, 54.0, 55.0, 3.0, 56.0...</td>\n",
              "      <td>[0.0, 2.0, 1.0, 11.0, 0.0, 4.0, 3.0, 0.0, 8.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[4Dbling, 's, place, til, monday, ,, party, pa...</td>\n",
              "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...</td>\n",
              "      <td>[0.0, 2.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                       upos_indices\n",
              "0             0  ...  [0.0, 1.0, 2.0, 3.0, 0.0, 4.0, 5.0, 1.0, 6.0, ...\n",
              "1             1  ...  [4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...\n",
              "2             2  ...  [13.0, 11.0, 7.0, 0.0, 9.0, 0.0, 4.0, 9.0, 7.0...\n",
              "3             3  ...     [0.0, 2.0, 1.0, 11.0, 0.0, 4.0, 3.0, 0.0, 8.0]\n",
              "4             4  ...  [0.0, 2.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "q7PU0sx-WBLu",
        "outputId": "72d97806-99b5-43a6-8cdd-99431d0230cc"
      },
      "source": [
        "# process the dev set\n",
        "wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'\n",
        "dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "dev_copy_bo = extract_features_pos_bo(dev)\n",
        "dev_seqs_bo = tokens2sequences_pos(dev_copy_bo)\n",
        "dev_seqs_bo.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>upos_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[Stabilized, approach, or, not, ?, That, , s,...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[14801.0, 10361.0, 414.0, 556.0, 131.0, 1740.0...</td>\n",
              "      <td>[9.0, 0.0, 16.0, 15.0, 8.0, 1.0, 10.0, 15.0, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[You, should, ', ve, stayed, on, Redondo, Beac...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[151.0, 1018.0, 573.0, 12927.0, 9346.0, 137.0,...</td>\n",
              "      <td>[1.0, 2.0, 8.0, 0.0, 14.0, 4.0, 9.0, 9.0, 9.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[All, I, ', ve, been, doing, is, BINGE, watchi...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[405.0, 7.0, 573.0, 12927.0, 90.0, 848.0, 52.0...</td>\n",
              "      <td>[3.0, 1.0, 8.0, 0.0, 2.0, 14.0, 2.0, 9.0, 14.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[wow, emma, and, kaite, is, so, very, cute, an...</td>\n",
              "      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[4777.0, 14801.0, 113.0, 14801.0, 52.0, 79.0, ...</td>\n",
              "      <td>[13.0, 0.0, 16.0, 0.0, 2.0, 5.0, 5.0, 11.0, 16...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[THIS, IS, SO, GOOD]</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0]</td>\n",
              "      <td>[2239.0, 1567.0, 1089.0, 9176.0]</td>\n",
              "      <td>[3.0, 2.0, 5.0, 11.0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                       upos_indices\n",
              "0             0  ...  [9.0, 0.0, 16.0, 15.0, 8.0, 1.0, 10.0, 15.0, 1...\n",
              "1             1  ...  [1.0, 2.0, 8.0, 0.0, 14.0, 4.0, 9.0, 9.0, 9.0,...\n",
              "2             2  ...  [3.0, 1.0, 8.0, 0.0, 2.0, 14.0, 2.0, 9.0, 14.0...\n",
              "3             3  ...  [13.0, 0.0, 16.0, 0.0, 2.0, 5.0, 5.0, 11.0, 16...\n",
              "4             4  ...                              [3.0, 2.0, 5.0, 11.0]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js7KdHq8WIDQ",
        "outputId": "5dd3d7b7-ecaa-49b1-d3a3-59b9ddf2782a"
      },
      "source": [
        "# process the 2-class training set: padding the tokens, pos and labels, and one-hot encoding the labels\n",
        "train_seqs_bo_padded = pad_sequences(train_seqs_bo['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                     dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "train_pos_bo_padded = pad_sequences(train_seqs_bo['upos_indices'].tolist(), maxlen=seq_length,\n",
        "                                    dtype='int32', padding='post', truncating='post', value=padpos)\n",
        "padlab_bo = 2\n",
        "train_labs_bo_padded = pad_sequences(train_seqs_bo['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                     dtype='int32', padding='post', truncating='post', value=padlab_bo)\n",
        "n_labs_bo = 3\n",
        "train_labs_bo_onehot = [to_categorical(i, num_classes=n_labs_bo) for i in train_labs_bo_padded]\n",
        "\n",
        "print('Example of padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(train_seqs_bo.loc[1])\n",
        "print('Length of input sequence: %i' % len(train_seqs_bo_padded[1]))\n",
        "print('Length of pos sequence: %i' % len(train_pos_bo_padded[1]))\n",
        "print('Length of label sequence: %i' % len(train_labs_bo_onehot[1]))\n",
        "print(train_pos_bo_padded[1][:11])\n",
        "print(train_labs_bo_padded[1][:11])\n",
        "print(train_labs_bo_onehot[1][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     1\n",
            "token            [From, Green, Newsfeed, :, AHFA, extends, dead...\n",
            "bio_only         [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...\n",
            "token_indices    [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
            "upos_indices     [4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...\n",
            "Name: 1, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of pos sequence: 105\n",
            "Length of label sequence: 105\n",
            "[4 9 9 8 9 0 0 4 9 9 4]\n",
            "[1 1 1 1 0 1 1 1 1 1 1]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXTqZkB9XlHa",
        "outputId": "dbac0a41-ba38-4919-eb98-9d7078ca6102"
      },
      "source": [
        "# now process the 2-class dev set in the same way: padding the tokens, pos and labels, and one-hot encoding the labels\n",
        "dev_seqs_bo_padded = pad_sequences(dev_seqs_bo['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                   dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "dev_pos_bo_padded = pad_sequences(dev_seqs_bo['upos_indices'].tolist(), maxlen=seq_length,\n",
        "                                  dtype='int32', padding='post', truncating='post', value=padpos)\n",
        "dev_labs_bo_padded = pad_sequences(dev_seqs_bo['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                   dtype='int32', padding='post', truncating='post', value=padlab_bo)\n",
        "dev_labs_bo_onehot = [to_categorical(i, num_classes=n_labs_bo) for i in dev_labs_bo_padded]\n",
        "\n",
        "print('Dev set padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(dev_seqs_bo.loc[2])\n",
        "print('Length of input sequence: %i' % len(dev_seqs_bo_padded[1]))\n",
        "print('Length of pos sequence: %i' % len(dev_pos_bo_padded[1]))\n",
        "print('Length of label sequence: %i' % len(dev_labs_bo_onehot[1]))\n",
        "print(dev_pos_bo_padded[2][:11])\n",
        "print(dev_labs_bo_padded[2][:11])\n",
        "print(dev_labs_bo_onehot[2][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev set padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     2\n",
            "token            [All, I, ', ve, been, doing, is, BINGE, watchi...\n",
            "bio_only         [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
            "token_indices    [405.0, 7.0, 573.0, 12927.0, 90.0, 848.0, 52.0...\n",
            "upos_indices     [3.0, 1.0, 8.0, 0.0, 2.0, 14.0, 2.0, 9.0, 14.0...\n",
            "Name: 2, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of pos sequence: 105\n",
            "Length of label sequence: 105\n",
            "[ 3  1  8  0  2 14  2  9 14  9 16]\n",
            "[1 1 1 1 1 1 1 1 1 0 0]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ROSXIbXXOHN",
        "outputId": "d373fcc0-fc97-4c9a-e6bd-66f668dd7ef3"
      },
      "source": [
        "all_labs_bo = [l for lab in train_labs_bo_padded for l in lab]\n",
        "label_count = Counter(all_labs_bo)\n",
        "total_labs = len(all_labs_bo)\n",
        "print(label_count)\n",
        "print(total_labs)\n",
        "\n",
        "# use this to define an initial model bias\n",
        "initial_bias = [(label_count[0]/total_labs), (label_count[1]/total_labs), (label_count[2]/total_labs)]\n",
        "print('Initial bias:')\n",
        "print(initial_bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({2: 292139, 1: 59095, 0: 3141})\n",
            "354375\n",
            "Initial bias:\n",
            "[0.008863492063492063, 0.1667583774250441, 0.8243781305114638]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73LITYR5X8aY",
        "outputId": "ef757b60-5486-4b2a-99f9-8a69cd99399e"
      },
      "source": [
        "# prepare sequences and labels as numpy arrays, check dimensions\n",
        "X_token = np.array(train_seqs_bo_padded)\n",
        "X_pos = np.array(train_pos_bo_padded)\n",
        "y = np.array(train_labs_bo_onehot)\n",
        "print('Input token sequence dimensions (n.docs, seq.length):')\n",
        "print(X_token.shape)\n",
        "print('Input pos sequence dimensions (n.docs, seq.length):')\n",
        "print(X_pos.shape)\n",
        "print('Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels):')\n",
        "print(y.shape)\n",
        "\n",
        "\n",
        "# use the same model, with the only difference in the output dimensions\n",
        "def make_model_bo(metrics=METRICS, output_bias=None, seed=42):\n",
        "    init_random_seed(seed)\n",
        "    if output_bias is not None:\n",
        "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "    token = keras.layers.Input(shape=(seq_length,), name='input_token')\n",
        "    pos = keras.layers.Input(shape=(seq_length,), name='input_pos')\n",
        "    embed_token = keras.layers.Embedding(input_dim=vocab_size, output_dim=token_embed_size, input_length=seq_length, mask_zero=True, trainable=True, name='token_embedding')(token)\n",
        "    embed_pos = keras.layers.Embedding(input_dim=pos_size, output_dim=pos_embed_size, input_length=seq_length, mask_zero=True, trainable=True, name='pos_embedding')(pos)\n",
        "    concat = keras.layers.Concatenate()([embed_token, embed_pos])\n",
        "    biLSTM = keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(concat)  # 2 directions, 50 units each, concatenated (can change this)\n",
        "    dropout = keras.layers.Dropout(0.5)(biLSTM)\n",
        "    dense = keras.layers.TimeDistributed(keras.layers.Dense(n_labs_bo, activation='softmax', bias_initializer=output_bias))(dropout)\n",
        "    model = keras.Model(inputs=[token, pos], outputs=dense)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)\n",
        "    return model\n",
        "\n",
        "print('**Defining a neural network**')\n",
        "# pass the bias to the model and re-evaluate\n",
        "model = make_model_bo(output_bias=initial_bias)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input token sequence dimensions (n.docs, seq.length):\n",
            "(3375, 105)\n",
            "Input pos sequence dimensions (n.docs, seq.length):\n",
            "(3375, 105)\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels):\n",
            "(3375, 105, 3)\n",
            "**Defining a neural network**\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 105)]        0           []                               \n",
            "                                                                                                  \n",
            " input_pos (InputLayer)         [(None, 105)]        0           []                               \n",
            "                                                                                                  \n",
            " token_embedding (Embedding)    (None, 105, 128)     1894784     ['input_token[0][0]']            \n",
            "                                                                                                  \n",
            " pos_embedding (Embedding)      (None, 105, 16)      320         ['input_pos[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 105, 144)     0           ['token_embedding[0][0]',        \n",
            "                                                                  'pos_embedding[0][0]']          \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, 105, 100)    78000       ['concatenate_2[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 105, 100)     0           ['bidirectional_2[0][0]']        \n",
            "                                                                                                  \n",
            " time_distributed_2 (TimeDistri  (None, 105, 3)      303         ['dropout_2[0][0]']              \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,973,407\n",
            "Trainable params: 1,973,407\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-1Ay5BYYlQ2"
      },
      "source": [
        "# now try the weighted one-hot encoding\n",
        "def down_weight_bo(labs_onehot, weight=1):\n",
        "    weights_onehot = copy.deepcopy(labs_onehot)\n",
        "\n",
        "    # our first-pass class weights: normal for named entities (0), down-weighted for non named entities (1 and 2)\n",
        "    class_wts = [1, weight, weight]\n",
        "\n",
        "    # apply our weights to the label lists\n",
        "    for i, labs in enumerate(weights_onehot):\n",
        "        for j, lablist in enumerate(labs):\n",
        "            lablistaslist = lablist.tolist()\n",
        "            whichismax = lablistaslist.index(max(lablistaslist))\n",
        "            weights_onehot[i][j][whichismax] = class_wts[whichismax]\n",
        "    \n",
        "    return weights_onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC1Nsm2LYwuz"
      },
      "source": [
        "# prepare the dev sequences, pos and labels as numpy arrays\n",
        "dev_X_token = np.array(dev_seqs_bo_padded)\n",
        "dev_X_pos = np.array(dev_pos_bo_padded)\n",
        "dev_y = np.array(dev_labs_bo_onehot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o-vYXcQZIgh",
        "outputId": "ab4748a4-1dac-4a59-f7fc-df02c8f9cfe6"
      },
      "source": [
        "downweight_models = {}\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n",
        "    train_weights_onehot = down_weight_bo(train_labs_bo_onehot, weight)\n",
        "    y = np.array(train_weights_onehot)\n",
        "    print('Down-weighting:', weight)\n",
        "    print('Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels):', np.shape(y))\n",
        "\n",
        "    downweight_model = make_model_bo(output_bias=initial_bias)\n",
        "    downweight_model.fit([X_token, X_pos], y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=([dev_X_token, dev_X_pos], dev_y))\n",
        "\n",
        "    downweight_models[weight] = downweight_model\n",
        "    if weight == 1.0:\n",
        "        downweight_model.save('BiLSTM_PoS_2class.h5')\n",
        "        print(f'Model saved at BiLSTM_PoS_2class.h5')\n",
        "    else:\n",
        "        downweight_model.save(f'BiLSTM_PoS_2class_downweight_{weight}.h5')\n",
        "        print(f'Model saved at BiLSTM_PoS_2class_downweight_{weight}.h5')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Down-weighting: 0.1\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 49s 387ms/step - loss: 0.0243 - tp: 519359.0000 - fp: 23658.0000 - tn: 1198967.0000 - fn: 41190.0000 - accuracy: 0.7128 - precision: 0.9564 - recall: 0.9265 - auc: 0.9947 - val_loss: 0.0486 - val_tp: 100480.0000 - val_fp: 886.0000 - val_tn: 202168.0000 - val_fn: 1047.0000 - val_accuracy: 0.9937 - val_precision: 0.9913 - val_recall: 0.9897 - val_auc: 0.9998\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0082 - tp: 336878.0000 - fp: 4545.0000 - tn: 679849.0000 - fn: 5319.0000 - accuracy: 0.6641 - precision: 0.9867 - recall: 0.9845 - auc: 0.9996 - val_loss: 0.0246 - val_tp: 100694.0000 - val_fp: 740.0000 - val_tn: 202314.0000 - val_fn: 833.0000 - val_accuracy: 0.9948 - val_precision: 0.9927 - val_recall: 0.9918 - val_auc: 0.9999\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0042 - tp: 339228.0000 - fp: 2810.0000 - tn: 681584.0000 - fn: 2969.0000 - accuracy: 0.6662 - precision: 0.9918 - recall: 0.9913 - auc: 0.9998 - val_loss: 0.0180 - val_tp: 100818.0000 - val_fp: 669.0000 - val_tn: 202385.0000 - val_fn: 709.0000 - val_accuracy: 0.9955 - val_precision: 0.9934 - val_recall: 0.9930 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0024 - tp: 340719.0000 - fp: 1435.0000 - tn: 682959.0000 - fn: 1478.0000 - accuracy: 0.6677 - precision: 0.9958 - recall: 0.9957 - auc: 0.9999 - val_loss: 0.0185 - val_tp: 100769.0000 - val_fp: 737.0000 - val_tn: 202317.0000 - val_fn: 758.0000 - val_accuracy: 0.9951 - val_precision: 0.9927 - val_recall: 0.9925 - val_auc: 0.9997\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 39s 369ms/step - loss: 0.0016 - tp: 341157.0000 - fp: 1016.0000 - tn: 683378.0000 - fn: 1040.0000 - accuracy: 0.6681 - precision: 0.9970 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0197 - val_tp: 100755.0000 - val_fp: 757.0000 - val_tn: 202297.0000 - val_fn: 772.0000 - val_accuracy: 0.9950 - val_precision: 0.9925 - val_recall: 0.9924 - val_auc: 0.9996\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0013 - tp: 341394.0000 - fp: 792.0000 - tn: 683602.0000 - fn: 803.0000 - accuracy: 0.6684 - precision: 0.9977 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0207 - val_tp: 100764.0000 - val_fp: 755.0000 - val_tn: 202299.0000 - val_fn: 763.0000 - val_accuracy: 0.9950 - val_precision: 0.9926 - val_recall: 0.9925 - val_auc: 0.9994\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 9.9482e-04 - tp: 341505.0000 - fp: 688.0000 - tn: 683706.0000 - fn: 692.0000 - accuracy: 0.6685 - precision: 0.9980 - recall: 0.9980 - auc: 0.9999 - val_loss: 0.0222 - val_tp: 100734.0000 - val_fp: 780.0000 - val_tn: 202274.0000 - val_fn: 793.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9922 - val_auc: 0.9993\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 39s 365ms/step - loss: 8.2788e-04 - tp: 341632.0000 - fp: 561.0000 - tn: 683833.0000 - fn: 565.0000 - accuracy: 0.6686 - precision: 0.9984 - recall: 0.9983 - auc: 0.9999 - val_loss: 0.0231 - val_tp: 100772.0000 - val_fp: 748.0000 - val_tn: 202306.0000 - val_fn: 755.0000 - val_accuracy: 0.9951 - val_precision: 0.9926 - val_recall: 0.9926 - val_auc: 0.9991\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 6.7202e-04 - tp: 341739.0000 - fp: 453.0000 - tn: 683941.0000 - fn: 458.0000 - accuracy: 0.6688 - precision: 0.9987 - recall: 0.9987 - auc: 0.9999 - val_loss: 0.0242 - val_tp: 100732.0000 - val_fp: 793.0000 - val_tn: 202261.0000 - val_fn: 795.0000 - val_accuracy: 0.9948 - val_precision: 0.9922 - val_recall: 0.9922 - val_auc: 0.9991\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 5.7932e-04 - tp: 341775.0000 - fp: 420.0000 - tn: 683974.0000 - fn: 422.0000 - accuracy: 0.6688 - precision: 0.9988 - recall: 0.9988 - auc: 0.9999 - val_loss: 0.0269 - val_tp: 100779.0000 - val_fp: 744.0000 - val_tn: 202310.0000 - val_fn: 748.0000 - val_accuracy: 0.9951 - val_precision: 0.9927 - val_recall: 0.9926 - val_auc: 0.9988\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 4.7949e-04 - tp: 341860.0000 - fp: 330.0000 - tn: 684064.0000 - fn: 337.0000 - accuracy: 0.6689 - precision: 0.9990 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0249 - val_tp: 100780.0000 - val_fp: 742.0000 - val_tn: 202312.0000 - val_fn: 747.0000 - val_accuracy: 0.9951 - val_precision: 0.9927 - val_recall: 0.9926 - val_auc: 0.9989\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 3.9561e-04 - tp: 341913.0000 - fp: 284.0000 - tn: 684110.0000 - fn: 284.0000 - accuracy: 0.6689 - precision: 0.9992 - recall: 0.9992 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 3.9561e-04 - tp: 341913.0000 - fp: 284.0000 - tn: 684110.0000 - fn: 284.0000 - accuracy: 0.6689 - precision: 0.9992 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0268 - val_tp: 100781.0000 - val_fp: 738.0000 - val_tn: 202316.0000 - val_fn: 746.0000 - val_accuracy: 0.9951 - val_precision: 0.9927 - val_recall: 0.9927 - val_auc: 0.9987\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downweight_0.1.h5\n",
            "\n",
            "Down-weighting: 0.2\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 50s 391ms/step - loss: 0.0387 - tp: 422919.0000 - fp: 13132.0000 - tn: 874316.0000 - fn: 20805.0000 - accuracy: 0.7325 - precision: 0.9699 - recall: 0.9531 - auc: 0.9970 - val_loss: 0.0391 - val_tp: 100555.0000 - val_fp: 931.0000 - val_tn: 202123.0000 - val_fn: 972.0000 - val_accuracy: 0.9938 - val_precision: 0.9908 - val_recall: 0.9904 - val_auc: 0.9997\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0122 - tp: 338982.0000 - fp: 2780.0000 - tn: 681614.0000 - fn: 3215.0000 - accuracy: 0.6650 - precision: 0.9919 - recall: 0.9906 - auc: 0.9998 - val_loss: 0.0216 - val_tp: 100741.0000 - val_fp: 713.0000 - val_tn: 202341.0000 - val_fn: 786.0000 - val_accuracy: 0.9951 - val_precision: 0.9930 - val_recall: 0.9923 - val_auc: 0.9999\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0060 - tp: 340164.0000 - fp: 1906.0000 - tn: 682488.0000 - fn: 2033.0000 - accuracy: 0.6669 - precision: 0.9944 - recall: 0.9941 - auc: 0.9999 - val_loss: 0.0176 - val_tp: 100811.0000 - val_fp: 685.0000 - val_tn: 202369.0000 - val_fn: 716.0000 - val_accuracy: 0.9954 - val_precision: 0.9933 - val_recall: 0.9929 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 0.0035 - tp: 341150.0000 - fp: 1010.0000 - tn: 683384.0000 - fn: 1047.0000 - accuracy: 0.6680 - precision: 0.9970 - recall: 0.9969 - auc: 0.9999 - val_loss: 0.0183 - val_tp: 100784.0000 - val_fp: 723.0000 - val_tn: 202331.0000 - val_fn: 743.0000 - val_accuracy: 0.9952 - val_precision: 0.9929 - val_recall: 0.9927 - val_auc: 0.9997\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 0.0025 - tp: 341436.0000 - fp: 739.0000 - tn: 683655.0000 - fn: 761.0000 - accuracy: 0.6684 - precision: 0.9978 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0195 - val_tp: 100764.0000 - val_fp: 745.0000 - val_tn: 202309.0000 - val_fn: 763.0000 - val_accuracy: 0.9950 - val_precision: 0.9927 - val_recall: 0.9925 - val_auc: 0.9996\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 0.0020 - tp: 341560.0000 - fp: 624.0000 - tn: 683770.0000 - fn: 637.0000 - accuracy: 0.6685 - precision: 0.9982 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0215 - val_tp: 100758.0000 - val_fp: 760.0000 - val_tn: 202294.0000 - val_fn: 769.0000 - val_accuracy: 0.9950 - val_precision: 0.9925 - val_recall: 0.9924 - val_auc: 0.9993\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 39s 370ms/step - loss: 0.0016 - tp: 341632.0000 - fp: 555.0000 - tn: 683839.0000 - fn: 565.0000 - accuracy: 0.6686 - precision: 0.9984 - recall: 0.9983 - auc: 1.0000 - val_loss: 0.0227 - val_tp: 100760.0000 - val_fp: 761.0000 - val_tn: 202293.0000 - val_fn: 767.0000 - val_accuracy: 0.9950 - val_precision: 0.9925 - val_recall: 0.9924 - val_auc: 0.9992\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0013 - tp: 341728.0000 - fp: 460.0000 - tn: 683934.0000 - fn: 469.0000 - accuracy: 0.6687 - precision: 0.9987 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0244 - val_tp: 100758.0000 - val_fp: 766.0000 - val_tn: 202288.0000 - val_fn: 769.0000 - val_accuracy: 0.9950 - val_precision: 0.9925 - val_recall: 0.9924 - val_auc: 0.9991\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0011 - tp: 341801.0000 - fp: 392.0000 - tn: 684002.0000 - fn: 396.0000 - accuracy: 0.6688 - precision: 0.9989 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0251 - val_tp: 100749.0000 - val_fp: 775.0000 - val_tn: 202279.0000 - val_fn: 778.0000 - val_accuracy: 0.9949 - val_precision: 0.9924 - val_recall: 0.9923 - val_auc: 0.9990\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 39s 372ms/step - loss: 9.3798e-04 - tp: 341860.0000 - fp: 331.0000 - tn: 684063.0000 - fn: 337.0000 - accuracy: 0.6689 - precision: 0.9990 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0270 - val_tp: 100753.0000 - val_fp: 772.0000 - val_tn: 202282.0000 - val_fn: 774.0000 - val_accuracy: 0.9949 - val_precision: 0.9924 - val_recall: 0.9924 - val_auc: 0.9988\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 8.2169e-04 - tp: 341890.0000 - fp: 304.0000 - tn: 684090.0000 - fn: 307.0000 - accuracy: 0.6689 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0261 - val_tp: 100767.0000 - val_fp: 758.0000 - val_tn: 202296.0000 - val_fn: 760.0000 - val_accuracy: 0.9950 - val_precision: 0.9925 - val_recall: 0.9925 - val_auc: 0.9989\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 6.3778e-04 - tp: 341975.0000 - fp: 219.0000 - tn: 684175.0000 - fn: 222.0000 - accuracy: 0.6690 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 39s 371ms/step - loss: 6.3778e-04 - tp: 341975.0000 - fp: 219.0000 - tn: 684175.0000 - fn: 222.0000 - accuracy: 0.6690 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0267 - val_tp: 100773.0000 - val_fp: 753.0000 - val_tn: 202301.0000 - val_fn: 754.0000 - val_accuracy: 0.9951 - val_precision: 0.9926 - val_recall: 0.9926 - val_auc: 0.9988\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downweight_0.2.h5\n",
            "\n",
            "Down-weighting: 0.3\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 54s 391ms/step - loss: 0.0518 - tp: 423737.0000 - fp: 13411.0000 - tn: 874037.0000 - fn: 19987.0000 - accuracy: 0.7323 - precision: 0.9693 - recall: 0.9550 - auc: 0.9970 - val_loss: 0.0362 - val_tp: 100566.0000 - val_fp: 934.0000 - val_tn: 202120.0000 - val_fn: 961.0000 - val_accuracy: 0.9938 - val_precision: 0.9908 - val_recall: 0.9905 - val_auc: 0.9997\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0153 - tp: 339422.0000 - fp: 2509.0000 - tn: 681885.0000 - fn: 2775.0000 - accuracy: 0.6648 - precision: 0.9927 - recall: 0.9919 - auc: 0.9998 - val_loss: 0.0215 - val_tp: 100700.0000 - val_fp: 791.0000 - val_tn: 202263.0000 - val_fn: 827.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9999\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0077 - tp: 340480.0000 - fp: 1596.0000 - tn: 682798.0000 - fn: 1717.0000 - accuracy: 0.6671 - precision: 0.9953 - recall: 0.9950 - auc: 0.9999 - val_loss: 0.0183 - val_tp: 100746.0000 - val_fp: 753.0000 - val_tn: 202301.0000 - val_fn: 781.0000 - val_accuracy: 0.9950 - val_precision: 0.9926 - val_recall: 0.9923 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0044 - tp: 341290.0000 - fp: 865.0000 - tn: 683529.0000 - fn: 907.0000 - accuracy: 0.6681 - precision: 0.9975 - recall: 0.9973 - auc: 0.9999 - val_loss: 0.0187 - val_tp: 100780.0000 - val_fp: 723.0000 - val_tn: 202331.0000 - val_fn: 747.0000 - val_accuracy: 0.9952 - val_precision: 0.9929 - val_recall: 0.9926 - val_auc: 0.9997\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0031 - tp: 341525.0000 - fp: 644.0000 - tn: 683750.0000 - fn: 672.0000 - accuracy: 0.6684 - precision: 0.9981 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0199 - val_tp: 100759.0000 - val_fp: 748.0000 - val_tn: 202306.0000 - val_fn: 768.0000 - val_accuracy: 0.9950 - val_precision: 0.9926 - val_recall: 0.9924 - val_auc: 0.9996\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0025 - tp: 341640.0000 - fp: 542.0000 - tn: 683852.0000 - fn: 557.0000 - accuracy: 0.6685 - precision: 0.9984 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0222 - val_tp: 100752.0000 - val_fp: 772.0000 - val_tn: 202282.0000 - val_fn: 775.0000 - val_accuracy: 0.9949 - val_precision: 0.9924 - val_recall: 0.9924 - val_auc: 0.9993\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0021 - tp: 341707.0000 - fp: 483.0000 - tn: 683911.0000 - fn: 490.0000 - accuracy: 0.6686 - precision: 0.9986 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0237 - val_tp: 100735.0000 - val_fp: 786.0000 - val_tn: 202268.0000 - val_fn: 792.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9922 - val_auc: 0.9992\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0018 - tp: 341757.0000 - fp: 429.0000 - tn: 683965.0000 - fn: 440.0000 - accuracy: 0.6687 - precision: 0.9987 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0252 - val_tp: 100743.0000 - val_fp: 779.0000 - val_tn: 202275.0000 - val_fn: 784.0000 - val_accuracy: 0.9949 - val_precision: 0.9923 - val_recall: 0.9923 - val_auc: 0.9990\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0015 - tp: 341821.0000 - fp: 371.0000 - tn: 684023.0000 - fn: 376.0000 - accuracy: 0.6688 - precision: 0.9989 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0258 - val_tp: 100745.0000 - val_fp: 777.0000 - val_tn: 202277.0000 - val_fn: 782.0000 - val_accuracy: 0.9949 - val_precision: 0.9923 - val_recall: 0.9923 - val_auc: 0.9989\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0012 - tp: 341891.0000 - fp: 298.0000 - tn: 684096.0000 - fn: 306.0000 - accuracy: 0.6689 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0274 - val_tp: 100756.0000 - val_fp: 766.0000 - val_tn: 202288.0000 - val_fn: 771.0000 - val_accuracy: 0.9950 - val_precision: 0.9925 - val_recall: 0.9924 - val_auc: 0.9987\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0011 - tp: 341925.0000 - fp: 269.0000 - tn: 684125.0000 - fn: 272.0000 - accuracy: 0.6689 - precision: 0.9992 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0257 - val_tp: 100776.0000 - val_fp: 747.0000 - val_tn: 202307.0000 - val_fn: 751.0000 - val_accuracy: 0.9951 - val_precision: 0.9926 - val_recall: 0.9926 - val_auc: 0.9989\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 8.1185e-04 - tp: 342004.0000 - fp: 191.0000 - tn: 684203.0000 - fn: 193.0000 - accuracy: 0.6690 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 8.1185e-04 - tp: 342004.0000 - fp: 191.0000 - tn: 684203.0000 - fn: 193.0000 - accuracy: 0.6690 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0281 - val_tp: 100785.0000 - val_fp: 740.0000 - val_tn: 202314.0000 - val_fn: 742.0000 - val_accuracy: 0.9951 - val_precision: 0.9927 - val_recall: 0.9927 - val_auc: 0.9987\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downweight_0.3.h5\n",
            "\n",
            "Down-weighting: 0.4\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 51s 398ms/step - loss: 0.0645 - tp: 424167.0000 - fp: 13586.0000 - tn: 873862.0000 - fn: 19557.0000 - accuracy: 0.7322 - precision: 0.9690 - recall: 0.9559 - auc: 0.9970 - val_loss: 0.0355 - val_tp: 100571.0000 - val_fp: 935.0000 - val_tn: 202119.0000 - val_fn: 956.0000 - val_accuracy: 0.9938 - val_precision: 0.9908 - val_recall: 0.9906 - val_auc: 0.9996\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0180 - tp: 339542.0000 - fp: 2465.0000 - tn: 681929.0000 - fn: 2655.0000 - accuracy: 0.6646 - precision: 0.9928 - recall: 0.9922 - auc: 0.9998 - val_loss: 0.0224 - val_tp: 100636.0000 - val_fp: 866.0000 - val_tn: 202188.0000 - val_fn: 891.0000 - val_accuracy: 0.9942 - val_precision: 0.9915 - val_recall: 0.9912 - val_auc: 0.9999\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0092 - tp: 340568.0000 - fp: 1496.0000 - tn: 682898.0000 - fn: 1629.0000 - accuracy: 0.6670 - precision: 0.9956 - recall: 0.9952 - auc: 0.9999 - val_loss: 0.0192 - val_tp: 100708.0000 - val_fp: 793.0000 - val_tn: 202261.0000 - val_fn: 819.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0053 - tp: 341348.0000 - fp: 805.0000 - tn: 683589.0000 - fn: 849.0000 - accuracy: 0.6681 - precision: 0.9976 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0194 - val_tp: 100747.0000 - val_fp: 755.0000 - val_tn: 202299.0000 - val_fn: 780.0000 - val_accuracy: 0.9950 - val_precision: 0.9926 - val_recall: 0.9923 - val_auc: 0.9997\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0037 - tp: 341575.0000 - fp: 596.0000 - tn: 683798.0000 - fn: 622.0000 - accuracy: 0.6684 - precision: 0.9983 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0206 - val_tp: 100749.0000 - val_fp: 762.0000 - val_tn: 202292.0000 - val_fn: 778.0000 - val_accuracy: 0.9949 - val_precision: 0.9925 - val_recall: 0.9923 - val_auc: 0.9995\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0030 - tp: 341677.0000 - fp: 505.0000 - tn: 683889.0000 - fn: 520.0000 - accuracy: 0.6685 - precision: 0.9985 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0229 - val_tp: 100725.0000 - val_fp: 798.0000 - val_tn: 202256.0000 - val_fn: 802.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9921 - val_auc: 0.9993\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0026 - tp: 341720.0000 - fp: 463.0000 - tn: 683931.0000 - fn: 477.0000 - accuracy: 0.6686 - precision: 0.9986 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0245 - val_tp: 100723.0000 - val_fp: 799.0000 - val_tn: 202255.0000 - val_fn: 804.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9921 - val_auc: 0.9991\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0022 - tp: 341768.0000 - fp: 417.0000 - tn: 683977.0000 - fn: 429.0000 - accuracy: 0.6687 - precision: 0.9988 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0273 - val_tp: 100708.0000 - val_fp: 816.0000 - val_tn: 202238.0000 - val_fn: 819.0000 - val_accuracy: 0.9946 - val_precision: 0.9920 - val_recall: 0.9919 - val_auc: 0.9989\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 381ms/step - loss: 0.0018 - tp: 341826.0000 - fp: 363.0000 - tn: 684031.0000 - fn: 371.0000 - accuracy: 0.6688 - precision: 0.9989 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0276 - val_tp: 100730.0000 - val_fp: 791.0000 - val_tn: 202263.0000 - val_fn: 797.0000 - val_accuracy: 0.9948 - val_precision: 0.9922 - val_recall: 0.9921 - val_auc: 0.9988\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0015 - tp: 341897.0000 - fp: 293.0000 - tn: 684101.0000 - fn: 300.0000 - accuracy: 0.6689 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0279 - val_tp: 100735.0000 - val_fp: 788.0000 - val_tn: 202266.0000 - val_fn: 792.0000 - val_accuracy: 0.9948 - val_precision: 0.9922 - val_recall: 0.9922 - val_auc: 0.9987\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0013 - tp: 341935.0000 - fp: 254.0000 - tn: 684140.0000 - fn: 262.0000 - accuracy: 0.6689 - precision: 0.9993 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0262 - val_tp: 100773.0000 - val_fp: 750.0000 - val_tn: 202304.0000 - val_fn: 754.0000 - val_accuracy: 0.9951 - val_precision: 0.9926 - val_recall: 0.9926 - val_auc: 0.9989\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0010 - tp: 342004.0000 - fp: 190.0000 - tn: 684204.0000 - fn: 193.0000 - accuracy: 0.6690 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0010 - tp: 342004.0000 - fp: 190.0000 - tn: 684204.0000 - fn: 193.0000 - accuracy: 0.6690 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0289 - val_tp: 100770.0000 - val_fp: 751.0000 - val_tn: 202303.0000 - val_fn: 757.0000 - val_accuracy: 0.9950 - val_precision: 0.9926 - val_recall: 0.9925 - val_auc: 0.9986\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downweight_0.4.h5\n",
            "\n",
            "Down-weighting: 0.5\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 50s 396ms/step - loss: 0.0768 - tp: 424423.0000 - fp: 13730.0000 - tn: 873718.0000 - fn: 19301.0000 - accuracy: 0.7321 - precision: 0.9687 - recall: 0.9565 - auc: 0.9969 - val_loss: 0.0356 - val_tp: 100572.0000 - val_fp: 938.0000 - val_tn: 202116.0000 - val_fn: 955.0000 - val_accuracy: 0.9938 - val_precision: 0.9908 - val_recall: 0.9906 - val_auc: 0.9996\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0203 - tp: 339550.0000 - fp: 2504.0000 - tn: 681890.0000 - fn: 2647.0000 - accuracy: 0.6644 - precision: 0.9927 - recall: 0.9923 - auc: 0.9998 - val_loss: 0.0234 - val_tp: 100596.0000 - val_fp: 917.0000 - val_tn: 202137.0000 - val_fn: 931.0000 - val_accuracy: 0.9939 - val_precision: 0.9910 - val_recall: 0.9908 - val_auc: 0.9998\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0106 - tp: 340542.0000 - fp: 1527.0000 - tn: 682867.0000 - fn: 1655.0000 - accuracy: 0.6667 - precision: 0.9955 - recall: 0.9952 - auc: 0.9999 - val_loss: 0.0200 - val_tp: 100689.0000 - val_fp: 818.0000 - val_tn: 202236.0000 - val_fn: 838.0000 - val_accuracy: 0.9946 - val_precision: 0.9919 - val_recall: 0.9917 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 40s 373ms/step - loss: 0.0061 - tp: 341353.0000 - fp: 792.0000 - tn: 683602.0000 - fn: 844.0000 - accuracy: 0.6680 - precision: 0.9977 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0200 - val_tp: 100725.0000 - val_fp: 787.0000 - val_tn: 202267.0000 - val_fn: 802.0000 - val_accuracy: 0.9948 - val_precision: 0.9922 - val_recall: 0.9921 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0043 - tp: 341582.0000 - fp: 580.0000 - tn: 683814.0000 - fn: 615.0000 - accuracy: 0.6684 - precision: 0.9983 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0211 - val_tp: 100731.0000 - val_fp: 781.0000 - val_tn: 202273.0000 - val_fn: 796.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9922 - val_auc: 0.9994\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0034 - tp: 341670.0000 - fp: 507.0000 - tn: 683887.0000 - fn: 527.0000 - accuracy: 0.6685 - precision: 0.9985 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0234 - val_tp: 100710.0000 - val_fp: 812.0000 - val_tn: 202242.0000 - val_fn: 817.0000 - val_accuracy: 0.9947 - val_precision: 0.9920 - val_recall: 0.9920 - val_auc: 0.9993\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0030 - tp: 341722.0000 - fp: 452.0000 - tn: 683942.0000 - fn: 475.0000 - accuracy: 0.6686 - precision: 0.9987 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0250 - val_tp: 100707.0000 - val_fp: 815.0000 - val_tn: 202239.0000 - val_fn: 820.0000 - val_accuracy: 0.9946 - val_precision: 0.9920 - val_recall: 0.9919 - val_auc: 0.9991\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0025 - tp: 341786.0000 - fp: 404.0000 - tn: 683990.0000 - fn: 411.0000 - accuracy: 0.6687 - precision: 0.9988 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0285 - val_tp: 100684.0000 - val_fp: 841.0000 - val_tn: 202213.0000 - val_fn: 843.0000 - val_accuracy: 0.9945 - val_precision: 0.9917 - val_recall: 0.9917 - val_auc: 0.9988\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0021 - tp: 341829.0000 - fp: 357.0000 - tn: 684037.0000 - fn: 368.0000 - accuracy: 0.6687 - precision: 0.9990 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0286 - val_tp: 100706.0000 - val_fp: 817.0000 - val_tn: 202237.0000 - val_fn: 821.0000 - val_accuracy: 0.9946 - val_precision: 0.9920 - val_recall: 0.9919 - val_auc: 0.9988\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0019 - tp: 341871.0000 - fp: 319.0000 - tn: 684075.0000 - fn: 326.0000 - accuracy: 0.6688 - precision: 0.9991 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0292 - val_tp: 100718.0000 - val_fp: 809.0000 - val_tn: 202245.0000 - val_fn: 809.0000 - val_accuracy: 0.9947 - val_precision: 0.9920 - val_recall: 0.9920 - val_auc: 0.9987\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0015 - tp: 341936.0000 - fp: 258.0000 - tn: 684136.0000 - fn: 261.0000 - accuracy: 0.6689 - precision: 0.9992 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0282 - val_tp: 100735.0000 - val_fp: 790.0000 - val_tn: 202264.0000 - val_fn: 792.0000 - val_accuracy: 0.9948 - val_precision: 0.9922 - val_recall: 0.9922 - val_auc: 0.9987\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0012 - tp: 342013.0000 - fp: 179.0000 - tn: 684215.0000 - fn: 184.0000 - accuracy: 0.6690 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0012 - tp: 342013.0000 - fp: 179.0000 - tn: 684215.0000 - fn: 184.0000 - accuracy: 0.6690 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0299 - val_tp: 100742.0000 - val_fp: 780.0000 - val_tn: 202274.0000 - val_fn: 785.0000 - val_accuracy: 0.9949 - val_precision: 0.9923 - val_recall: 0.9923 - val_auc: 0.9986\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downweight_0.5.h5\n",
            "\n",
            "Down-weighting: 0.6\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 51s 396ms/step - loss: 0.0888 - tp: 424579.0000 - fp: 13830.0000 - tn: 873618.0000 - fn: 19145.0000 - accuracy: 0.7320 - precision: 0.9685 - recall: 0.9569 - auc: 0.9969 - val_loss: 0.0360 - val_tp: 100574.0000 - val_fp: 938.0000 - val_tn: 202116.0000 - val_fn: 953.0000 - val_accuracy: 0.9938 - val_precision: 0.9908 - val_recall: 0.9906 - val_auc: 0.9995\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0223 - tp: 339519.0000 - fp: 2568.0000 - tn: 681826.0000 - fn: 2678.0000 - accuracy: 0.6643 - precision: 0.9925 - recall: 0.9922 - auc: 0.9998 - val_loss: 0.0244 - val_tp: 100583.0000 - val_fp: 933.0000 - val_tn: 202121.0000 - val_fn: 944.0000 - val_accuracy: 0.9938 - val_precision: 0.9908 - val_recall: 0.9907 - val_auc: 0.9998\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 41s 384ms/step - loss: 0.0119 - tp: 340465.0000 - fp: 1610.0000 - tn: 682784.0000 - fn: 1732.0000 - accuracy: 0.6664 - precision: 0.9953 - recall: 0.9949 - auc: 0.9999 - val_loss: 0.0208 - val_tp: 100676.0000 - val_fp: 838.0000 - val_tn: 202216.0000 - val_fn: 851.0000 - val_accuracy: 0.9945 - val_precision: 0.9917 - val_recall: 0.9916 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0068 - tp: 341327.0000 - fp: 813.0000 - tn: 683581.0000 - fn: 870.0000 - accuracy: 0.6679 - precision: 0.9976 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0207 - val_tp: 100714.0000 - val_fp: 799.0000 - val_tn: 202255.0000 - val_fn: 813.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9920 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 381ms/step - loss: 0.0047 - tp: 341579.0000 - fp: 591.0000 - tn: 683803.0000 - fn: 618.0000 - accuracy: 0.6683 - precision: 0.9983 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0217 - val_tp: 100721.0000 - val_fp: 795.0000 - val_tn: 202259.0000 - val_fn: 806.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9921 - val_auc: 0.9994\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0038 - tp: 341688.0000 - fp: 484.0000 - tn: 683910.0000 - fn: 509.0000 - accuracy: 0.6685 - precision: 0.9986 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0240 - val_tp: 100700.0000 - val_fp: 826.0000 - val_tn: 202228.0000 - val_fn: 827.0000 - val_accuracy: 0.9946 - val_precision: 0.9919 - val_recall: 0.9919 - val_auc: 0.9993\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0033 - tp: 341721.0000 - fp: 456.0000 - tn: 683938.0000 - fn: 476.0000 - accuracy: 0.6686 - precision: 0.9987 - recall: 0.9986 - auc: 1.0000 - val_loss: 0.0257 - val_tp: 100696.0000 - val_fp: 829.0000 - val_tn: 202225.0000 - val_fn: 831.0000 - val_accuracy: 0.9945 - val_precision: 0.9918 - val_recall: 0.9918 - val_auc: 0.9991\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 381ms/step - loss: 0.0028 - tp: 341794.0000 - fp: 392.0000 - tn: 684002.0000 - fn: 403.0000 - accuracy: 0.6687 - precision: 0.9989 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0294 - val_tp: 100676.0000 - val_fp: 847.0000 - val_tn: 202207.0000 - val_fn: 851.0000 - val_accuracy: 0.9944 - val_precision: 0.9917 - val_recall: 0.9916 - val_auc: 0.9988\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 381ms/step - loss: 0.0024 - tp: 341833.0000 - fp: 356.0000 - tn: 684038.0000 - fn: 364.0000 - accuracy: 0.6687 - precision: 0.9990 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0299 - val_tp: 100681.0000 - val_fp: 845.0000 - val_tn: 202209.0000 - val_fn: 846.0000 - val_accuracy: 0.9944 - val_precision: 0.9917 - val_recall: 0.9917 - val_auc: 0.9987\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0021 - tp: 341882.0000 - fp: 306.0000 - tn: 684088.0000 - fn: 315.0000 - accuracy: 0.6688 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0297 - val_tp: 100701.0000 - val_fp: 826.0000 - val_tn: 202228.0000 - val_fn: 826.0000 - val_accuracy: 0.9946 - val_precision: 0.9919 - val_recall: 0.9919 - val_auc: 0.9987\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 382ms/step - loss: 0.0017 - tp: 341933.0000 - fp: 258.0000 - tn: 684136.0000 - fn: 264.0000 - accuracy: 0.6689 - precision: 0.9992 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0277 - val_tp: 100729.0000 - val_fp: 794.0000 - val_tn: 202260.0000 - val_fn: 798.0000 - val_accuracy: 0.9948 - val_precision: 0.9922 - val_recall: 0.9921 - val_auc: 0.9988\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0014 - tp: 342004.0000 - fp: 192.0000 - tn: 684202.0000 - fn: 193.0000 - accuracy: 0.6690 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0014 - tp: 342004.0000 - fp: 192.0000 - tn: 684202.0000 - fn: 193.0000 - accuracy: 0.6690 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0307 - val_tp: 100731.0000 - val_fp: 791.0000 - val_tn: 202263.0000 - val_fn: 796.0000 - val_accuracy: 0.9948 - val_precision: 0.9922 - val_recall: 0.9922 - val_auc: 0.9985\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downweight_0.6.h5\n",
            "\n",
            "Down-weighting: 0.7\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 50s 397ms/step - loss: 0.1007 - tp: 424670.0000 - fp: 13901.0000 - tn: 873547.0000 - fn: 19054.0000 - accuracy: 0.7319 - precision: 0.9683 - recall: 0.9571 - auc: 0.9968 - val_loss: 0.0365 - val_tp: 100577.0000 - val_fp: 941.0000 - val_tn: 202113.0000 - val_fn: 950.0000 - val_accuracy: 0.9938 - val_precision: 0.9907 - val_recall: 0.9906 - val_auc: 0.9995\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 376ms/step - loss: 0.0241 - tp: 339502.0000 - fp: 2607.0000 - tn: 681787.0000 - fn: 2695.0000 - accuracy: 0.6642 - precision: 0.9924 - recall: 0.9921 - auc: 0.9997 - val_loss: 0.0254 - val_tp: 100582.0000 - val_fp: 937.0000 - val_tn: 202117.0000 - val_fn: 945.0000 - val_accuracy: 0.9938 - val_precision: 0.9908 - val_recall: 0.9907 - val_auc: 0.9998\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0132 - tp: 340360.0000 - fp: 1708.0000 - tn: 682686.0000 - fn: 1837.0000 - accuracy: 0.6661 - precision: 0.9950 - recall: 0.9946 - auc: 0.9999 - val_loss: 0.0216 - val_tp: 100652.0000 - val_fp: 864.0000 - val_tn: 202190.0000 - val_fn: 875.0000 - val_accuracy: 0.9943 - val_precision: 0.9915 - val_recall: 0.9914 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0075 - tp: 341291.0000 - fp: 845.0000 - tn: 683549.0000 - fn: 906.0000 - accuracy: 0.6678 - precision: 0.9975 - recall: 0.9974 - auc: 0.9999 - val_loss: 0.0213 - val_tp: 100693.0000 - val_fp: 817.0000 - val_tn: 202237.0000 - val_fn: 834.0000 - val_accuracy: 0.9946 - val_precision: 0.9920 - val_recall: 0.9918 - val_auc: 0.9996\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 41s 388ms/step - loss: 0.0052 - tp: 341578.0000 - fp: 585.0000 - tn: 683809.0000 - fn: 619.0000 - accuracy: 0.6683 - precision: 0.9983 - recall: 0.9982 - auc: 0.9999 - val_loss: 0.0223 - val_tp: 100707.0000 - val_fp: 810.0000 - val_tn: 202244.0000 - val_fn: 820.0000 - val_accuracy: 0.9946 - val_precision: 0.9920 - val_recall: 0.9919 - val_auc: 0.9994\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0042 - tp: 341688.0000 - fp: 485.0000 - tn: 683909.0000 - fn: 509.0000 - accuracy: 0.6685 - precision: 0.9986 - recall: 0.9985 - auc: 0.9999 - val_loss: 0.0248 - val_tp: 100694.0000 - val_fp: 831.0000 - val_tn: 202223.0000 - val_fn: 833.0000 - val_accuracy: 0.9945 - val_precision: 0.9918 - val_recall: 0.9918 - val_auc: 0.9992\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 41s 383ms/step - loss: 0.0036 - tp: 341719.0000 - fp: 451.0000 - tn: 683943.0000 - fn: 478.0000 - accuracy: 0.6685 - precision: 0.9987 - recall: 0.9986 - auc: 0.9999 - val_loss: 0.0263 - val_tp: 100695.0000 - val_fp: 832.0000 - val_tn: 202222.0000 - val_fn: 832.0000 - val_accuracy: 0.9945 - val_precision: 0.9918 - val_recall: 0.9918 - val_auc: 0.9990\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 41s 385ms/step - loss: 0.0030 - tp: 341793.0000 - fp: 390.0000 - tn: 684004.0000 - fn: 404.0000 - accuracy: 0.6686 - precision: 0.9989 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0298 - val_tp: 100684.0000 - val_fp: 841.0000 - val_tn: 202213.0000 - val_fn: 843.0000 - val_accuracy: 0.9945 - val_precision: 0.9917 - val_recall: 0.9917 - val_auc: 0.9987\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0026 - tp: 341845.0000 - fp: 335.0000 - tn: 684059.0000 - fn: 352.0000 - accuracy: 0.6687 - precision: 0.9990 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0304 - val_tp: 100684.0000 - val_fp: 842.0000 - val_tn: 202212.0000 - val_fn: 843.0000 - val_accuracy: 0.9945 - val_precision: 0.9917 - val_recall: 0.9917 - val_auc: 0.9987\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 41s 382ms/step - loss: 0.0023 - tp: 341886.0000 - fp: 301.0000 - tn: 684093.0000 - fn: 311.0000 - accuracy: 0.6688 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0307 - val_tp: 100699.0000 - val_fp: 828.0000 - val_tn: 202226.0000 - val_fn: 828.0000 - val_accuracy: 0.9946 - val_precision: 0.9918 - val_recall: 0.9918 - val_auc: 0.9986\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 41s 386ms/step - loss: 0.0019 - tp: 341936.0000 - fp: 248.0000 - tn: 684146.0000 - fn: 261.0000 - accuracy: 0.6689 - precision: 0.9993 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.0279 - val_tp: 100719.0000 - val_fp: 806.0000 - val_tn: 202248.0000 - val_fn: 808.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9920 - val_auc: 0.9988\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0016 - tp: 341995.0000 - fp: 198.0000 - tn: 684196.0000 - fn: 202.0000 - accuracy: 0.6689 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 41s 387ms/step - loss: 0.0016 - tp: 341995.0000 - fp: 198.0000 - tn: 684196.0000 - fn: 202.0000 - accuracy: 0.6689 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0323 - val_tp: 100704.0000 - val_fp: 821.0000 - val_tn: 202233.0000 - val_fn: 823.0000 - val_accuracy: 0.9946 - val_precision: 0.9919 - val_recall: 0.9919 - val_auc: 0.9984\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downweight_0.7.h5\n",
            "\n",
            "Down-weighting: 0.8\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 53s 405ms/step - loss: 0.1126 - tp: 424758.0000 - fp: 13993.0000 - tn: 873455.0000 - fn: 18966.0000 - accuracy: 0.7318 - precision: 0.9681 - recall: 0.9573 - auc: 0.9967 - val_loss: 0.0372 - val_tp: 100577.0000 - val_fp: 937.0000 - val_tn: 202117.0000 - val_fn: 950.0000 - val_accuracy: 0.9938 - val_precision: 0.9908 - val_recall: 0.9906 - val_auc: 0.9995\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 41s 386ms/step - loss: 0.0258 - tp: 339497.0000 - fp: 2634.0000 - tn: 681760.0000 - fn: 2700.0000 - accuracy: 0.6641 - precision: 0.9923 - recall: 0.9921 - auc: 0.9997 - val_loss: 0.0264 - val_tp: 100582.0000 - val_fp: 937.0000 - val_tn: 202117.0000 - val_fn: 945.0000 - val_accuracy: 0.9938 - val_precision: 0.9908 - val_recall: 0.9907 - val_auc: 0.9998\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0144 - tp: 340219.0000 - fp: 1839.0000 - tn: 682555.0000 - fn: 1978.0000 - accuracy: 0.6658 - precision: 0.9946 - recall: 0.9942 - auc: 0.9999 - val_loss: 0.0223 - val_tp: 100639.0000 - val_fp: 880.0000 - val_tn: 202174.0000 - val_fn: 888.0000 - val_accuracy: 0.9942 - val_precision: 0.9913 - val_recall: 0.9913 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 41s 384ms/step - loss: 0.0082 - tp: 341261.0000 - fp: 875.0000 - tn: 683519.0000 - fn: 936.0000 - accuracy: 0.6677 - precision: 0.9974 - recall: 0.9973 - auc: 0.9999 - val_loss: 0.0219 - val_tp: 100689.0000 - val_fp: 827.0000 - val_tn: 202227.0000 - val_fn: 838.0000 - val_accuracy: 0.9945 - val_precision: 0.9919 - val_recall: 0.9917 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 41s 386ms/step - loss: 0.0056 - tp: 341563.0000 - fp: 589.0000 - tn: 683805.0000 - fn: 634.0000 - accuracy: 0.6683 - precision: 0.9983 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0230 - val_tp: 100694.0000 - val_fp: 823.0000 - val_tn: 202231.0000 - val_fn: 833.0000 - val_accuracy: 0.9946 - val_precision: 0.9919 - val_recall: 0.9918 - val_auc: 0.9994\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 41s 388ms/step - loss: 0.0045 - tp: 341673.0000 - fp: 495.0000 - tn: 683899.0000 - fn: 524.0000 - accuracy: 0.6684 - precision: 0.9986 - recall: 0.9985 - auc: 0.9999 - val_loss: 0.0254 - val_tp: 100690.0000 - val_fp: 836.0000 - val_tn: 202218.0000 - val_fn: 837.0000 - val_accuracy: 0.9945 - val_precision: 0.9918 - val_recall: 0.9918 - val_auc: 0.9991\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 41s 384ms/step - loss: 0.0039 - tp: 341719.0000 - fp: 458.0000 - tn: 683936.0000 - fn: 478.0000 - accuracy: 0.6685 - precision: 0.9987 - recall: 0.9986 - auc: 0.9999 - val_loss: 0.0268 - val_tp: 100686.0000 - val_fp: 838.0000 - val_tn: 202216.0000 - val_fn: 841.0000 - val_accuracy: 0.9945 - val_precision: 0.9917 - val_recall: 0.9917 - val_auc: 0.9990\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 41s 383ms/step - loss: 0.0033 - tp: 341794.0000 - fp: 383.0000 - tn: 684011.0000 - fn: 403.0000 - accuracy: 0.6686 - precision: 0.9989 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0306 - val_tp: 100674.0000 - val_fp: 849.0000 - val_tn: 202205.0000 - val_fn: 853.0000 - val_accuracy: 0.9944 - val_precision: 0.9916 - val_recall: 0.9916 - val_auc: 0.9987\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0028 - tp: 341840.0000 - fp: 349.0000 - tn: 684045.0000 - fn: 357.0000 - accuracy: 0.6687 - precision: 0.9990 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0314 - val_tp: 100681.0000 - val_fp: 845.0000 - val_tn: 202209.0000 - val_fn: 846.0000 - val_accuracy: 0.9944 - val_precision: 0.9917 - val_recall: 0.9917 - val_auc: 0.9986\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 382ms/step - loss: 0.0024 - tp: 341894.0000 - fp: 293.0000 - tn: 684101.0000 - fn: 303.0000 - accuracy: 0.6688 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0317 - val_tp: 100688.0000 - val_fp: 839.0000 - val_tn: 202215.0000 - val_fn: 839.0000 - val_accuracy: 0.9945 - val_precision: 0.9917 - val_recall: 0.9917 - val_auc: 0.9985\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 382ms/step - loss: 0.0020 - tp: 341949.0000 - fp: 238.0000 - tn: 684156.0000 - fn: 248.0000 - accuracy: 0.6689 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0276 - val_tp: 100722.0000 - val_fp: 802.0000 - val_tn: 202252.0000 - val_fn: 805.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9921 - val_auc: 0.9988\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0017 - tp: 342005.0000 - fp: 186.0000 - tn: 684208.0000 - fn: 192.0000 - accuracy: 0.6689 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0017 - tp: 342005.0000 - fp: 186.0000 - tn: 684208.0000 - fn: 192.0000 - accuracy: 0.6689 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0338 - val_tp: 100698.0000 - val_fp: 826.0000 - val_tn: 202228.0000 - val_fn: 829.0000 - val_accuracy: 0.9946 - val_precision: 0.9919 - val_recall: 0.9918 - val_auc: 0.9983\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downweight_0.8.h5\n",
            "\n",
            "Down-weighting: 0.9\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 52s 404ms/step - loss: 0.1243 - tp: 424823.0000 - fp: 14046.0000 - tn: 873402.0000 - fn: 18901.0000 - accuracy: 0.7318 - precision: 0.9680 - recall: 0.9574 - auc: 0.9967 - val_loss: 0.0379 - val_tp: 100578.0000 - val_fp: 937.0000 - val_tn: 202117.0000 - val_fn: 949.0000 - val_accuracy: 0.9938 - val_precision: 0.9908 - val_recall: 0.9907 - val_auc: 0.9994\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 382ms/step - loss: 0.0273 - tp: 339495.0000 - fp: 2656.0000 - tn: 681738.0000 - fn: 2702.0000 - accuracy: 0.6641 - precision: 0.9922 - recall: 0.9921 - auc: 0.9997 - val_loss: 0.0274 - val_tp: 100583.0000 - val_fp: 939.0000 - val_tn: 202115.0000 - val_fn: 944.0000 - val_accuracy: 0.9938 - val_precision: 0.9908 - val_recall: 0.9907 - val_auc: 0.9998\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 375ms/step - loss: 0.0155 - tp: 340109.0000 - fp: 1948.0000 - tn: 682446.0000 - fn: 2088.0000 - accuracy: 0.6655 - precision: 0.9943 - recall: 0.9939 - auc: 0.9999 - val_loss: 0.0231 - val_tp: 100633.0000 - val_fp: 888.0000 - val_tn: 202166.0000 - val_fn: 894.0000 - val_accuracy: 0.9941 - val_precision: 0.9913 - val_recall: 0.9912 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0089 - tp: 341203.0000 - fp: 936.0000 - tn: 683458.0000 - fn: 994.0000 - accuracy: 0.6676 - precision: 0.9973 - recall: 0.9971 - auc: 0.9999 - val_loss: 0.0226 - val_tp: 100675.0000 - val_fp: 840.0000 - val_tn: 202214.0000 - val_fn: 852.0000 - val_accuracy: 0.9944 - val_precision: 0.9917 - val_recall: 0.9916 - val_auc: 0.9995\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 41s 386ms/step - loss: 0.0060 - tp: 341559.0000 - fp: 591.0000 - tn: 683803.0000 - fn: 638.0000 - accuracy: 0.6682 - precision: 0.9983 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0235 - val_tp: 100684.0000 - val_fp: 837.0000 - val_tn: 202217.0000 - val_fn: 843.0000 - val_accuracy: 0.9945 - val_precision: 0.9918 - val_recall: 0.9917 - val_auc: 0.9993\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 40s 381ms/step - loss: 0.0048 - tp: 341678.0000 - fp: 485.0000 - tn: 683909.0000 - fn: 519.0000 - accuracy: 0.6684 - precision: 0.9986 - recall: 0.9985 - auc: 0.9999 - val_loss: 0.0260 - val_tp: 100678.0000 - val_fp: 848.0000 - val_tn: 202206.0000 - val_fn: 849.0000 - val_accuracy: 0.9944 - val_precision: 0.9916 - val_recall: 0.9916 - val_auc: 0.9991\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0042 - tp: 341715.0000 - fp: 460.0000 - tn: 683934.0000 - fn: 482.0000 - accuracy: 0.6685 - precision: 0.9987 - recall: 0.9986 - auc: 0.9999 - val_loss: 0.0277 - val_tp: 100680.0000 - val_fp: 845.0000 - val_tn: 202209.0000 - val_fn: 847.0000 - val_accuracy: 0.9944 - val_precision: 0.9917 - val_recall: 0.9917 - val_auc: 0.9989\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 381ms/step - loss: 0.0035 - tp: 341783.0000 - fp: 394.0000 - tn: 684000.0000 - fn: 414.0000 - accuracy: 0.6686 - precision: 0.9988 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0310 - val_tp: 100678.0000 - val_fp: 846.0000 - val_tn: 202208.0000 - val_fn: 849.0000 - val_accuracy: 0.9944 - val_precision: 0.9917 - val_recall: 0.9916 - val_auc: 0.9987\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0030 - tp: 341824.0000 - fp: 357.0000 - tn: 684037.0000 - fn: 373.0000 - accuracy: 0.6687 - precision: 0.9990 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0319 - val_tp: 100674.0000 - val_fp: 849.0000 - val_tn: 202205.0000 - val_fn: 853.0000 - val_accuracy: 0.9944 - val_precision: 0.9916 - val_recall: 0.9916 - val_auc: 0.9986\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0026 - tp: 341892.0000 - fp: 294.0000 - tn: 684100.0000 - fn: 305.0000 - accuracy: 0.6688 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0321 - val_tp: 100679.0000 - val_fp: 846.0000 - val_tn: 202208.0000 - val_fn: 848.0000 - val_accuracy: 0.9944 - val_precision: 0.9917 - val_recall: 0.9916 - val_auc: 0.9985\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0022 - tp: 341953.0000 - fp: 238.0000 - tn: 684156.0000 - fn: 244.0000 - accuracy: 0.6688 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0283 - val_tp: 100704.0000 - val_fp: 820.0000 - val_tn: 202234.0000 - val_fn: 823.0000 - val_accuracy: 0.9946 - val_precision: 0.9919 - val_recall: 0.9919 - val_auc: 0.9987\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0019 - tp: 341990.0000 - fp: 204.0000 - tn: 684190.0000 - fn: 207.0000 - accuracy: 0.6689 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 41s 383ms/step - loss: 0.0019 - tp: 341990.0000 - fp: 204.0000 - tn: 684190.0000 - fn: 207.0000 - accuracy: 0.6689 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0350 - val_tp: 100690.0000 - val_fp: 834.0000 - val_tn: 202220.0000 - val_fn: 837.0000 - val_accuracy: 0.9945 - val_precision: 0.9918 - val_recall: 0.9918 - val_auc: 0.9981\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downweight_0.9.h5\n",
            "\n",
            "Down-weighting: 1.0\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (3375, 105, 3)\n",
            "Epoch 1/100\n",
            "106/106 [==============================] - 50s 396ms/step - loss: 0.1359 - tp: 424871.0000 - fp: 14083.0000 - tn: 873365.0000 - fn: 18853.0000 - accuracy: 0.9753 - precision: 0.9679 - recall: 0.9575 - auc: 0.9966 - val_loss: 0.0385 - val_tp: 100578.0000 - val_fp: 940.0000 - val_tn: 202114.0000 - val_fn: 949.0000 - val_accuracy: 0.9938 - val_precision: 0.9907 - val_recall: 0.9907 - val_auc: 0.9994\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0287 - tp: 339493.0000 - fp: 2663.0000 - tn: 681731.0000 - fn: 2704.0000 - accuracy: 0.9948 - precision: 0.9922 - recall: 0.9921 - auc: 0.9997 - val_loss: 0.0282 - val_tp: 100586.0000 - val_fp: 940.0000 - val_tn: 202114.0000 - val_fn: 941.0000 - val_accuracy: 0.9938 - val_precision: 0.9907 - val_recall: 0.9907 - val_auc: 0.9998\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0167 - tp: 340019.0000 - fp: 2055.0000 - tn: 682339.0000 - fn: 2178.0000 - accuracy: 0.9959 - precision: 0.9940 - recall: 0.9936 - auc: 0.9999 - val_loss: 0.0237 - val_tp: 100626.0000 - val_fp: 894.0000 - val_tn: 202160.0000 - val_fn: 901.0000 - val_accuracy: 0.9941 - val_precision: 0.9912 - val_recall: 0.9911 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 40s 382ms/step - loss: 0.0095 - tp: 341139.0000 - fp: 984.0000 - tn: 683410.0000 - fn: 1058.0000 - accuracy: 0.9980 - precision: 0.9971 - recall: 0.9969 - auc: 0.9999 - val_loss: 0.0231 - val_tp: 100671.0000 - val_fp: 847.0000 - val_tn: 202207.0000 - val_fn: 856.0000 - val_accuracy: 0.9944 - val_precision: 0.9917 - val_recall: 0.9916 - val_auc: 0.9994\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 40s 379ms/step - loss: 0.0064 - tp: 341533.0000 - fp: 611.0000 - tn: 683783.0000 - fn: 664.0000 - accuracy: 0.9988 - precision: 0.9982 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0241 - val_tp: 100678.0000 - val_fp: 843.0000 - val_tn: 202211.0000 - val_fn: 849.0000 - val_accuracy: 0.9944 - val_precision: 0.9917 - val_recall: 0.9916 - val_auc: 0.9993\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0051 - tp: 341667.0000 - fp: 492.0000 - tn: 683902.0000 - fn: 530.0000 - accuracy: 0.9990 - precision: 0.9986 - recall: 0.9985 - auc: 0.9999 - val_loss: 0.0265 - val_tp: 100670.0000 - val_fp: 851.0000 - val_tn: 202203.0000 - val_fn: 857.0000 - val_accuracy: 0.9944 - val_precision: 0.9916 - val_recall: 0.9916 - val_auc: 0.9991\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0044 - tp: 341713.0000 - fp: 457.0000 - tn: 683937.0000 - fn: 484.0000 - accuracy: 0.9991 - precision: 0.9987 - recall: 0.9986 - auc: 0.9999 - val_loss: 0.0281 - val_tp: 100679.0000 - val_fp: 845.0000 - val_tn: 202209.0000 - val_fn: 848.0000 - val_accuracy: 0.9944 - val_precision: 0.9917 - val_recall: 0.9916 - val_auc: 0.9989\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 40s 378ms/step - loss: 0.0037 - tp: 341784.0000 - fp: 390.0000 - tn: 684004.0000 - fn: 413.0000 - accuracy: 0.9992 - precision: 0.9989 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0312 - val_tp: 100668.0000 - val_fp: 852.0000 - val_tn: 202202.0000 - val_fn: 859.0000 - val_accuracy: 0.9944 - val_precision: 0.9916 - val_recall: 0.9915 - val_auc: 0.9987\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0032 - tp: 341831.0000 - fp: 350.0000 - tn: 684044.0000 - fn: 366.0000 - accuracy: 0.9993 - precision: 0.9990 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0324 - val_tp: 100674.0000 - val_fp: 852.0000 - val_tn: 202202.0000 - val_fn: 853.0000 - val_accuracy: 0.9944 - val_precision: 0.9916 - val_recall: 0.9916 - val_auc: 0.9986\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 40s 374ms/step - loss: 0.0028 - tp: 341886.0000 - fp: 299.0000 - tn: 684095.0000 - fn: 311.0000 - accuracy: 0.9994 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0327 - val_tp: 100682.0000 - val_fp: 843.0000 - val_tn: 202211.0000 - val_fn: 845.0000 - val_accuracy: 0.9945 - val_precision: 0.9917 - val_recall: 0.9917 - val_auc: 0.9985\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 40s 380ms/step - loss: 0.0023 - tp: 341948.0000 - fp: 236.0000 - tn: 684158.0000 - fn: 249.0000 - accuracy: 0.9995 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0285 - val_tp: 100706.0000 - val_fp: 818.0000 - val_tn: 202236.0000 - val_fn: 821.0000 - val_accuracy: 0.9946 - val_precision: 0.9919 - val_recall: 0.9919 - val_auc: 0.9987\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0020 - tp: 341986.0000 - fp: 210.0000 - tn: 684184.0000 - fn: 211.0000 - accuracy: 0.9996 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
            "106/106 [==============================] - 40s 377ms/step - loss: 0.0020 - tp: 341986.0000 - fp: 210.0000 - tn: 684184.0000 - fn: 211.0000 - accuracy: 0.9996 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0356 - val_tp: 100687.0000 - val_fp: 838.0000 - val_tn: 202216.0000 - val_fn: 840.0000 - val_accuracy: 0.9945 - val_precision: 0.9917 - val_recall: 0.9917 - val_auc: 0.9981\n",
            "Epoch 00012: early stopping\n",
            "Model saved at BiLSTM_PoS_2class.h5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB2PFFmubyDR"
      },
      "source": [
        "def reverse_bo(ind):\n",
        "    bo = 'O'  # for any pad=2 predictions\n",
        "    if ind == 0:\n",
        "        bo = 'B'\n",
        "    elif ind == 1:\n",
        "        bo = 'O'\n",
        "    return bo\n",
        "\n",
        "def reverse_bio_from_bo(inds):\n",
        "    bo = [reverse_bo(i) for i in inds]\n",
        "    bio = copy.deepcopy(bo)\n",
        "    for i in range(len(bo)):\n",
        "        if i >= 1 and bo[i] == 'B' and bo[i - 1] == 'B':\n",
        "            bio[i] = 'I'\n",
        "    return bio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQocmL1cby-p",
        "outputId": "3b884223-501b-4d5d-c132-415ffba7cfc0"
      },
      "source": [
        "for weight in [1.0, .9, .8, .7, .6, .5, .4, .3, .2, .1]:\n",
        "    preds = np.argmax(downweight_models[weight].predict([dev_seqs_bo_padded, dev_pos_bo_padded]), axis=-1)\n",
        "\n",
        "    dev_seqs_bo['prediction'] = ''\n",
        "    for i in dev_seqs_bo.index:\n",
        "        this_seq_length = len(dev_seqs_bo['token'][i])\n",
        "        dev_seqs_bo['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    dev_long = dev_seqs_bo.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = reverse_bio_from_bo(dev_long['bio_only'])\n",
        "    dev_long['bio_only'] = bio_labs\n",
        "    pred_labs = reverse_bio_from_bo(dev_long['prediction'])\n",
        "    dev_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(dev_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 1.0:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 805\n",
            "True positives = 0, False positives = 0, False negatives = 805\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 805\n",
            "True positives = 0, False positives = 0, False negatives = 805\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 805\n",
            "True positives = 0, False positives = 0, False negatives = 805\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 1\n",
            "Sum of TP and FN = 805\n",
            "True positives = 0, False positives = 1, False negatives = 805\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 12\n",
            "Sum of TP and FN = 805\n",
            "True positives = 2, False positives = 10, False negatives = 803\n",
            "Precision = 0.167, Recall = 0.002, F1 = 0.005\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 46\n",
            "Sum of TP and FN = 805\n",
            "True positives = 10, False positives = 36, False negatives = 795\n",
            "Precision = 0.217, Recall = 0.012, F1 = 0.024\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 112\n",
            "Sum of TP and FN = 805\n",
            "True positives = 42, False positives = 70, False negatives = 763\n",
            "Precision = 0.375, Recall = 0.052, F1 = 0.092\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 271\n",
            "Sum of TP and FN = 805\n",
            "True positives = 107, False positives = 164, False negatives = 698\n",
            "Precision = 0.395, Recall = 0.133, F1 = 0.199\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 518\n",
            "Sum of TP and FN = 805\n",
            "True positives = 215, False positives = 303, False negatives = 590\n",
            "Precision = 0.415, Recall = 0.267, F1 = 0.325\n",
            "\n",
            "Weight = 0.1:\n",
            "Sum of TP and FP = 826\n",
            "Sum of TP and FN = 805\n",
            "True positives = 340, False positives = 486, False negatives = 465\n",
            "Precision = 0.412, Recall = 0.422, F1 = 0.417\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4YMgoiHVTJG"
      },
      "source": [
        "#### Merging B and I labels, with down-sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "MFjxz4eCuZ6M",
        "outputId": "3fbce183-4d58-4efc-c084-2f90e32cb6db"
      },
      "source": [
        "mask = train_seqs.bio_only.apply(lambda labels: 0.0 in labels)\n",
        "train_seqs_downsampled = train_seqs_bo[mask]\n",
        "train_seqs_downsampled.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>upos_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[@paulwalk, It, 's, the, view, from, where, I,...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 0.0, 4.0, 5.0, 1.0, 6.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[From, Green, Newsfeed, :, AHFA, extends, dead...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....</td>\n",
              "      <td>[4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Pxleyes, Top, 50, Photography, Contest, Pictu...</td>\n",
              "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....</td>\n",
              "      <td>[13.0, 11.0, 7.0, 0.0, 9.0, 0.0, 4.0, 9.0, 7.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[4Dbling, 's, place, til, monday, ,, party, pa...</td>\n",
              "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...</td>\n",
              "      <td>[0.0, 2.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>[watching, the, VMA, pre-show, again, lol, it,...</td>\n",
              "      <td>[1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[65.0, 3.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0...</td>\n",
              "      <td>[14.0, 3.0, 9.0, 5.0, 5.0, 13.0, 1.0, 2.0, 15....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                       upos_indices\n",
              "0             0  ...  [0.0, 1.0, 2.0, 3.0, 0.0, 4.0, 5.0, 1.0, 6.0, ...\n",
              "1             1  ...  [4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...\n",
              "2             2  ...  [13.0, 11.0, 7.0, 0.0, 9.0, 0.0, 4.0, 9.0, 7.0...\n",
              "4             4  ...  [0.0, 2.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, ...\n",
              "5             5  ...  [14.0, 3.0, 9.0, 5.0, 5.0, 13.0, 1.0, 2.0, 15....\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0AfMkJ9ubzc",
        "outputId": "8eb8ef81-4579-418b-a329-b7efda8b953f"
      },
      "source": [
        "train_seqs_downsampled_padded = pad_sequences(train_seqs_downsampled['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                              dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "train_pos_downsampled_padded = pad_sequences(train_seqs_downsampled['upos_indices'].tolist(), maxlen=seq_length,\n",
        "                                             dtype='int32', padding='post', truncating='post', value=padpos)\n",
        "train_labs_downsampled_padded = pad_sequences(train_seqs_downsampled['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                              dtype='int32', padding='post', truncating='post', value=padlab_bo)\n",
        "train_labs_downsampled_onehot = [to_categorical(i, num_classes=n_labs_bo) for i in train_labs_downsampled_padded]\n",
        "\n",
        "# follow the print outputs below to see how the labels are transformed\n",
        "print('Example of padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(train_seqs_downsampled.loc[1])\n",
        "print('Length of input sequence: %i' % len(train_seqs_downsampled_padded[1]))\n",
        "print('Length of pos sequence: %i' % len(train_pos_downsampled_padded[1]))\n",
        "print('Length of label sequence: %i' % len(train_labs_downsampled_onehot[1]))\n",
        "print(train_labs_downsampled_padded[1][:11])\n",
        "print(train_pos_downsampled_padded[1][:11])\n",
        "print(train_labs_downsampled_onehot[1][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     1\n",
            "token            [From, Green, Newsfeed, :, AHFA, extends, dead...\n",
            "bio_only         [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...\n",
            "token_indices    [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
            "upos_indices     [4.0, 9.0, 9.0, 8.0, 9.0, 0.0, 0.0, 4.0, 9.0, ...\n",
            "Name: 1, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of pos sequence: 105\n",
            "Length of label sequence: 105\n",
            "[1 1 1 1 0 1 1 1 1 1 1]\n",
            "[4 9 9 8 9 0 0 4 9 9 4]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djoCcyJOuv1s",
        "outputId": "e81e2737-9f93-401a-9a27-aa4338068d5c"
      },
      "source": [
        "# figure out the label distribution in our downsampled fixed-length texts\n",
        "all_labs = [l for lab in train_labs_downsampled_padded for l in lab]\n",
        "label_count = Counter(all_labs)\n",
        "total_labs = len(all_labs)\n",
        "print(label_count)\n",
        "print(total_labs)\n",
        "\n",
        "# use this to define an initial model bias\n",
        "initial_bias = [(label_count[0]/total_labs), (label_count[1]/total_labs), (label_count[2]/total_labs)]\n",
        "print('Initial bias:')\n",
        "print(initial_bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({2: 103017, 1: 22152, 0: 3141})\n",
            "128310\n",
            "Initial bias:\n",
            "[0.02447977554360533, 0.17264437689969606, 0.8028758475566986]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj0xS38Yu1CH",
        "outputId": "8b47e12e-565b-43be-b5ff-becebf7d3487"
      },
      "source": [
        "# prepare sequences and labels as numpy arrays, check dimensions\n",
        "X_token = np.array(train_seqs_downsampled_padded)\n",
        "X_pos = np.array(train_pos_downsampled_padded)\n",
        "y = np.array(train_labs_downsampled_onehot)\n",
        "print('Input token sequence dimensions (n.docs, seq.length):')\n",
        "print(X_token.shape)\n",
        "print('Input pos sequence dimensions (n.docs, seq.length):')\n",
        "print(X_pos.shape)\n",
        "print('Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels):')\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input token sequence dimensions (n.docs, seq.length):\n",
            "(1222, 105)\n",
            "Input pos sequence dimensions (n.docs, seq.length):\n",
            "(1222, 105)\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels):\n",
            "(1222, 105, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsa4pTaNvFV1",
        "outputId": "50a0fe02-4071-47d0-8533-caa24fd4d441"
      },
      "source": [
        "# now try the weighted one-hot encoding\n",
        "downweight_models = {}\n",
        "\n",
        "for weight in [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n",
        "    train_weights_onehot = down_weight_bo(train_labs_downsampled_onehot, weight)\n",
        "    y = np.array(train_weights_onehot)\n",
        "    print('Down-weighting:', weight)\n",
        "    print('Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels):', np.shape(y))\n",
        "\n",
        "    downweight_model = make_model_bo(output_bias=initial_bias)\n",
        "    downweight_model.fit([X_token, X_pos], y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=([dev_X_token, dev_X_pos], dev_y))\n",
        "\n",
        "    downweight_models[weight] = downweight_model\n",
        "    if weight == 1.0:\n",
        "        downweight_model.save('BiLSTM_PoS_2class_downsampled.h5')\n",
        "        print(f'Model saved at BiLSTM_PoS_2class_downsampled.h5')\n",
        "    else:\n",
        "        downweight_model.save(f'BiLSTM_PoS_2class_downsampled_downweight_{weight}.h5')\n",
        "        print(f'Model saved at BiLSTM_PoS_2class_downsampled_downweight_{weight}.h5')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Down-weighting: 0.1\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 26s 464ms/step - loss: 0.0563 - tp: 102436.0000 - fp: 10324.0000 - tn: 1263549.0000 - fn: 363402.0000 - accuracy: 0.7206 - precision: 0.9084 - recall: 0.2199 - auc: 0.9715 - val_loss: 0.1361 - val_tp: 88669.0000 - val_fp: 7503.0000 - val_tn: 195551.0000 - val_fn: 12858.0000 - val_accuracy: 0.9332 - val_precision: 0.9220 - val_recall: 0.8734 - val_auc: 0.9926\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0255 - tp: 108843.0000 - fp: 11156.0000 - tn: 236126.0000 - fn: 14798.0000 - accuracy: 0.6420 - precision: 0.9070 - recall: 0.8803 - auc: 0.9901 - val_loss: 0.1069 - val_tp: 91613.0000 - val_fp: 8185.0000 - val_tn: 194869.0000 - val_fn: 9914.0000 - val_accuracy: 0.9406 - val_precision: 0.9180 - val_recall: 0.9024 - val_auc: 0.9941\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0197 - tp: 114496.0000 - fp: 7797.0000 - tn: 239485.0000 - fn: 9145.0000 - accuracy: 0.6518 - precision: 0.9362 - recall: 0.9260 - auc: 0.9948 - val_loss: 0.0531 - val_tp: 99731.0000 - val_fp: 1321.0000 - val_tn: 201733.0000 - val_fn: 1796.0000 - val_accuracy: 0.9898 - val_precision: 0.9869 - val_recall: 0.9823 - val_auc: 0.9996\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 417ms/step - loss: 0.0126 - tp: 119382.0000 - fp: 3783.0000 - tn: 243499.0000 - fn: 4259.0000 - accuracy: 0.6630 - precision: 0.9693 - recall: 0.9656 - auc: 0.9983 - val_loss: 0.0334 - val_tp: 100341.0000 - val_fp: 1005.0000 - val_tn: 202049.0000 - val_fn: 1186.0000 - val_accuracy: 0.9928 - val_precision: 0.9901 - val_recall: 0.9883 - val_auc: 0.9997\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 415ms/step - loss: 0.0081 - tp: 121266.0000 - fp: 2217.0000 - tn: 245065.0000 - fn: 2375.0000 - accuracy: 0.6673 - precision: 0.9820 - recall: 0.9808 - auc: 0.9993 - val_loss: 0.0279 - val_tp: 100486.0000 - val_fp: 948.0000 - val_tn: 202106.0000 - val_fn: 1041.0000 - val_accuracy: 0.9935 - val_precision: 0.9907 - val_recall: 0.9897 - val_auc: 0.9998\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 0.0055 - tp: 122181.0000 - fp: 1374.0000 - tn: 245908.0000 - fn: 1460.0000 - accuracy: 0.6698 - precision: 0.9889 - recall: 0.9882 - auc: 0.9997 - val_loss: 0.0233 - val_tp: 100576.0000 - val_fp: 851.0000 - val_tn: 202203.0000 - val_fn: 951.0000 - val_accuracy: 0.9941 - val_precision: 0.9916 - val_recall: 0.9906 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0041 - tp: 122641.0000 - fp: 968.0000 - tn: 246314.0000 - fn: 1000.0000 - accuracy: 0.6709 - precision: 0.9922 - recall: 0.9919 - auc: 0.9998 - val_loss: 0.0239 - val_tp: 100506.0000 - val_fp: 925.0000 - val_tn: 202129.0000 - val_fn: 1021.0000 - val_accuracy: 0.9936 - val_precision: 0.9909 - val_recall: 0.9899 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 0.0033 - tp: 122845.0000 - fp: 771.0000 - tn: 246511.0000 - fn: 796.0000 - accuracy: 0.6715 - precision: 0.9938 - recall: 0.9936 - auc: 0.9998 - val_loss: 0.0233 - val_tp: 100536.0000 - val_fp: 915.0000 - val_tn: 202139.0000 - val_fn: 991.0000 - val_accuracy: 0.9937 - val_precision: 0.9910 - val_recall: 0.9902 - val_auc: 0.9997\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 415ms/step - loss: 0.0026 - tp: 122984.0000 - fp: 648.0000 - tn: 246634.0000 - fn: 657.0000 - accuracy: 0.6719 - precision: 0.9948 - recall: 0.9947 - auc: 0.9998 - val_loss: 0.0233 - val_tp: 100556.0000 - val_fp: 897.0000 - val_tn: 202157.0000 - val_fn: 971.0000 - val_accuracy: 0.9939 - val_precision: 0.9912 - val_recall: 0.9904 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 413ms/step - loss: 0.0022 - tp: 123074.0000 - fp: 558.0000 - tn: 246724.0000 - fn: 567.0000 - accuracy: 0.6721 - precision: 0.9955 - recall: 0.9954 - auc: 0.9998 - val_loss: 0.0221 - val_tp: 100667.0000 - val_fp: 818.0000 - val_tn: 202236.0000 - val_fn: 860.0000 - val_accuracy: 0.9945 - val_precision: 0.9919 - val_recall: 0.9915 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 0.0019 - tp: 123183.0000 - fp: 455.0000 - tn: 246827.0000 - fn: 458.0000 - accuracy: 0.6724 - precision: 0.9963 - recall: 0.9963 - auc: 0.9998 - val_loss: 0.0247 - val_tp: 100542.0000 - val_fp: 921.0000 - val_tn: 202133.0000 - val_fn: 985.0000 - val_accuracy: 0.9937 - val_precision: 0.9909 - val_recall: 0.9903 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 0.0017 - tp: 123223.0000 - fp: 414.0000 - tn: 246868.0000 - fn: 418.0000 - accuracy: 0.6725 - precision: 0.9967 - recall: 0.9966 - auc: 0.9998 - val_loss: 0.0240 - val_tp: 100618.0000 - val_fp: 864.0000 - val_tn: 202190.0000 - val_fn: 909.0000 - val_accuracy: 0.9942 - val_precision: 0.9915 - val_recall: 0.9910 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 0.0014 - tp: 123281.0000 - fp: 357.0000 - tn: 246925.0000 - fn: 360.0000 - accuracy: 0.6727 - precision: 0.9971 - recall: 0.9971 - auc: 0.9999 - val_loss: 0.0235 - val_tp: 100669.0000 - val_fp: 819.0000 - val_tn: 202235.0000 - val_fn: 858.0000 - val_accuracy: 0.9945 - val_precision: 0.9919 - val_recall: 0.9915 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 0.0013 - tp: 123326.0000 - fp: 312.0000 - tn: 246970.0000 - fn: 315.0000 - accuracy: 0.6729 - precision: 0.9975 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0227 - val_tp: 100734.0000 - val_fp: 764.0000 - val_tn: 202290.0000 - val_fn: 793.0000 - val_accuracy: 0.9949 - val_precision: 0.9925 - val_recall: 0.9922 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - 16s 417ms/step - loss: 0.0011 - tp: 123398.0000 - fp: 242.0000 - tn: 247040.0000 - fn: 243.0000 - accuracy: 0.6731 - precision: 0.9980 - recall: 0.9980 - auc: 0.9999 - val_loss: 0.0239 - val_tp: 100718.0000 - val_fp: 779.0000 - val_tn: 202275.0000 - val_fn: 809.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9920 - val_auc: 0.9993\n",
            "Epoch 16/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 8.9111e-04 - tp: 123424.0000 - fp: 214.0000 - tn: 247068.0000 - fn: 217.0000 - accuracy: 0.6731 - precision: 0.9983 - recall: 0.9982 - auc: 0.9999Restoring model weights from the end of the best epoch: 6.\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 8.9111e-04 - tp: 123424.0000 - fp: 214.0000 - tn: 247068.0000 - fn: 217.0000 - accuracy: 0.6731 - precision: 0.9983 - recall: 0.9982 - auc: 0.9999 - val_loss: 0.0240 - val_tp: 100739.0000 - val_fp: 764.0000 - val_tn: 202290.0000 - val_fn: 788.0000 - val_accuracy: 0.9949 - val_precision: 0.9925 - val_recall: 0.9922 - val_auc: 0.9992\n",
            "Epoch 00016: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downsampled_downweight_0.1.h5\n",
            "\n",
            "Down-weighting: 0.2\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 26s 468ms/step - loss: 0.0877 - tp: 204480.0000 - fp: 10785.0000 - tn: 439551.0000 - fn: 20688.0000 - accuracy: 0.7998 - precision: 0.9499 - recall: 0.9081 - auc: 0.9922 - val_loss: 0.1053 - val_tp: 95879.0000 - val_fp: 448.0000 - val_tn: 202606.0000 - val_fn: 5648.0000 - val_accuracy: 0.9800 - val_precision: 0.9953 - val_recall: 0.9444 - val_auc: 0.9993\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 415ms/step - loss: 0.0381 - tp: 116326.0000 - fp: 3537.0000 - tn: 243745.0000 - fn: 7315.0000 - accuracy: 0.6584 - precision: 0.9705 - recall: 0.9408 - auc: 0.9974 - val_loss: 0.0742 - val_tp: 99361.0000 - val_fp: 1288.0000 - val_tn: 201766.0000 - val_fn: 2166.0000 - val_accuracy: 0.9887 - val_precision: 0.9872 - val_recall: 0.9787 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 415ms/step - loss: 0.0286 - tp: 119071.0000 - fp: 3700.0000 - tn: 243582.0000 - fn: 4570.0000 - accuracy: 0.6606 - precision: 0.9699 - recall: 0.9630 - auc: 0.9984 - val_loss: 0.0412 - val_tp: 100315.0000 - val_fp: 880.0000 - val_tn: 202174.0000 - val_fn: 1212.0000 - val_accuracy: 0.9931 - val_precision: 0.9913 - val_recall: 0.9881 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 416ms/step - loss: 0.0183 - tp: 120988.0000 - fp: 2313.0000 - tn: 244969.0000 - fn: 2653.0000 - accuracy: 0.6662 - precision: 0.9812 - recall: 0.9785 - auc: 0.9993 - val_loss: 0.0265 - val_tp: 100587.0000 - val_fp: 780.0000 - val_tn: 202274.0000 - val_fn: 940.0000 - val_accuracy: 0.9944 - val_precision: 0.9923 - val_recall: 0.9907 - val_auc: 0.9999\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 419ms/step - loss: 0.0115 - tp: 122123.0000 - fp: 1392.0000 - tn: 245890.0000 - fn: 1518.0000 - accuracy: 0.6693 - precision: 0.9887 - recall: 0.9877 - auc: 0.9997 - val_loss: 0.0220 - val_tp: 100684.0000 - val_fp: 744.0000 - val_tn: 202310.0000 - val_fn: 843.0000 - val_accuracy: 0.9948 - val_precision: 0.9927 - val_recall: 0.9917 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 419ms/step - loss: 0.0078 - tp: 122740.0000 - fp: 860.0000 - tn: 246422.0000 - fn: 901.0000 - accuracy: 0.6710 - precision: 0.9930 - recall: 0.9927 - auc: 0.9998 - val_loss: 0.0201 - val_tp: 100733.0000 - val_fp: 733.0000 - val_tn: 202321.0000 - val_fn: 794.0000 - val_accuracy: 0.9950 - val_precision: 0.9928 - val_recall: 0.9922 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 417ms/step - loss: 0.0060 - tp: 122979.0000 - fp: 641.0000 - tn: 246641.0000 - fn: 662.0000 - accuracy: 0.6717 - precision: 0.9948 - recall: 0.9946 - auc: 0.9999 - val_loss: 0.0203 - val_tp: 100692.0000 - val_fp: 774.0000 - val_tn: 202280.0000 - val_fn: 835.0000 - val_accuracy: 0.9947 - val_precision: 0.9924 - val_recall: 0.9918 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0049 - tp: 123070.0000 - fp: 554.0000 - tn: 246728.0000 - fn: 571.0000 - accuracy: 0.6720 - precision: 0.9955 - recall: 0.9954 - auc: 0.9999 - val_loss: 0.0204 - val_tp: 100680.0000 - val_fp: 783.0000 - val_tn: 202271.0000 - val_fn: 847.0000 - val_accuracy: 0.9946 - val_precision: 0.9923 - val_recall: 0.9917 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0041 - tp: 123168.0000 - fp: 468.0000 - tn: 246814.0000 - fn: 473.0000 - accuracy: 0.6722 - precision: 0.9962 - recall: 0.9962 - auc: 0.9999 - val_loss: 0.0209 - val_tp: 100665.0000 - val_fp: 800.0000 - val_tn: 202254.0000 - val_fn: 862.0000 - val_accuracy: 0.9945 - val_precision: 0.9921 - val_recall: 0.9915 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 419ms/step - loss: 0.0036 - tp: 123183.0000 - fp: 451.0000 - tn: 246831.0000 - fn: 458.0000 - accuracy: 0.6723 - precision: 0.9964 - recall: 0.9963 - auc: 0.9999 - val_loss: 0.0212 - val_tp: 100697.0000 - val_fp: 798.0000 - val_tn: 202256.0000 - val_fn: 830.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9918 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0032 - tp: 123247.0000 - fp: 388.0000 - tn: 246894.0000 - fn: 394.0000 - accuracy: 0.6725 - precision: 0.9969 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0220 - val_tp: 100669.0000 - val_fp: 821.0000 - val_tn: 202233.0000 - val_fn: 858.0000 - val_accuracy: 0.9945 - val_precision: 0.9919 - val_recall: 0.9915 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 417ms/step - loss: 0.0029 - tp: 123293.0000 - fp: 344.0000 - tn: 246938.0000 - fn: 348.0000 - accuracy: 0.6727 - precision: 0.9972 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0218 - val_tp: 100707.0000 - val_fp: 789.0000 - val_tn: 202265.0000 - val_fn: 820.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9995\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 418ms/step - loss: 0.0026 - tp: 123308.0000 - fp: 327.0000 - tn: 246955.0000 - fn: 333.0000 - accuracy: 0.6727 - precision: 0.9974 - recall: 0.9973 - auc: 0.9999 - val_loss: 0.0223 - val_tp: 100693.0000 - val_fp: 800.0000 - val_tn: 202254.0000 - val_fn: 834.0000 - val_accuracy: 0.9946 - val_precision: 0.9921 - val_recall: 0.9918 - val_auc: 0.9995\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 417ms/step - loss: 0.0023 - tp: 123354.0000 - fp: 282.0000 - tn: 247000.0000 - fn: 287.0000 - accuracy: 0.6729 - precision: 0.9977 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0224 - val_tp: 100715.0000 - val_fp: 788.0000 - val_tn: 202266.0000 - val_fn: 812.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9920 - val_auc: 0.9994\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0020 - tp: 123392.0000 - fp: 243.0000 - tn: 247039.0000 - fn: 249.0000 - accuracy: 0.6730 - precision: 0.9980 - recall: 0.9980 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 417ms/step - loss: 0.0020 - tp: 123392.0000 - fp: 243.0000 - tn: 247039.0000 - fn: 249.0000 - accuracy: 0.6730 - precision: 0.9980 - recall: 0.9980 - auc: 0.9999 - val_loss: 0.0234 - val_tp: 100688.0000 - val_fp: 820.0000 - val_tn: 202234.0000 - val_fn: 839.0000 - val_accuracy: 0.9946 - val_precision: 0.9919 - val_recall: 0.9917 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downsampled_downweight_0.2.h5\n",
            "\n",
            "Down-weighting: 0.3\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 471ms/step - loss: 0.1171 - tp: 205045.0000 - fp: 11268.0000 - tn: 439068.0000 - fn: 20123.0000 - accuracy: 0.7990 - precision: 0.9479 - recall: 0.9106 - auc: 0.9923 - val_loss: 0.0976 - val_tp: 97361.0000 - val_fp: 622.0000 - val_tn: 202432.0000 - val_fn: 4166.0000 - val_accuracy: 0.9843 - val_precision: 0.9937 - val_recall: 0.9590 - val_auc: 0.9992\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 419ms/step - loss: 0.0474 - tp: 118939.0000 - fp: 2598.0000 - tn: 244684.0000 - fn: 4702.0000 - accuracy: 0.6599 - precision: 0.9786 - recall: 0.9620 - auc: 0.9982 - val_loss: 0.0594 - val_tp: 100274.0000 - val_fp: 878.0000 - val_tn: 202176.0000 - val_fn: 1253.0000 - val_accuracy: 0.9930 - val_precision: 0.9913 - val_recall: 0.9877 - val_auc: 0.9997\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0360 - tp: 120256.0000 - fp: 2818.0000 - tn: 244464.0000 - fn: 3385.0000 - accuracy: 0.6610 - precision: 0.9771 - recall: 0.9726 - auc: 0.9989 - val_loss: 0.0385 - val_tp: 100407.0000 - val_fp: 891.0000 - val_tn: 202163.0000 - val_fn: 1120.0000 - val_accuracy: 0.9934 - val_precision: 0.9912 - val_recall: 0.9890 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0241 - tp: 121453.0000 - fp: 1894.0000 - tn: 245388.0000 - fn: 2188.0000 - accuracy: 0.6664 - precision: 0.9846 - recall: 0.9823 - auc: 0.9995 - val_loss: 0.0252 - val_tp: 100616.0000 - val_fp: 774.0000 - val_tn: 202280.0000 - val_fn: 911.0000 - val_accuracy: 0.9945 - val_precision: 0.9924 - val_recall: 0.9910 - val_auc: 0.9999\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 420ms/step - loss: 0.0148 - tp: 122339.0000 - fp: 1187.0000 - tn: 246095.0000 - fn: 1302.0000 - accuracy: 0.6696 - precision: 0.9904 - recall: 0.9895 - auc: 0.9997 - val_loss: 0.0205 - val_tp: 100747.0000 - val_fp: 689.0000 - val_tn: 202365.0000 - val_fn: 780.0000 - val_accuracy: 0.9952 - val_precision: 0.9932 - val_recall: 0.9923 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 419ms/step - loss: 0.0098 - tp: 122891.0000 - fp: 707.0000 - tn: 246575.0000 - fn: 750.0000 - accuracy: 0.6713 - precision: 0.9943 - recall: 0.9939 - auc: 0.9998 - val_loss: 0.0191 - val_tp: 100762.0000 - val_fp: 711.0000 - val_tn: 202343.0000 - val_fn: 765.0000 - val_accuracy: 0.9952 - val_precision: 0.9930 - val_recall: 0.9925 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0075 - tp: 123064.0000 - fp: 551.0000 - tn: 246731.0000 - fn: 577.0000 - accuracy: 0.6718 - precision: 0.9955 - recall: 0.9953 - auc: 0.9999 - val_loss: 0.0193 - val_tp: 100751.0000 - val_fp: 726.0000 - val_tn: 202328.0000 - val_fn: 776.0000 - val_accuracy: 0.9951 - val_precision: 0.9928 - val_recall: 0.9924 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0062 - tp: 123154.0000 - fp: 471.0000 - tn: 246811.0000 - fn: 487.0000 - accuracy: 0.6721 - precision: 0.9962 - recall: 0.9961 - auc: 0.9999 - val_loss: 0.0195 - val_tp: 100736.0000 - val_fp: 741.0000 - val_tn: 202313.0000 - val_fn: 791.0000 - val_accuracy: 0.9950 - val_precision: 0.9927 - val_recall: 0.9922 - val_auc: 0.9997\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 419ms/step - loss: 0.0052 - tp: 123223.0000 - fp: 410.0000 - tn: 246872.0000 - fn: 418.0000 - accuracy: 0.6723 - precision: 0.9967 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0199 - val_tp: 100728.0000 - val_fp: 755.0000 - val_tn: 202299.0000 - val_fn: 799.0000 - val_accuracy: 0.9949 - val_precision: 0.9926 - val_recall: 0.9921 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 420ms/step - loss: 0.0046 - tp: 123248.0000 - fp: 391.0000 - tn: 246891.0000 - fn: 393.0000 - accuracy: 0.6724 - precision: 0.9968 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0205 - val_tp: 100718.0000 - val_fp: 777.0000 - val_tn: 202277.0000 - val_fn: 809.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9920 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0042 - tp: 123292.0000 - fp: 342.0000 - tn: 246940.0000 - fn: 349.0000 - accuracy: 0.6726 - precision: 0.9972 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0209 - val_tp: 100701.0000 - val_fp: 799.0000 - val_tn: 202255.0000 - val_fn: 826.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9919 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0037 - tp: 123324.0000 - fp: 314.0000 - tn: 246968.0000 - fn: 317.0000 - accuracy: 0.6727 - precision: 0.9975 - recall: 0.9974 - auc: 0.9999 - val_loss: 0.0213 - val_tp: 100714.0000 - val_fp: 783.0000 - val_tn: 202271.0000 - val_fn: 813.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9920 - val_auc: 0.9995\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 420ms/step - loss: 0.0034 - tp: 123333.0000 - fp: 304.0000 - tn: 246978.0000 - fn: 308.0000 - accuracy: 0.6727 - precision: 0.9975 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0217 - val_tp: 100705.0000 - val_fp: 784.0000 - val_tn: 202270.0000 - val_fn: 822.0000 - val_accuracy: 0.9947 - val_precision: 0.9923 - val_recall: 0.9919 - val_auc: 0.9995\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0031 - tp: 123371.0000 - fp: 267.0000 - tn: 247015.0000 - fn: 270.0000 - accuracy: 0.6728 - precision: 0.9978 - recall: 0.9978 - auc: 0.9999 - val_loss: 0.0220 - val_tp: 100723.0000 - val_fp: 779.0000 - val_tn: 202275.0000 - val_fn: 804.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0028 - tp: 123401.0000 - fp: 238.0000 - tn: 247044.0000 - fn: 240.0000 - accuracy: 0.6729 - precision: 0.9981 - recall: 0.9981 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 422ms/step - loss: 0.0028 - tp: 123401.0000 - fp: 238.0000 - tn: 247044.0000 - fn: 240.0000 - accuracy: 0.6729 - precision: 0.9981 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0226 - val_tp: 100714.0000 - val_fp: 784.0000 - val_tn: 202270.0000 - val_fn: 813.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9920 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downsampled_downweight_0.3.h5\n",
            "\n",
            "Down-weighting: 0.4\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 476ms/step - loss: 0.1459 - tp: 205374.0000 - fp: 11537.0000 - tn: 438799.0000 - fn: 19794.0000 - accuracy: 0.7987 - precision: 0.9468 - recall: 0.9121 - auc: 0.9923 - val_loss: 0.0939 - val_tp: 97905.0000 - val_fp: 712.0000 - val_tn: 202342.0000 - val_fn: 3622.0000 - val_accuracy: 0.9858 - val_precision: 0.9928 - val_recall: 0.9643 - val_auc: 0.9991\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0552 - tp: 119638.0000 - fp: 2598.0000 - tn: 244684.0000 - fn: 4003.0000 - accuracy: 0.6597 - precision: 0.9787 - recall: 0.9676 - auc: 0.9981 - val_loss: 0.0535 - val_tp: 100349.0000 - val_fp: 920.0000 - val_tn: 202134.0000 - val_fn: 1178.0000 - val_accuracy: 0.9931 - val_precision: 0.9909 - val_recall: 0.9884 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 424ms/step - loss: 0.0420 - tp: 120655.0000 - fp: 2572.0000 - tn: 244710.0000 - fn: 2986.0000 - accuracy: 0.6606 - precision: 0.9791 - recall: 0.9758 - auc: 0.9990 - val_loss: 0.0372 - val_tp: 100466.0000 - val_fp: 912.0000 - val_tn: 202142.0000 - val_fn: 1061.0000 - val_accuracy: 0.9935 - val_precision: 0.9910 - val_recall: 0.9895 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 424ms/step - loss: 0.0294 - tp: 121538.0000 - fp: 1847.0000 - tn: 245435.0000 - fn: 2103.0000 - accuracy: 0.6654 - precision: 0.9850 - recall: 0.9830 - auc: 0.9995 - val_loss: 0.0252 - val_tp: 100586.0000 - val_fp: 829.0000 - val_tn: 202225.0000 - val_fn: 941.0000 - val_accuracy: 0.9942 - val_precision: 0.9918 - val_recall: 0.9907 - val_auc: 0.9999\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0182 - tp: 122409.0000 - fp: 1126.0000 - tn: 246156.0000 - fn: 1232.0000 - accuracy: 0.6694 - precision: 0.9909 - recall: 0.9900 - auc: 0.9998 - val_loss: 0.0201 - val_tp: 100748.0000 - val_fp: 698.0000 - val_tn: 202356.0000 - val_fn: 779.0000 - val_accuracy: 0.9952 - val_precision: 0.9931 - val_recall: 0.9923 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0118 - tp: 122932.0000 - fp: 660.0000 - tn: 246622.0000 - fn: 709.0000 - accuracy: 0.6713 - precision: 0.9947 - recall: 0.9943 - auc: 0.9999 - val_loss: 0.0188 - val_tp: 100768.0000 - val_fp: 721.0000 - val_tn: 202333.0000 - val_fn: 759.0000 - val_accuracy: 0.9951 - val_precision: 0.9929 - val_recall: 0.9925 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 422ms/step - loss: 0.0090 - tp: 123105.0000 - fp: 501.0000 - tn: 246781.0000 - fn: 536.0000 - accuracy: 0.6718 - precision: 0.9959 - recall: 0.9957 - auc: 0.9999 - val_loss: 0.0189 - val_tp: 100749.0000 - val_fp: 727.0000 - val_tn: 202327.0000 - val_fn: 778.0000 - val_accuracy: 0.9951 - val_precision: 0.9928 - val_recall: 0.9923 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0073 - tp: 123176.0000 - fp: 451.0000 - tn: 246831.0000 - fn: 465.0000 - accuracy: 0.6721 - precision: 0.9964 - recall: 0.9962 - auc: 0.9999 - val_loss: 0.0192 - val_tp: 100727.0000 - val_fp: 747.0000 - val_tn: 202307.0000 - val_fn: 800.0000 - val_accuracy: 0.9949 - val_precision: 0.9926 - val_recall: 0.9921 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0062 - tp: 123263.0000 - fp: 370.0000 - tn: 246912.0000 - fn: 378.0000 - accuracy: 0.6724 - precision: 0.9970 - recall: 0.9969 - auc: 0.9999 - val_loss: 0.0195 - val_tp: 100747.0000 - val_fp: 733.0000 - val_tn: 202321.0000 - val_fn: 780.0000 - val_accuracy: 0.9950 - val_precision: 0.9928 - val_recall: 0.9923 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0055 - tp: 123276.0000 - fp: 356.0000 - tn: 246926.0000 - fn: 365.0000 - accuracy: 0.6724 - precision: 0.9971 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0202 - val_tp: 100725.0000 - val_fp: 777.0000 - val_tn: 202277.0000 - val_fn: 802.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 16s 424ms/step - loss: 0.0050 - tp: 123309.0000 - fp: 320.0000 - tn: 246962.0000 - fn: 332.0000 - accuracy: 0.6726 - precision: 0.9974 - recall: 0.9973 - auc: 0.9999 - val_loss: 0.0206 - val_tp: 100732.0000 - val_fp: 768.0000 - val_tn: 202286.0000 - val_fn: 795.0000 - val_accuracy: 0.9949 - val_precision: 0.9924 - val_recall: 0.9922 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0044 - tp: 123348.0000 - fp: 287.0000 - tn: 246995.0000 - fn: 293.0000 - accuracy: 0.6727 - precision: 0.9977 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0212 - val_tp: 100717.0000 - val_fp: 785.0000 - val_tn: 202269.0000 - val_fn: 810.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9920 - val_auc: 0.9995\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0042 - tp: 123341.0000 - fp: 295.0000 - tn: 246987.0000 - fn: 300.0000 - accuracy: 0.6727 - precision: 0.9976 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0214 - val_tp: 100705.0000 - val_fp: 787.0000 - val_tn: 202267.0000 - val_fn: 822.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 422ms/step - loss: 0.0038 - tp: 123394.0000 - fp: 238.0000 - tn: 247044.0000 - fn: 247.0000 - accuracy: 0.6729 - precision: 0.9981 - recall: 0.9980 - auc: 0.9999 - val_loss: 0.0218 - val_tp: 100723.0000 - val_fp: 784.0000 - val_tn: 202270.0000 - val_fn: 804.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0034 - tp: 123416.0000 - fp: 223.0000 - tn: 247059.0000 - fn: 225.0000 - accuracy: 0.6729 - precision: 0.9982 - recall: 0.9982 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 420ms/step - loss: 0.0034 - tp: 123416.0000 - fp: 223.0000 - tn: 247059.0000 - fn: 225.0000 - accuracy: 0.6729 - precision: 0.9982 - recall: 0.9982 - auc: 0.9999 - val_loss: 0.0222 - val_tp: 100739.0000 - val_fp: 772.0000 - val_tn: 202282.0000 - val_fn: 788.0000 - val_accuracy: 0.9949 - val_precision: 0.9924 - val_recall: 0.9922 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downsampled_downweight_0.4.h5\n",
            "\n",
            "Down-weighting: 0.5\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 485ms/step - loss: 0.1745 - tp: 205602.0000 - fp: 11697.0000 - tn: 438639.0000 - fn: 19566.0000 - accuracy: 0.7985 - precision: 0.9462 - recall: 0.9131 - auc: 0.9923 - val_loss: 0.0917 - val_tp: 98213.0000 - val_fp: 774.0000 - val_tn: 202280.0000 - val_fn: 3314.0000 - val_accuracy: 0.9866 - val_precision: 0.9922 - val_recall: 0.9674 - val_auc: 0.9990\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0621 - tp: 119916.0000 - fp: 2650.0000 - tn: 244632.0000 - fn: 3725.0000 - accuracy: 0.6595 - precision: 0.9784 - recall: 0.9699 - auc: 0.9980 - val_loss: 0.0500 - val_tp: 100389.0000 - val_fp: 931.0000 - val_tn: 202123.0000 - val_fn: 1138.0000 - val_accuracy: 0.9932 - val_precision: 0.9908 - val_recall: 0.9888 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0472 - tp: 120772.0000 - fp: 2585.0000 - tn: 244697.0000 - fn: 2869.0000 - accuracy: 0.6600 - precision: 0.9790 - recall: 0.9768 - auc: 0.9989 - val_loss: 0.0364 - val_tp: 100495.0000 - val_fp: 928.0000 - val_tn: 202126.0000 - val_fn: 1032.0000 - val_accuracy: 0.9936 - val_precision: 0.9909 - val_recall: 0.9898 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0343 - tp: 121485.0000 - fp: 1952.0000 - tn: 245330.0000 - fn: 2156.0000 - accuracy: 0.6640 - precision: 0.9842 - recall: 0.9826 - auc: 0.9995 - val_loss: 0.0257 - val_tp: 100533.0000 - val_fp: 901.0000 - val_tn: 202153.0000 - val_fn: 994.0000 - val_accuracy: 0.9938 - val_precision: 0.9911 - val_recall: 0.9902 - val_auc: 0.9999\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0217 - tp: 122403.0000 - fp: 1130.0000 - tn: 246152.0000 - fn: 1238.0000 - accuracy: 0.6689 - precision: 0.9909 - recall: 0.9900 - auc: 0.9998 - val_loss: 0.0203 - val_tp: 100706.0000 - val_fp: 732.0000 - val_tn: 202322.0000 - val_fn: 821.0000 - val_accuracy: 0.9949 - val_precision: 0.9928 - val_recall: 0.9919 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0138 - tp: 122936.0000 - fp: 655.0000 - tn: 246627.0000 - fn: 705.0000 - accuracy: 0.6711 - precision: 0.9947 - recall: 0.9943 - auc: 0.9998 - val_loss: 0.0189 - val_tp: 100754.0000 - val_fp: 726.0000 - val_tn: 202328.0000 - val_fn: 773.0000 - val_accuracy: 0.9951 - val_precision: 0.9928 - val_recall: 0.9924 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0103 - tp: 123130.0000 - fp: 476.0000 - tn: 246806.0000 - fn: 511.0000 - accuracy: 0.6718 - precision: 0.9961 - recall: 0.9959 - auc: 0.9999 - val_loss: 0.0189 - val_tp: 100738.0000 - val_fp: 735.0000 - val_tn: 202319.0000 - val_fn: 789.0000 - val_accuracy: 0.9950 - val_precision: 0.9928 - val_recall: 0.9922 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 16s 421ms/step - loss: 0.0084 - tp: 123184.0000 - fp: 431.0000 - tn: 246851.0000 - fn: 457.0000 - accuracy: 0.6720 - precision: 0.9965 - recall: 0.9963 - auc: 0.9999 - val_loss: 0.0193 - val_tp: 100727.0000 - val_fp: 759.0000 - val_tn: 202295.0000 - val_fn: 800.0000 - val_accuracy: 0.9949 - val_precision: 0.9925 - val_recall: 0.9921 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0070 - tp: 123273.0000 - fp: 357.0000 - tn: 246925.0000 - fn: 368.0000 - accuracy: 0.6723 - precision: 0.9971 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0194 - val_tp: 100740.0000 - val_fp: 750.0000 - val_tn: 202304.0000 - val_fn: 787.0000 - val_accuracy: 0.9950 - val_precision: 0.9926 - val_recall: 0.9922 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 16s 424ms/step - loss: 0.0062 - tp: 123301.0000 - fp: 329.0000 - tn: 246953.0000 - fn: 340.0000 - accuracy: 0.6724 - precision: 0.9973 - recall: 0.9973 - auc: 0.9999 - val_loss: 0.0202 - val_tp: 100718.0000 - val_fp: 784.0000 - val_tn: 202270.0000 - val_fn: 809.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9920 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0058 - tp: 123326.0000 - fp: 307.0000 - tn: 246975.0000 - fn: 315.0000 - accuracy: 0.6725 - precision: 0.9975 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0205 - val_tp: 100721.0000 - val_fp: 778.0000 - val_tn: 202276.0000 - val_fn: 806.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0051 - tp: 123354.0000 - fp: 278.0000 - tn: 247004.0000 - fn: 287.0000 - accuracy: 0.6727 - precision: 0.9978 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0212 - val_tp: 100708.0000 - val_fp: 794.0000 - val_tn: 202260.0000 - val_fn: 819.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0048 - tp: 123344.0000 - fp: 293.0000 - tn: 246989.0000 - fn: 297.0000 - accuracy: 0.6726 - precision: 0.9976 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0214 - val_tp: 100702.0000 - val_fp: 797.0000 - val_tn: 202257.0000 - val_fn: 825.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9919 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 16s 424ms/step - loss: 0.0044 - tp: 123387.0000 - fp: 246.0000 - tn: 247036.0000 - fn: 254.0000 - accuracy: 0.6728 - precision: 0.9980 - recall: 0.9979 - auc: 0.9999 - val_loss: 0.0216 - val_tp: 100718.0000 - val_fp: 785.0000 - val_tn: 202269.0000 - val_fn: 809.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9920 - val_auc: 0.9994\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0040 - tp: 123406.0000 - fp: 232.0000 - tn: 247050.0000 - fn: 235.0000 - accuracy: 0.6728 - precision: 0.9981 - recall: 0.9981 - auc: 1.0000Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 16s 422ms/step - loss: 0.0040 - tp: 123406.0000 - fp: 232.0000 - tn: 247050.0000 - fn: 235.0000 - accuracy: 0.6728 - precision: 0.9981 - recall: 0.9981 - auc: 1.0000 - val_loss: 0.0220 - val_tp: 100730.0000 - val_fp: 781.0000 - val_tn: 202273.0000 - val_fn: 797.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downsampled_downweight_0.5.h5\n",
            "\n",
            "Down-weighting: 0.6\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 479ms/step - loss: 0.2028 - tp: 205707.0000 - fp: 11861.0000 - tn: 438475.0000 - fn: 19461.0000 - accuracy: 0.7982 - precision: 0.9455 - recall: 0.9136 - auc: 0.9922 - val_loss: 0.0900 - val_tp: 98437.0000 - val_fp: 831.0000 - val_tn: 202223.0000 - val_fn: 3090.0000 - val_accuracy: 0.9871 - val_precision: 0.9916 - val_recall: 0.9696 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0683 - tp: 120082.0000 - fp: 2696.0000 - tn: 244586.0000 - fn: 3559.0000 - accuracy: 0.6594 - precision: 0.9780 - recall: 0.9712 - auc: 0.9978 - val_loss: 0.0476 - val_tp: 100423.0000 - val_fp: 938.0000 - val_tn: 202116.0000 - val_fn: 1104.0000 - val_accuracy: 0.9933 - val_precision: 0.9907 - val_recall: 0.9891 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0516 - tp: 120817.0000 - fp: 2597.0000 - tn: 244685.0000 - fn: 2824.0000 - accuracy: 0.6598 - precision: 0.9790 - recall: 0.9772 - auc: 0.9988 - val_loss: 0.0357 - val_tp: 100519.0000 - val_fp: 940.0000 - val_tn: 202114.0000 - val_fn: 1008.0000 - val_accuracy: 0.9936 - val_precision: 0.9907 - val_recall: 0.9901 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0387 - tp: 121345.0000 - fp: 2113.0000 - tn: 245169.0000 - fn: 2296.0000 - accuracy: 0.6626 - precision: 0.9829 - recall: 0.9814 - auc: 0.9994 - val_loss: 0.0262 - val_tp: 100525.0000 - val_fp: 918.0000 - val_tn: 202136.0000 - val_fn: 1002.0000 - val_accuracy: 0.9937 - val_precision: 0.9910 - val_recall: 0.9901 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0252 - tp: 122299.0000 - fp: 1225.0000 - tn: 246057.0000 - fn: 1342.0000 - accuracy: 0.6681 - precision: 0.9901 - recall: 0.9891 - auc: 0.9997 - val_loss: 0.0207 - val_tp: 100650.0000 - val_fp: 810.0000 - val_tn: 202244.0000 - val_fn: 877.0000 - val_accuracy: 0.9945 - val_precision: 0.9920 - val_recall: 0.9914 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 16s 424ms/step - loss: 0.0159 - tp: 122919.0000 - fp: 663.0000 - tn: 246619.0000 - fn: 722.0000 - accuracy: 0.6709 - precision: 0.9946 - recall: 0.9942 - auc: 0.9998 - val_loss: 0.0191 - val_tp: 100723.0000 - val_fp: 766.0000 - val_tn: 202288.0000 - val_fn: 804.0000 - val_accuracy: 0.9948 - val_precision: 0.9925 - val_recall: 0.9921 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0118 - tp: 123125.0000 - fp: 479.0000 - tn: 246803.0000 - fn: 516.0000 - accuracy: 0.6717 - precision: 0.9961 - recall: 0.9958 - auc: 0.9998 - val_loss: 0.0189 - val_tp: 100744.0000 - val_fp: 739.0000 - val_tn: 202315.0000 - val_fn: 783.0000 - val_accuracy: 0.9950 - val_precision: 0.9927 - val_recall: 0.9923 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0094 - tp: 123197.0000 - fp: 415.0000 - tn: 246867.0000 - fn: 444.0000 - accuracy: 0.6720 - precision: 0.9966 - recall: 0.9964 - auc: 0.9999 - val_loss: 0.0195 - val_tp: 100720.0000 - val_fp: 767.0000 - val_tn: 202287.0000 - val_fn: 807.0000 - val_accuracy: 0.9948 - val_precision: 0.9924 - val_recall: 0.9921 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0079 - tp: 123273.0000 - fp: 344.0000 - tn: 246938.0000 - fn: 368.0000 - accuracy: 0.6723 - precision: 0.9972 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0195 - val_tp: 100740.0000 - val_fp: 750.0000 - val_tn: 202304.0000 - val_fn: 787.0000 - val_accuracy: 0.9950 - val_precision: 0.9926 - val_recall: 0.9922 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0070 - tp: 123300.0000 - fp: 324.0000 - tn: 246958.0000 - fn: 341.0000 - accuracy: 0.6724 - precision: 0.9974 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0203 - val_tp: 100710.0000 - val_fp: 797.0000 - val_tn: 202257.0000 - val_fn: 817.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9920 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0064 - tp: 123327.0000 - fp: 307.0000 - tn: 246975.0000 - fn: 314.0000 - accuracy: 0.6725 - precision: 0.9975 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0207 - val_tp: 100710.0000 - val_fp: 791.0000 - val_tn: 202263.0000 - val_fn: 817.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9920 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0057 - tp: 123358.0000 - fp: 274.0000 - tn: 247008.0000 - fn: 283.0000 - accuracy: 0.6726 - precision: 0.9978 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0215 - val_tp: 100691.0000 - val_fp: 806.0000 - val_tn: 202248.0000 - val_fn: 836.0000 - val_accuracy: 0.9946 - val_precision: 0.9921 - val_recall: 0.9918 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0054 - tp: 123349.0000 - fp: 289.0000 - tn: 246993.0000 - fn: 292.0000 - accuracy: 0.6726 - precision: 0.9977 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0215 - val_tp: 100701.0000 - val_fp: 802.0000 - val_tn: 202252.0000 - val_fn: 826.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9919 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0049 - tp: 123388.0000 - fp: 246.0000 - tn: 247036.0000 - fn: 253.0000 - accuracy: 0.6728 - precision: 0.9980 - recall: 0.9980 - auc: 0.9999 - val_loss: 0.0219 - val_tp: 100721.0000 - val_fp: 780.0000 - val_tn: 202274.0000 - val_fn: 806.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0045 - tp: 123403.0000 - fp: 233.0000 - tn: 247049.0000 - fn: 238.0000 - accuracy: 0.6728 - precision: 0.9981 - recall: 0.9981 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0045 - tp: 123403.0000 - fp: 233.0000 - tn: 247049.0000 - fn: 238.0000 - accuracy: 0.6728 - precision: 0.9981 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0221 - val_tp: 100723.0000 - val_fp: 785.0000 - val_tn: 202269.0000 - val_fn: 804.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downsampled_downweight_0.6.h5\n",
            "\n",
            "Down-weighting: 0.7\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 496ms/step - loss: 0.2311 - tp: 205830.0000 - fp: 11993.0000 - tn: 438343.0000 - fn: 19338.0000 - accuracy: 0.7980 - precision: 0.9449 - recall: 0.9141 - auc: 0.9922 - val_loss: 0.0887 - val_tp: 98591.0000 - val_fp: 890.0000 - val_tn: 202164.0000 - val_fn: 2936.0000 - val_accuracy: 0.9874 - val_precision: 0.9911 - val_recall: 0.9711 - val_auc: 0.9989\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 424ms/step - loss: 0.0741 - tp: 120172.0000 - fp: 2742.0000 - tn: 244540.0000 - fn: 3469.0000 - accuracy: 0.6593 - precision: 0.9777 - recall: 0.9719 - auc: 0.9977 - val_loss: 0.0458 - val_tp: 100440.0000 - val_fp: 945.0000 - val_tn: 202109.0000 - val_fn: 1087.0000 - val_accuracy: 0.9933 - val_precision: 0.9907 - val_recall: 0.9893 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0557 - tp: 120850.0000 - fp: 2613.0000 - tn: 244669.0000 - fn: 2791.0000 - accuracy: 0.6597 - precision: 0.9788 - recall: 0.9774 - auc: 0.9987 - val_loss: 0.0352 - val_tp: 100529.0000 - val_fp: 944.0000 - val_tn: 202110.0000 - val_fn: 998.0000 - val_accuracy: 0.9936 - val_precision: 0.9907 - val_recall: 0.9902 - val_auc: 0.9998\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0427 - tp: 121218.0000 - fp: 2269.0000 - tn: 245013.0000 - fn: 2423.0000 - accuracy: 0.6616 - precision: 0.9816 - recall: 0.9804 - auc: 0.9994 - val_loss: 0.0265 - val_tp: 100542.0000 - val_fp: 925.0000 - val_tn: 202129.0000 - val_fn: 985.0000 - val_accuracy: 0.9937 - val_precision: 0.9909 - val_recall: 0.9903 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 17s 424ms/step - loss: 0.0285 - tp: 122166.0000 - fp: 1357.0000 - tn: 245925.0000 - fn: 1475.0000 - accuracy: 0.6671 - precision: 0.9890 - recall: 0.9881 - auc: 0.9997 - val_loss: 0.0210 - val_tp: 100617.0000 - val_fp: 844.0000 - val_tn: 202210.0000 - val_fn: 910.0000 - val_accuracy: 0.9942 - val_precision: 0.9917 - val_recall: 0.9910 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0181 - tp: 122870.0000 - fp: 704.0000 - tn: 246578.0000 - fn: 771.0000 - accuracy: 0.6706 - precision: 0.9943 - recall: 0.9938 - auc: 0.9998 - val_loss: 0.0193 - val_tp: 100705.0000 - val_fp: 788.0000 - val_tn: 202266.0000 - val_fn: 822.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0133 - tp: 123106.0000 - fp: 495.0000 - tn: 246787.0000 - fn: 535.0000 - accuracy: 0.6716 - precision: 0.9960 - recall: 0.9957 - auc: 0.9998 - val_loss: 0.0190 - val_tp: 100729.0000 - val_fp: 760.0000 - val_tn: 202294.0000 - val_fn: 798.0000 - val_accuracy: 0.9949 - val_precision: 0.9925 - val_recall: 0.9921 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0104 - tp: 123193.0000 - fp: 409.0000 - tn: 246873.0000 - fn: 448.0000 - accuracy: 0.6720 - precision: 0.9967 - recall: 0.9964 - auc: 0.9999 - val_loss: 0.0195 - val_tp: 100713.0000 - val_fp: 776.0000 - val_tn: 202278.0000 - val_fn: 814.0000 - val_accuracy: 0.9948 - val_precision: 0.9924 - val_recall: 0.9920 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0087 - tp: 123266.0000 - fp: 348.0000 - tn: 246934.0000 - fn: 375.0000 - accuracy: 0.6722 - precision: 0.9972 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0196 - val_tp: 100728.0000 - val_fp: 762.0000 - val_tn: 202292.0000 - val_fn: 799.0000 - val_accuracy: 0.9949 - val_precision: 0.9925 - val_recall: 0.9921 - val_auc: 0.9996\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0077 - tp: 123290.0000 - fp: 331.0000 - tn: 246951.0000 - fn: 351.0000 - accuracy: 0.6723 - precision: 0.9973 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0203 - val_tp: 100713.0000 - val_fp: 790.0000 - val_tn: 202264.0000 - val_fn: 814.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9920 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 424ms/step - loss: 0.0070 - tp: 123329.0000 - fp: 305.0000 - tn: 246977.0000 - fn: 312.0000 - accuracy: 0.6725 - precision: 0.9975 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0206 - val_tp: 100721.0000 - val_fp: 779.0000 - val_tn: 202275.0000 - val_fn: 806.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0062 - tp: 123361.0000 - fp: 272.0000 - tn: 247010.0000 - fn: 280.0000 - accuracy: 0.6726 - precision: 0.9978 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0214 - val_tp: 100707.0000 - val_fp: 785.0000 - val_tn: 202269.0000 - val_fn: 820.0000 - val_accuracy: 0.9947 - val_precision: 0.9923 - val_recall: 0.9919 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0059 - tp: 123359.0000 - fp: 278.0000 - tn: 247004.0000 - fn: 282.0000 - accuracy: 0.6726 - precision: 0.9978 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0214 - val_tp: 100726.0000 - val_fp: 772.0000 - val_tn: 202282.0000 - val_fn: 801.0000 - val_accuracy: 0.9948 - val_precision: 0.9924 - val_recall: 0.9921 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0053 - tp: 123394.0000 - fp: 239.0000 - tn: 247043.0000 - fn: 247.0000 - accuracy: 0.6728 - precision: 0.9981 - recall: 0.9980 - auc: 0.9999 - val_loss: 0.0219 - val_tp: 100729.0000 - val_fp: 778.0000 - val_tn: 202276.0000 - val_fn: 798.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0048 - tp: 123419.0000 - fp: 218.0000 - tn: 247064.0000 - fn: 222.0000 - accuracy: 0.6728 - precision: 0.9982 - recall: 0.9982 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0048 - tp: 123419.0000 - fp: 218.0000 - tn: 247064.0000 - fn: 222.0000 - accuracy: 0.6728 - precision: 0.9982 - recall: 0.9982 - auc: 0.9999 - val_loss: 0.0220 - val_tp: 100727.0000 - val_fp: 784.0000 - val_tn: 202270.0000 - val_fn: 800.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9993\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downsampled_downweight_0.7.h5\n",
            "\n",
            "Down-weighting: 0.8\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 479ms/step - loss: 0.2592 - tp: 205927.0000 - fp: 12084.0000 - tn: 438252.0000 - fn: 19241.0000 - accuracy: 0.7979 - precision: 0.9446 - recall: 0.9145 - auc: 0.9922 - val_loss: 0.0877 - val_tp: 98722.0000 - val_fp: 941.0000 - val_tn: 202113.0000 - val_fn: 2805.0000 - val_accuracy: 0.9877 - val_precision: 0.9906 - val_recall: 0.9724 - val_auc: 0.9988\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0795 - tp: 120221.0000 - fp: 2758.0000 - tn: 244524.0000 - fn: 3420.0000 - accuracy: 0.6592 - precision: 0.9776 - recall: 0.9723 - auc: 0.9975 - val_loss: 0.0445 - val_tp: 100455.0000 - val_fp: 954.0000 - val_tn: 202100.0000 - val_fn: 1072.0000 - val_accuracy: 0.9933 - val_precision: 0.9906 - val_recall: 0.9894 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0593 - tp: 120876.0000 - fp: 2627.0000 - tn: 244655.0000 - fn: 2765.0000 - accuracy: 0.6596 - precision: 0.9787 - recall: 0.9776 - auc: 0.9987 - val_loss: 0.0348 - val_tp: 100536.0000 - val_fp: 945.0000 - val_tn: 202109.0000 - val_fn: 991.0000 - val_accuracy: 0.9936 - val_precision: 0.9907 - val_recall: 0.9902 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0463 - tp: 121135.0000 - fp: 2400.0000 - tn: 244882.0000 - fn: 2506.0000 - accuracy: 0.6608 - precision: 0.9806 - recall: 0.9797 - auc: 0.9993 - val_loss: 0.0269 - val_tp: 100538.0000 - val_fp: 934.0000 - val_tn: 202120.0000 - val_fn: 989.0000 - val_accuracy: 0.9937 - val_precision: 0.9908 - val_recall: 0.9903 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0316 - tp: 122017.0000 - fp: 1498.0000 - tn: 245784.0000 - fn: 1624.0000 - accuracy: 0.6661 - precision: 0.9879 - recall: 0.9869 - auc: 0.9997 - val_loss: 0.0214 - val_tp: 100603.0000 - val_fp: 867.0000 - val_tn: 202187.0000 - val_fn: 924.0000 - val_accuracy: 0.9941 - val_precision: 0.9915 - val_recall: 0.9909 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0202 - tp: 122786.0000 - fp: 776.0000 - tn: 246506.0000 - fn: 855.0000 - accuracy: 0.6701 - precision: 0.9937 - recall: 0.9931 - auc: 0.9998 - val_loss: 0.0196 - val_tp: 100685.0000 - val_fp: 803.0000 - val_tn: 202251.0000 - val_fn: 842.0000 - val_accuracy: 0.9946 - val_precision: 0.9921 - val_recall: 0.9917 - val_auc: 0.9997\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0146 - tp: 123082.0000 - fp: 516.0000 - tn: 246766.0000 - fn: 559.0000 - accuracy: 0.6714 - precision: 0.9958 - recall: 0.9955 - auc: 0.9998 - val_loss: 0.0191 - val_tp: 100721.0000 - val_fp: 772.0000 - val_tn: 202282.0000 - val_fn: 806.0000 - val_accuracy: 0.9948 - val_precision: 0.9924 - val_recall: 0.9921 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0113 - tp: 123199.0000 - fp: 412.0000 - tn: 246870.0000 - fn: 442.0000 - accuracy: 0.6719 - precision: 0.9967 - recall: 0.9964 - auc: 0.9998 - val_loss: 0.0198 - val_tp: 100701.0000 - val_fp: 789.0000 - val_tn: 202265.0000 - val_fn: 826.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0095 - tp: 123266.0000 - fp: 349.0000 - tn: 246933.0000 - fn: 375.0000 - accuracy: 0.6722 - precision: 0.9972 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0197 - val_tp: 100728.0000 - val_fp: 766.0000 - val_tn: 202288.0000 - val_fn: 799.0000 - val_accuracy: 0.9949 - val_precision: 0.9925 - val_recall: 0.9921 - val_auc: 0.9995\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0083 - tp: 123294.0000 - fp: 324.0000 - tn: 246958.0000 - fn: 347.0000 - accuracy: 0.6723 - precision: 0.9974 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0204 - val_tp: 100707.0000 - val_fp: 787.0000 - val_tn: 202267.0000 - val_fn: 820.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0076 - tp: 123326.0000 - fp: 302.0000 - tn: 246980.0000 - fn: 315.0000 - accuracy: 0.6724 - precision: 0.9976 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0207 - val_tp: 100720.0000 - val_fp: 779.0000 - val_tn: 202275.0000 - val_fn: 807.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0067 - tp: 123357.0000 - fp: 271.0000 - tn: 247011.0000 - fn: 284.0000 - accuracy: 0.6725 - precision: 0.9978 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0216 - val_tp: 100708.0000 - val_fp: 786.0000 - val_tn: 202268.0000 - val_fn: 819.0000 - val_accuracy: 0.9947 - val_precision: 0.9923 - val_recall: 0.9919 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0064 - tp: 123359.0000 - fp: 280.0000 - tn: 247002.0000 - fn: 282.0000 - accuracy: 0.6726 - precision: 0.9977 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0214 - val_tp: 100733.0000 - val_fp: 768.0000 - val_tn: 202286.0000 - val_fn: 794.0000 - val_accuracy: 0.9949 - val_precision: 0.9924 - val_recall: 0.9922 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 425ms/step - loss: 0.0058 - tp: 123392.0000 - fp: 238.0000 - tn: 247044.0000 - fn: 249.0000 - accuracy: 0.6727 - precision: 0.9981 - recall: 0.9980 - auc: 0.9999 - val_loss: 0.0219 - val_tp: 100737.0000 - val_fp: 767.0000 - val_tn: 202287.0000 - val_fn: 790.0000 - val_accuracy: 0.9949 - val_precision: 0.9924 - val_recall: 0.9922 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0052 - tp: 123420.0000 - fp: 221.0000 - tn: 247061.0000 - fn: 221.0000 - accuracy: 0.6728 - precision: 0.9982 - recall: 0.9982 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0052 - tp: 123420.0000 - fp: 221.0000 - tn: 247061.0000 - fn: 221.0000 - accuracy: 0.6728 - precision: 0.9982 - recall: 0.9982 - auc: 0.9999 - val_loss: 0.0221 - val_tp: 100737.0000 - val_fp: 772.0000 - val_tn: 202282.0000 - val_fn: 790.0000 - val_accuracy: 0.9949 - val_precision: 0.9924 - val_recall: 0.9922 - val_auc: 0.9992\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downsampled_downweight_0.8.h5\n",
            "\n",
            "Down-weighting: 0.9\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 27s 477ms/step - loss: 0.2874 - tp: 206005.0000 - fp: 12153.0000 - tn: 438183.0000 - fn: 19163.0000 - accuracy: 0.7978 - precision: 0.9443 - recall: 0.9149 - auc: 0.9921 - val_loss: 0.0868 - val_tp: 98824.0000 - val_fp: 971.0000 - val_tn: 202083.0000 - val_fn: 2703.0000 - val_accuracy: 0.9879 - val_precision: 0.9903 - val_recall: 0.9734 - val_auc: 0.9988\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0845 - tp: 120284.0000 - fp: 2790.0000 - tn: 244492.0000 - fn: 3357.0000 - accuracy: 0.6591 - precision: 0.9773 - recall: 0.9728 - auc: 0.9974 - val_loss: 0.0437 - val_tp: 100465.0000 - val_fp: 959.0000 - val_tn: 202095.0000 - val_fn: 1062.0000 - val_accuracy: 0.9934 - val_precision: 0.9905 - val_recall: 0.9895 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0627 - tp: 120886.0000 - fp: 2638.0000 - tn: 244644.0000 - fn: 2755.0000 - accuracy: 0.6596 - precision: 0.9786 - recall: 0.9777 - auc: 0.9986 - val_loss: 0.0345 - val_tp: 100539.0000 - val_fp: 950.0000 - val_tn: 202104.0000 - val_fn: 988.0000 - val_accuracy: 0.9936 - val_precision: 0.9906 - val_recall: 0.9903 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0498 - tp: 121064.0000 - fp: 2479.0000 - tn: 244803.0000 - fn: 2577.0000 - accuracy: 0.6603 - precision: 0.9799 - recall: 0.9792 - auc: 0.9993 - val_loss: 0.0273 - val_tp: 100544.0000 - val_fp: 940.0000 - val_tn: 202114.0000 - val_fn: 983.0000 - val_accuracy: 0.9937 - val_precision: 0.9907 - val_recall: 0.9903 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 16s 423ms/step - loss: 0.0347 - tp: 121847.0000 - fp: 1673.0000 - tn: 245609.0000 - fn: 1794.0000 - accuracy: 0.6650 - precision: 0.9865 - recall: 0.9855 - auc: 0.9996 - val_loss: 0.0219 - val_tp: 100581.0000 - val_fp: 899.0000 - val_tn: 202155.0000 - val_fn: 946.0000 - val_accuracy: 0.9939 - val_precision: 0.9911 - val_recall: 0.9907 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0223 - tp: 122692.0000 - fp: 859.0000 - tn: 246423.0000 - fn: 949.0000 - accuracy: 0.6695 - precision: 0.9930 - recall: 0.9923 - auc: 0.9998 - val_loss: 0.0199 - val_tp: 100665.0000 - val_fp: 834.0000 - val_tn: 202220.0000 - val_fn: 862.0000 - val_accuracy: 0.9944 - val_precision: 0.9918 - val_recall: 0.9915 - val_auc: 0.9997\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0160 - tp: 123058.0000 - fp: 537.0000 - tn: 246745.0000 - fn: 583.0000 - accuracy: 0.6712 - precision: 0.9957 - recall: 0.9953 - auc: 0.9998 - val_loss: 0.0193 - val_tp: 100707.0000 - val_fp: 796.0000 - val_tn: 202258.0000 - val_fn: 820.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 426ms/step - loss: 0.0122 - tp: 123190.0000 - fp: 414.0000 - tn: 246868.0000 - fn: 451.0000 - accuracy: 0.6719 - precision: 0.9967 - recall: 0.9964 - auc: 0.9998 - val_loss: 0.0200 - val_tp: 100688.0000 - val_fp: 807.0000 - val_tn: 202247.0000 - val_fn: 839.0000 - val_accuracy: 0.9946 - val_precision: 0.9920 - val_recall: 0.9917 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0102 - tp: 123258.0000 - fp: 359.0000 - tn: 246923.0000 - fn: 383.0000 - accuracy: 0.6721 - precision: 0.9971 - recall: 0.9969 - auc: 0.9999 - val_loss: 0.0200 - val_tp: 100705.0000 - val_fp: 791.0000 - val_tn: 202263.0000 - val_fn: 822.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9995\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0090 - tp: 123286.0000 - fp: 330.0000 - tn: 246952.0000 - fn: 355.0000 - accuracy: 0.6722 - precision: 0.9973 - recall: 0.9971 - auc: 0.9999 - val_loss: 0.0206 - val_tp: 100705.0000 - val_fp: 795.0000 - val_tn: 202259.0000 - val_fn: 822.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0082 - tp: 123323.0000 - fp: 303.0000 - tn: 246979.0000 - fn: 318.0000 - accuracy: 0.6724 - precision: 0.9975 - recall: 0.9974 - auc: 0.9999 - val_loss: 0.0209 - val_tp: 100714.0000 - val_fp: 785.0000 - val_tn: 202269.0000 - val_fn: 813.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9920 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0072 - tp: 123345.0000 - fp: 287.0000 - tn: 246995.0000 - fn: 296.0000 - accuracy: 0.6724 - precision: 0.9977 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0221 - val_tp: 100690.0000 - val_fp: 809.0000 - val_tn: 202245.0000 - val_fn: 837.0000 - val_accuracy: 0.9946 - val_precision: 0.9920 - val_recall: 0.9918 - val_auc: 0.9994\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 428ms/step - loss: 0.0069 - tp: 123353.0000 - fp: 283.0000 - tn: 246999.0000 - fn: 288.0000 - accuracy: 0.6725 - precision: 0.9977 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0215 - val_tp: 100716.0000 - val_fp: 781.0000 - val_tn: 202273.0000 - val_fn: 811.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9920 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 427ms/step - loss: 0.0063 - tp: 123400.0000 - fp: 229.0000 - tn: 247053.0000 - fn: 241.0000 - accuracy: 0.6727 - precision: 0.9981 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0223 - val_tp: 100715.0000 - val_fp: 783.0000 - val_tn: 202271.0000 - val_fn: 812.0000 - val_accuracy: 0.9948 - val_precision: 0.9923 - val_recall: 0.9920 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0057 - tp: 123407.0000 - fp: 229.0000 - tn: 247053.0000 - fn: 234.0000 - accuracy: 0.6727 - precision: 0.9981 - recall: 0.9981 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0057 - tp: 123407.0000 - fp: 229.0000 - tn: 247053.0000 - fn: 234.0000 - accuracy: 0.6727 - precision: 0.9981 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0224 - val_tp: 100737.0000 - val_fp: 776.0000 - val_tn: 202278.0000 - val_fn: 790.0000 - val_accuracy: 0.9949 - val_precision: 0.9924 - val_recall: 0.9922 - val_auc: 0.9992\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downsampled_downweight_0.9.h5\n",
            "\n",
            "Down-weighting: 1.0\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 3 NER labels): (1222, 105, 3)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 28s 480ms/step - loss: 0.3154 - tp: 206078.0000 - fp: 12246.0000 - tn: 438090.0000 - fn: 19090.0000 - accuracy: 0.9536 - precision: 0.9439 - recall: 0.9152 - auc: 0.9921 - val_loss: 0.0861 - val_tp: 98903.0000 - val_fp: 1008.0000 - val_tn: 202046.0000 - val_fn: 2624.0000 - val_accuracy: 0.9881 - val_precision: 0.9899 - val_recall: 0.9742 - val_auc: 0.9988\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0894 - tp: 120322.0000 - fp: 2809.0000 - tn: 244473.0000 - fn: 3319.0000 - accuracy: 0.9835 - precision: 0.9772 - recall: 0.9732 - auc: 0.9973 - val_loss: 0.0431 - val_tp: 100476.0000 - val_fp: 961.0000 - val_tn: 202093.0000 - val_fn: 1051.0000 - val_accuracy: 0.9934 - val_precision: 0.9905 - val_recall: 0.9896 - val_auc: 0.9995\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0658 - tp: 120893.0000 - fp: 2645.0000 - tn: 244637.0000 - fn: 2748.0000 - accuracy: 0.9855 - precision: 0.9786 - recall: 0.9778 - auc: 0.9986 - val_loss: 0.0343 - val_tp: 100548.0000 - val_fp: 950.0000 - val_tn: 202104.0000 - val_fn: 979.0000 - val_accuracy: 0.9937 - val_precision: 0.9906 - val_recall: 0.9904 - val_auc: 0.9997\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0530 - tp: 121039.0000 - fp: 2532.0000 - tn: 244750.0000 - fn: 2602.0000 - accuracy: 0.9862 - precision: 0.9795 - recall: 0.9790 - auc: 0.9992 - val_loss: 0.0278 - val_tp: 100549.0000 - val_fp: 944.0000 - val_tn: 202110.0000 - val_fn: 978.0000 - val_accuracy: 0.9937 - val_precision: 0.9907 - val_recall: 0.9904 - val_auc: 0.9998\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0379 - tp: 121675.0000 - fp: 1852.0000 - tn: 245430.0000 - fn: 1966.0000 - accuracy: 0.9897 - precision: 0.9850 - recall: 0.9841 - auc: 0.9996 - val_loss: 0.0225 - val_tp: 100585.0000 - val_fp: 907.0000 - val_tn: 202147.0000 - val_fn: 942.0000 - val_accuracy: 0.9939 - val_precision: 0.9911 - val_recall: 0.9907 - val_auc: 0.9999\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0245 - tp: 122623.0000 - fp: 939.0000 - tn: 246343.0000 - fn: 1018.0000 - accuracy: 0.9947 - precision: 0.9924 - recall: 0.9918 - auc: 0.9998 - val_loss: 0.0202 - val_tp: 100644.0000 - val_fp: 848.0000 - val_tn: 202206.0000 - val_fn: 883.0000 - val_accuracy: 0.9943 - val_precision: 0.9916 - val_recall: 0.9913 - val_auc: 0.9998\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 17s 430ms/step - loss: 0.0173 - tp: 123025.0000 - fp: 563.0000 - tn: 246719.0000 - fn: 616.0000 - accuracy: 0.9968 - precision: 0.9954 - recall: 0.9950 - auc: 0.9998 - val_loss: 0.0196 - val_tp: 100692.0000 - val_fp: 809.0000 - val_tn: 202245.0000 - val_fn: 835.0000 - val_accuracy: 0.9946 - val_precision: 0.9920 - val_recall: 0.9918 - val_auc: 0.9997\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 17s 431ms/step - loss: 0.0131 - tp: 123173.0000 - fp: 431.0000 - tn: 246851.0000 - fn: 468.0000 - accuracy: 0.9976 - precision: 0.9965 - recall: 0.9962 - auc: 0.9998 - val_loss: 0.0203 - val_tp: 100677.0000 - val_fp: 824.0000 - val_tn: 202230.0000 - val_fn: 850.0000 - val_accuracy: 0.9945 - val_precision: 0.9919 - val_recall: 0.9916 - val_auc: 0.9996\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 17s 431ms/step - loss: 0.0109 - tp: 123238.0000 - fp: 371.0000 - tn: 246911.0000 - fn: 403.0000 - accuracy: 0.9979 - precision: 0.9970 - recall: 0.9967 - auc: 0.9999 - val_loss: 0.0203 - val_tp: 100704.0000 - val_fp: 799.0000 - val_tn: 202255.0000 - val_fn: 823.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9919 - val_auc: 0.9995\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0096 - tp: 123278.0000 - fp: 341.0000 - tn: 246941.0000 - fn: 363.0000 - accuracy: 0.9981 - precision: 0.9972 - recall: 0.9971 - auc: 0.9999 - val_loss: 0.0209 - val_tp: 100697.0000 - val_fp: 807.0000 - val_tn: 202247.0000 - val_fn: 830.0000 - val_accuracy: 0.9946 - val_precision: 0.9920 - val_recall: 0.9918 - val_auc: 0.9995\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 17s 433ms/step - loss: 0.0088 - tp: 123306.0000 - fp: 316.0000 - tn: 246966.0000 - fn: 335.0000 - accuracy: 0.9982 - precision: 0.9974 - recall: 0.9973 - auc: 0.9999 - val_loss: 0.0213 - val_tp: 100700.0000 - val_fp: 802.0000 - val_tn: 202252.0000 - val_fn: 827.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9919 - val_auc: 0.9995\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 17s 431ms/step - loss: 0.0078 - tp: 123327.0000 - fp: 304.0000 - tn: 246978.0000 - fn: 314.0000 - accuracy: 0.9983 - precision: 0.9975 - recall: 0.9975 - auc: 0.9999 - val_loss: 0.0229 - val_tp: 100673.0000 - val_fp: 829.0000 - val_tn: 202225.0000 - val_fn: 854.0000 - val_accuracy: 0.9945 - val_precision: 0.9918 - val_recall: 0.9916 - val_auc: 0.9993\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0074 - tp: 123349.0000 - fp: 286.0000 - tn: 246996.0000 - fn: 292.0000 - accuracy: 0.9984 - precision: 0.9977 - recall: 0.9976 - auc: 0.9999 - val_loss: 0.0220 - val_tp: 100703.0000 - val_fp: 795.0000 - val_tn: 202259.0000 - val_fn: 824.0000 - val_accuracy: 0.9947 - val_precision: 0.9922 - val_recall: 0.9919 - val_auc: 0.9994\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0068 - tp: 123383.0000 - fp: 247.0000 - tn: 247035.0000 - fn: 258.0000 - accuracy: 0.9986 - precision: 0.9980 - recall: 0.9979 - auc: 0.9999 - val_loss: 0.0229 - val_tp: 100692.0000 - val_fp: 808.0000 - val_tn: 202246.0000 - val_fn: 835.0000 - val_accuracy: 0.9946 - val_precision: 0.9920 - val_recall: 0.9918 - val_auc: 0.9993\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0063 - tp: 123388.0000 - fp: 244.0000 - tn: 247038.0000 - fn: 253.0000 - accuracy: 0.9987 - precision: 0.9980 - recall: 0.9980 - auc: 0.9999Restoring model weights from the end of the best epoch: 5.\n",
            "39/39 [==============================] - 17s 429ms/step - loss: 0.0063 - tp: 123388.0000 - fp: 244.0000 - tn: 247038.0000 - fn: 253.0000 - accuracy: 0.9987 - precision: 0.9980 - recall: 0.9980 - auc: 0.9999 - val_loss: 0.0231 - val_tp: 100707.0000 - val_fp: 803.0000 - val_tn: 202251.0000 - val_fn: 820.0000 - val_accuracy: 0.9947 - val_precision: 0.9921 - val_recall: 0.9919 - val_auc: 0.9992\n",
            "Epoch 00015: early stopping\n",
            "Model saved at BiLSTM_PoS_2class_downsampled.h5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lEooZO1w2ZD",
        "outputId": "50239527-0b77-46a3-b437-c020353c47f6"
      },
      "source": [
        "for weight in [1.0, .9, .8, .7, .6, .5, .4, .3, .2, .1]:\n",
        "    preds = np.argmax(downweight_models[weight].predict([dev_seqs_bo_padded, dev_pos_bo_padded]), axis=-1)\n",
        "\n",
        "    dev_seqs_bo['prediction'] = ''\n",
        "    for i in dev_seqs_bo.index:\n",
        "        this_seq_length = len(dev_seqs_bo['token'][i])\n",
        "        dev_seqs_bo['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    dev_long = dev_seqs_bo.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = reverse_bio_from_bo(dev_long['bio_only'])\n",
        "    dev_long['bio_only'] = bio_labs\n",
        "    pred_labs = reverse_bio_from_bo(dev_long['prediction'])\n",
        "    dev_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(dev_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 1.0:\n",
            "Sum of TP and FP = 69\n",
            "Sum of TP and FN = 805\n",
            "True positives = 12, False positives = 57, False negatives = 793\n",
            "Precision = 0.174, Recall = 0.015, F1 = 0.027\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 113\n",
            "Sum of TP and FN = 805\n",
            "True positives = 21, False positives = 92, False negatives = 784\n",
            "Precision = 0.186, Recall = 0.026, F1 = 0.046\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 171\n",
            "Sum of TP and FN = 805\n",
            "True positives = 47, False positives = 124, False negatives = 758\n",
            "Precision = 0.275, Recall = 0.058, F1 = 0.096\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 224\n",
            "Sum of TP and FN = 805\n",
            "True positives = 66, False positives = 158, False negatives = 739\n",
            "Precision = 0.295, Recall = 0.082, F1 = 0.128\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 333\n",
            "Sum of TP and FN = 805\n",
            "True positives = 110, False positives = 223, False negatives = 695\n",
            "Precision = 0.330, Recall = 0.137, F1 = 0.193\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 479\n",
            "Sum of TP and FN = 805\n",
            "True positives = 187, False positives = 292, False negatives = 618\n",
            "Precision = 0.390, Recall = 0.232, F1 = 0.291\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 645\n",
            "Sum of TP and FN = 805\n",
            "True positives = 272, False positives = 373, False negatives = 533\n",
            "Precision = 0.422, Recall = 0.338, F1 = 0.375\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 760\n",
            "Sum of TP and FN = 805\n",
            "True positives = 327, False positives = 433, False negatives = 478\n",
            "Precision = 0.430, Recall = 0.406, F1 = 0.418\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 874\n",
            "Sum of TP and FN = 805\n",
            "True positives = 357, False positives = 517, False negatives = 448\n",
            "Precision = 0.408, Recall = 0.443, F1 = 0.425\n",
            "\n",
            "Weight = 0.1:\n",
            "Sum of TP and FP = 976\n",
            "Sum of TP and FN = 805\n",
            "True positives = 354, False positives = 622, False negatives = 451\n",
            "Precision = 0.363, Recall = 0.440, F1 = 0.398\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckySZlVt1tIA"
      },
      "source": [
        "### 5. Evaluation on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLTr9ZsHZsWZ",
        "outputId": "98279fef-c5c6-4def-c1ba-000dfa71fc48"
      },
      "source": [
        "# process the annotated test set\n",
        "wnuttest_annot = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_annotated_clean_tagged.txt'\n",
        "test_annot = pd.read_table(wnuttest_annot, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "test_annot_copy = extract_features(test_annot)\n",
        "test_annot_seqs = tokens2sequences(test_annot_copy)\n",
        "test_annot_longest = find_longest_sequence(test_annot_seqs, 0)\n",
        "assert test_annot_longest == test_longest\n",
        "print('The longest sequence in the test set is %i tokens long' % test_annot_longest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The longest sequence in the test set is 105 tokens long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F3wxwfHwvB0",
        "outputId": "8273c9d8-713c-4f6d-9ed5-e833f7a76728"
      },
      "source": [
        "# pad the tokens & labels, and one-hot encode the labels for the test set\n",
        "test_seqs_padded = pad_sequences(test_annot_seqs['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                 dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "test_labs_padded = pad_sequences(test_annot_seqs['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                 dtype='int32', padding='post', truncating='post', value=padlab)\n",
        "test_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in test_labs_padded]\n",
        "\n",
        "print('Test set padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(test_annot_seqs.loc[4])\n",
        "print('Length of input sequence: %i' % len(test_seqs_padded[1]))\n",
        "print('Length of label sequence: %i' % len(test_labs_onehot[1]))\n",
        "print(test_labs_padded[4][:11])\n",
        "print(test_labs_onehot[4][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     4\n",
            "token            [The, bodies, of, the, soldiers, were, recover...\n",
            "bio_only         [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...\n",
            "token_indices    [191.0, 14801.0, 45.0, 3.0, 14801.0, 225.0, 14...\n",
            "prediction       [0, 1, 2, 2, 0, 2, 2, 2, 2, 0, 1, 2, 2, 0, 1, ...\n",
            "Name: 4, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of label sequence: 105\n",
            "[2 2 2 2 2 2 2 2 2 2 2]\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hMWoxQSH43pa",
        "outputId": "2490849f-22e4-4b8e-909c-f2f46d0f661c"
      },
      "source": [
        "# 2-class test set\n",
        "test_annot_copy_bo = extract_features_bo(test_annot)\n",
        "test_annot_seqs_bo = tokens2sequences(test_annot_copy_bo)\n",
        "test_annot_seqs_bo.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[&amp;, gt, ;, *, The, soldier, was, killed, when,...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 1480...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[&amp;, gt, ;, *, Police, last, week, evacuated, 8...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 14801.0, 23...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[&amp;, gt, ;, *, The, army, on, Thursday, recover...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 1480...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[&amp;, gt, ;, *, The, four, civilians, killed, in...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 4012...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[The, bodies, of, the, soldiers, were, recover...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[191.0, 14801.0, 45.0, 3.0, 14801.0, 225.0, 14...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                      token_indices\n",
              "0             0  ...  [14801.0, 14801.0, 1625.0, 1743.0, 191.0, 1480...\n",
              "1             1  ...  [14801.0, 14801.0, 1625.0, 1743.0, 14801.0, 23...\n",
              "2             2  ...  [14801.0, 14801.0, 1625.0, 1743.0, 191.0, 1480...\n",
              "3             3  ...  [14801.0, 14801.0, 1625.0, 1743.0, 191.0, 4012...\n",
              "4             4  ...  [191.0, 14801.0, 45.0, 3.0, 14801.0, 225.0, 14...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1thwhpf5Fcf",
        "outputId": "662ee5d8-84db-43b2-ebce-e0e3d9dd92d5"
      },
      "source": [
        "# now process the 2-class test set in the same way: padding the tokens & labels, and one-hot encoding the labels\n",
        "test_seqs_bo_padded = pad_sequences(test_annot_seqs_bo['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                    dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "test_labs_bo_padded = pad_sequences(test_annot_seqs_bo['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                    dtype='int32', padding='post', truncating='post', value=padlab_bo)\n",
        "test_labs_bo_onehot = [to_categorical(i, num_classes=n_labs_bo) for i in test_labs_bo_padded]\n",
        "\n",
        "print('Test set padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(test_annot_seqs_bo.loc[4])\n",
        "print('Length of input sequence: %i' % len(test_seqs_bo_padded[1]))\n",
        "print('Length of label sequence: %i' % len(test_labs_bo_onehot[1]))\n",
        "print(test_labs_bo_padded[4][:11])\n",
        "print(test_labs_bo_onehot[4][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     4\n",
            "token            [The, bodies, of, the, soldiers, were, recover...\n",
            "bio_only         [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
            "token_indices    [191.0, 14801.0, 45.0, 3.0, 14801.0, 225.0, 14...\n",
            "Name: 4, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of label sequence: 105\n",
            "[1 1 1 1 1 1 1 1 1 1 1]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "1-TSQa8G7VFE",
        "outputId": "60b1c54c-4825-4dfb-ab1a-0ef62c988bdf"
      },
      "source": [
        "# test set with pos\n",
        "wnuttest_annot = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_annotated_clean_tagged.txt'\n",
        "test_annot = pd.read_table(wnuttest_annot, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "test_annot_copy = extract_features_pos(test_annot)\n",
        "test_annot_seqs = tokens2sequences_pos(test_annot_copy)\n",
        "test_annot_seqs.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>upos_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[&amp;, gt, ;, *, The, soldier, was, killed, when,...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 1480...</td>\n",
              "      <td>[16.0, 6.0, 8.0, 8.0, 3.0, 11.0, 2.0, 14.0, 5....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[&amp;, gt, ;, *, Police, last, week, evacuated, 8...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 14801.0, 23...</td>\n",
              "      <td>[16.0, 6.0, 8.0, 8.0, 0.0, 11.0, 0.0, 14.0, 7....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[&amp;, gt, ;, *, The, army, on, Thursday, recover...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 1480...</td>\n",
              "      <td>[16.0, 6.0, 8.0, 8.0, 3.0, 0.0, 4.0, 9.0, 14.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[&amp;, gt, ;, *, The, four, civilians, killed, in...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 4012...</td>\n",
              "      <td>[16.0, 6.0, 8.0, 8.0, 3.0, 7.0, 0.0, 14.0, 14....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[The, bodies, of, the, soldiers, were, recover...</td>\n",
              "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
              "      <td>[191.0, 14801.0, 45.0, 3.0, 14801.0, 225.0, 14...</td>\n",
              "      <td>[3.0, 0.0, 4.0, 3.0, 0.0, 2.0, 14.0, 5.0, 3.0,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                       upos_indices\n",
              "0             0  ...  [16.0, 6.0, 8.0, 8.0, 3.0, 11.0, 2.0, 14.0, 5....\n",
              "1             1  ...  [16.0, 6.0, 8.0, 8.0, 0.0, 11.0, 0.0, 14.0, 7....\n",
              "2             2  ...  [16.0, 6.0, 8.0, 8.0, 3.0, 0.0, 4.0, 9.0, 14.0...\n",
              "3             3  ...  [16.0, 6.0, 8.0, 8.0, 3.0, 7.0, 0.0, 14.0, 14....\n",
              "4             4  ...  [3.0, 0.0, 4.0, 3.0, 0.0, 2.0, 14.0, 5.0, 3.0,...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMdWEvX974Dh",
        "outputId": "a0d7f6ae-d822-4ef5-ec17-47be646a680a"
      },
      "source": [
        "# pad the tokens, pos and labels, and one-hot encode the labels\n",
        "test_seqs_padded = pad_sequences(test_annot_seqs['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                 dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "test_pos_padded = pad_sequences(test_annot_seqs['upos_indices'].tolist(), maxlen=seq_length,\n",
        "                                dtype='int32', padding='post', truncating='post', value=padpos)\n",
        "test_labs_padded = pad_sequences(test_annot_seqs['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                 dtype='int32', padding='post', truncating='post', value=padlab)\n",
        "test_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in test_labs_padded]\n",
        "\n",
        "print('Test set padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(test_annot_seqs.loc[4])\n",
        "print('Length of input sequence: %i' % len(test_seqs_padded[1]))\n",
        "print('Length of pos sequence: %i' % len(test_pos_padded[1]))\n",
        "print('Length of label sequence: %i' % len(test_labs_onehot[1]))\n",
        "print(test_pos_padded[4][:11])\n",
        "print(test_labs_padded[4][:11])\n",
        "print(test_labs_onehot[4][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     4\n",
            "token            [The, bodies, of, the, soldiers, were, recover...\n",
            "bio_only         [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...\n",
            "token_indices    [191.0, 14801.0, 45.0, 3.0, 14801.0, 225.0, 14...\n",
            "upos_indices     [3.0, 0.0, 4.0, 3.0, 0.0, 2.0, 14.0, 5.0, 3.0,...\n",
            "Name: 4, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of pos sequence: 105\n",
            "Length of label sequence: 105\n",
            "[ 3  0  4  3  0  2 14  5  3 11  0]\n",
            "[2 2 2 2 2 2 2 2 2 2 2]\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "OF9Lysgp8iKu",
        "outputId": "d70db465-8a95-4a87-b612-ecc694d885ea"
      },
      "source": [
        "# 2-class test set with pos\n",
        "test_annot_copy_bo = extract_features_pos_bo(test_annot)\n",
        "test_annot_seqs_bo = tokens2sequences_pos(test_annot_copy_bo)\n",
        "test_annot_seqs_bo.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>upos_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[&amp;, gt, ;, *, The, soldier, was, killed, when,...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 1480...</td>\n",
              "      <td>[16.0, 6.0, 8.0, 8.0, 3.0, 11.0, 2.0, 14.0, 5....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[&amp;, gt, ;, *, Police, last, week, evacuated, 8...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 14801.0, 23...</td>\n",
              "      <td>[16.0, 6.0, 8.0, 8.0, 0.0, 11.0, 0.0, 14.0, 7....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[&amp;, gt, ;, *, The, army, on, Thursday, recover...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 1480...</td>\n",
              "      <td>[16.0, 6.0, 8.0, 8.0, 3.0, 0.0, 4.0, 9.0, 14.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[&amp;, gt, ;, *, The, four, civilians, killed, in...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 4012...</td>\n",
              "      <td>[16.0, 6.0, 8.0, 8.0, 3.0, 7.0, 0.0, 14.0, 14....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[The, bodies, of, the, soldiers, were, recover...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[191.0, 14801.0, 45.0, 3.0, 14801.0, 225.0, 14...</td>\n",
              "      <td>[3.0, 0.0, 4.0, 3.0, 0.0, 2.0, 14.0, 5.0, 3.0,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                       upos_indices\n",
              "0             0  ...  [16.0, 6.0, 8.0, 8.0, 3.0, 11.0, 2.0, 14.0, 5....\n",
              "1             1  ...  [16.0, 6.0, 8.0, 8.0, 0.0, 11.0, 0.0, 14.0, 7....\n",
              "2             2  ...  [16.0, 6.0, 8.0, 8.0, 3.0, 0.0, 4.0, 9.0, 14.0...\n",
              "3             3  ...  [16.0, 6.0, 8.0, 8.0, 3.0, 7.0, 0.0, 14.0, 14....\n",
              "4             4  ...  [3.0, 0.0, 4.0, 3.0, 0.0, 2.0, 14.0, 5.0, 3.0,...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98ozQ4mO_pmz",
        "outputId": "166dfc46-2881-4056-8547-a2495275e37c"
      },
      "source": [
        "# now process the 2-class test set in the same way: padding the tokens, pos and labels, and one-hot encoding the labels\n",
        "test_seqs_bo_padded = pad_sequences(test_annot_seqs_bo['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                    dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "test_pos_bo_padded = pad_sequences(test_annot_seqs_bo['upos_indices'].tolist(), maxlen=seq_length,\n",
        "                                   dtype='int32', padding='post', truncating='post', value=padpos)\n",
        "test_labs_bo_padded = pad_sequences(test_annot_seqs_bo['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                    dtype='int32', padding='post', truncating='post', value=padlab_bo)\n",
        "test_labs_bo_onehot = [to_categorical(i, num_classes=n_labs_bo) for i in test_labs_bo_padded]\n",
        "\n",
        "print('Test set padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(test_annot_seqs_bo.loc[4])\n",
        "print('Length of input sequence: %i' % len(test_seqs_bo_padded[1]))\n",
        "print('Length of pos sequence: %i' % len(test_pos_bo_padded[1]))\n",
        "print('Length of label sequence: %i' % len(test_labs_bo_onehot[1]))\n",
        "print(test_pos_bo_padded[4][:11])\n",
        "print(test_labs_bo_padded[4][:11])\n",
        "print(test_labs_bo_onehot[4][:11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     4\n",
            "token            [The, bodies, of, the, soldiers, were, recover...\n",
            "bio_only         [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
            "token_indices    [191.0, 14801.0, 45.0, 3.0, 14801.0, 225.0, 14...\n",
            "upos_indices     [3.0, 0.0, 4.0, 3.0, 0.0, 2.0, 14.0, 5.0, 3.0,...\n",
            "Name: 4, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of pos sequence: 105\n",
            "Length of label sequence: 105\n",
            "[ 3  0  4  3  0  2 14  5  3 11  0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTMs7vQwt4Ev",
        "outputId": "021ddead-8672-4f04-ce41-bfa60cedc3c0"
      },
      "source": [
        "# BiLSTM\n",
        "for weight in [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]:\n",
        "    if weight == 1.0:\n",
        "        model = keras.models.load_model('BiLSTM.h5')\n",
        "    else:\n",
        "        model = keras.models.load_model(f'BiLSTM_downweight_{weight}.h5')\n",
        "\n",
        "    preds = np.argmax(model.predict(test_seqs_padded), axis=-1)\n",
        "\n",
        "    test_annot_seqs['prediction'] = ''\n",
        "    for i in test_annot_seqs.index:\n",
        "        this_seq_length = len(test_annot_seqs['token'][i])\n",
        "        test_annot_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    test_long = test_annot_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = [reverse_bio(b) for b in test_long['bio_only']]\n",
        "    test_long['bio_only'] = bio_labs\n",
        "    pred_labs = [reverse_bio(b) for b in test_long['prediction']]\n",
        "    test_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(test_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 1.0:\n",
            "Sum of TP and FP = 3\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 3, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 8\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 8, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 14\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 1, False positives = 13, False negatives = 1075\n",
            "Precision = 0.071, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 24\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 2, False positives = 22, False negatives = 1074\n",
            "Precision = 0.083, Recall = 0.002, F1 = 0.004\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 47\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 3, False positives = 44, False negatives = 1073\n",
            "Precision = 0.064, Recall = 0.003, F1 = 0.005\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 77\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 5, False positives = 72, False negatives = 1071\n",
            "Precision = 0.065, Recall = 0.005, F1 = 0.009\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 5\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 5, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 25\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 25, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 248\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 15, False positives = 233, False negatives = 1061\n",
            "Precision = 0.060, Recall = 0.014, F1 = 0.023\n",
            "\n",
            "Weight = 0.1:\n",
            "Sum of TP and FP = 949\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 124, False positives = 825, False negatives = 952\n",
            "Precision = 0.131, Recall = 0.115, F1 = 0.122\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcO6tCCC06b3",
        "outputId": "032d17ff-2062-472a-afae-298f723efa3c"
      },
      "source": [
        "# BiLSTM (downsampled)\n",
        "for weight in [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]:\n",
        "    if weight == 1.0:\n",
        "        model = keras.models.load_model('BiLSTM_downsampled.h5')\n",
        "    else:\n",
        "        model = keras.models.load_model(f'BiLSTM_downsampled_downweight_{weight}.h5')\n",
        "\n",
        "    preds = np.argmax(model.predict(test_seqs_padded), axis=-1)\n",
        "\n",
        "    test_annot_seqs['prediction'] = ''\n",
        "    for i in test_annot_seqs.index:\n",
        "        this_seq_length = len(test_annot_seqs['token'][i])\n",
        "        test_annot_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    test_long = test_annot_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = [reverse_bio(b) for b in test_long['bio_only']]\n",
        "    test_long['bio_only'] = bio_labs\n",
        "    pred_labs = [reverse_bio(b) for b in test_long['prediction']]\n",
        "    test_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(test_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 1.0:\n",
            "Sum of TP and FP = 3\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 3, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 7\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 7, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 10\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 10, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 23\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 23, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 49\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 1, False positives = 48, False negatives = 1075\n",
            "Precision = 0.020, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 133\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 2, False positives = 131, False negatives = 1074\n",
            "Precision = 0.015, Recall = 0.002, F1 = 0.003\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 268\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 15, False positives = 253, False negatives = 1061\n",
            "Precision = 0.056, Recall = 0.014, F1 = 0.022\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 624\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 48, False positives = 576, False negatives = 1028\n",
            "Precision = 0.077, Recall = 0.045, F1 = 0.056\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 1504\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 112, False positives = 1392, False negatives = 964\n",
            "Precision = 0.074, Recall = 0.104, F1 = 0.087\n",
            "\n",
            "Weight = 0.1:\n",
            "Sum of TP and FP = 3348\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 338, False positives = 3010, False negatives = 738\n",
            "Precision = 0.101, Recall = 0.314, F1 = 0.153\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtuaDxCF2AUP",
        "outputId": "0b9c08df-6b13-4627-d005-6a1251867efa"
      },
      "source": [
        "# BiLSTM (2-class)\n",
        "for weight in [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]:\n",
        "    if weight == 1.0:\n",
        "        model = keras.models.load_model('BiLSTM_2class.h5')\n",
        "    else:\n",
        "        model = keras.models.load_model(f'BiLSTM_2class_downweight_{weight}.h5')\n",
        "\n",
        "    preds = np.argmax(model.predict(test_seqs_bo_padded), axis=-1)\n",
        "\n",
        "    test_annot_seqs_bo['prediction'] = ''\n",
        "    for i in test_annot_seqs_bo.index:\n",
        "        this_seq_length = len(test_annot_seqs_bo['token'][i])\n",
        "        test_annot_seqs_bo['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    test_long = test_annot_seqs_bo.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = reverse_bio_from_bo(test_long['bio_only'])\n",
        "    test_long['bio_only'] = bio_labs\n",
        "    pred_labs = reverse_bio_from_bo(test_long['prediction'])\n",
        "    test_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(test_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 1.0:\n",
            "Sum of TP and FP = 16\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 2, False positives = 14, False negatives = 1065\n",
            "Precision = 0.125, Recall = 0.002, F1 = 0.004\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 35\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 4, False positives = 31, False negatives = 1063\n",
            "Precision = 0.114, Recall = 0.004, F1 = 0.007\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 55\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 7, False positives = 48, False negatives = 1060\n",
            "Precision = 0.127, Recall = 0.007, F1 = 0.012\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 95\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 13, False positives = 82, False negatives = 1054\n",
            "Precision = 0.137, Recall = 0.012, F1 = 0.022\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 136\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 25, False positives = 111, False negatives = 1042\n",
            "Precision = 0.184, Recall = 0.023, F1 = 0.042\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 14\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 0, False positives = 14, False negatives = 1067\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 94\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 6, False positives = 88, False negatives = 1061\n",
            "Precision = 0.064, Recall = 0.006, F1 = 0.010\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 422\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 30, False positives = 392, False negatives = 1037\n",
            "Precision = 0.071, Recall = 0.028, F1 = 0.040\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 1133\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 103, False positives = 1030, False negatives = 964\n",
            "Precision = 0.091, Recall = 0.097, F1 = 0.094\n",
            "\n",
            "Weight = 0.1:\n",
            "Sum of TP and FP = 1560\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 237, False positives = 1323, False negatives = 830\n",
            "Precision = 0.152, Recall = 0.222, F1 = 0.180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZLtXGQb3z5C",
        "outputId": "1b5a86f5-217d-4d0b-fd75-33ed82c9025a"
      },
      "source": [
        "# BiLSTM (2-class, downsampled)\n",
        "for weight in [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]:\n",
        "    if weight == 1.0:\n",
        "        model = keras.models.load_model('BiLSTM_2class_downsampled.h5')\n",
        "    else:\n",
        "        model = keras.models.load_model(f'BiLSTM_2class_downsampled_downweight_{weight}.h5')\n",
        "\n",
        "    preds = np.argmax(model.predict(test_seqs_bo_padded), axis=-1)\n",
        "\n",
        "    test_annot_seqs_bo['prediction'] = ''\n",
        "    for i in test_annot_seqs_bo.index:\n",
        "        this_seq_length = len(test_annot_seqs_bo['token'][i])\n",
        "        test_annot_seqs_bo['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    test_long = test_annot_seqs_bo.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = reverse_bio_from_bo(test_long['bio_only'])\n",
        "    test_long['bio_only'] = bio_labs\n",
        "    pred_labs = reverse_bio_from_bo(test_long['prediction'])\n",
        "    test_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(test_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 1.0:\n",
            "Sum of TP and FP = 54\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 1, False positives = 53, False negatives = 1066\n",
            "Precision = 0.019, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 116\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 3, False positives = 113, False negatives = 1064\n",
            "Precision = 0.026, Recall = 0.003, F1 = 0.005\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 272\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 25, False positives = 247, False negatives = 1042\n",
            "Precision = 0.092, Recall = 0.023, F1 = 0.037\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 420\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 43, False positives = 377, False negatives = 1024\n",
            "Precision = 0.102, Recall = 0.040, F1 = 0.058\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 616\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 61, False positives = 555, False negatives = 1006\n",
            "Precision = 0.099, Recall = 0.057, F1 = 0.072\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 905\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 107, False positives = 798, False negatives = 960\n",
            "Precision = 0.118, Recall = 0.100, F1 = 0.109\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 1342\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 141, False positives = 1201, False negatives = 926\n",
            "Precision = 0.105, Recall = 0.132, F1 = 0.117\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 1893\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 229, False positives = 1664, False negatives = 838\n",
            "Precision = 0.121, Recall = 0.215, F1 = 0.155\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 2229\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 300, False positives = 1929, False negatives = 767\n",
            "Precision = 0.135, Recall = 0.281, F1 = 0.182\n",
            "\n",
            "Weight = 0.1:\n",
            "Sum of TP and FP = 3034\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 385, False positives = 2649, False negatives = 682\n",
            "Precision = 0.127, Recall = 0.361, F1 = 0.188\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pEkufn_8pDe",
        "outputId": "fbd63b1a-0acc-451d-9ded-8ce0ad64e4a4"
      },
      "source": [
        "# BiLSTM (PoS)\n",
        "for weight in [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]:\n",
        "    if weight == 1.0:\n",
        "        model = keras.models.load_model('BiLSTM_PoS.h5')\n",
        "    else:\n",
        "        model = keras.models.load_model(f'BiLSTM_PoS_downweight_{weight}.h5')\n",
        "\n",
        "    preds = np.argmax(model.predict([test_seqs_padded, test_pos_padded]), axis=-1)\n",
        "\n",
        "    test_annot_seqs['prediction'] = ''\n",
        "    for i in test_annot_seqs.index:\n",
        "        this_seq_length = len(test_annot_seqs['token'][i])\n",
        "        test_annot_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    test_long = test_annot_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = [reverse_bio(b) for b in test_long['bio_only']]\n",
        "    test_long['bio_only'] = bio_labs\n",
        "    pred_labs = [reverse_bio(b) for b in test_long['prediction']]\n",
        "    test_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(test_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 1.0:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 0, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 0, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 0, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 0\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 0, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 1\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 1, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 5\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 5, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 34\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 1, False positives = 33, False negatives = 1075\n",
            "Precision = 0.029, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 157\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 9, False positives = 148, False negatives = 1067\n",
            "Precision = 0.057, Recall = 0.008, F1 = 0.015\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 665\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 104, False positives = 561, False negatives = 972\n",
            "Precision = 0.156, Recall = 0.097, F1 = 0.119\n",
            "\n",
            "Weight = 0.1:\n",
            "Sum of TP and FP = 1714\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 341, False positives = 1373, False negatives = 735\n",
            "Precision = 0.199, Recall = 0.317, F1 = 0.244\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmbxls7h9HLw",
        "outputId": "a5337a9b-384d-47fd-b67d-e61fa84f1937"
      },
      "source": [
        "# BiLSTM (PoS, downsampled)\n",
        "for weight in [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]:\n",
        "    if weight == 1.0:\n",
        "        model = keras.models.load_model('BiLSTM_PoS_downsampled.h5')\n",
        "    else:\n",
        "        model = keras.models.load_model(f'BiLSTM_PoS_downsampled_downweight_{weight}.h5')\n",
        "\n",
        "    preds = np.argmax(model.predict([test_seqs_padded, test_pos_padded]), axis=-1)\n",
        "\n",
        "    test_annot_seqs['prediction'] = ''\n",
        "    for i in test_annot_seqs.index:\n",
        "        this_seq_length = len(test_annot_seqs['token'][i])\n",
        "        test_annot_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    test_long = test_annot_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = [reverse_bio(b) for b in test_long['bio_only']]\n",
        "    test_long['bio_only'] = bio_labs\n",
        "    pred_labs = [reverse_bio(b) for b in test_long['prediction']]\n",
        "    test_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(test_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 1.0:\n",
            "Sum of TP and FP = 3\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 3, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 11\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 0, False positives = 11, False negatives = 1076\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 26\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 1, False positives = 25, False negatives = 1075\n",
            "Precision = 0.038, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 61\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 1, False positives = 60, False negatives = 1075\n",
            "Precision = 0.016, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 188\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 12, False positives = 176, False negatives = 1064\n",
            "Precision = 0.064, Recall = 0.011, F1 = 0.019\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 372\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 41, False positives = 331, False negatives = 1035\n",
            "Precision = 0.110, Recall = 0.038, F1 = 0.057\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 700\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 122, False positives = 578, False negatives = 954\n",
            "Precision = 0.174, Recall = 0.113, F1 = 0.137\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 1089\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 271, False positives = 818, False negatives = 805\n",
            "Precision = 0.249, Recall = 0.252, F1 = 0.250\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 1348\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 362, False positives = 986, False negatives = 714\n",
            "Precision = 0.269, Recall = 0.336, F1 = 0.299\n",
            "\n",
            "Weight = 0.1:\n",
            "Sum of TP and FP = 1796\n",
            "Sum of TP and FN = 1076\n",
            "True positives = 432, False positives = 1364, False negatives = 644\n",
            "Precision = 0.241, Recall = 0.401, F1 = 0.301\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gfalLwRAE88",
        "outputId": "2ab88313-eec5-4d29-f2b5-9a97beb1ab83"
      },
      "source": [
        "# BiLSTM (PoS, 2-class)\n",
        "for weight in [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]:\n",
        "    if weight == 1.0:\n",
        "        model = keras.models.load_model('BiLSTM_PoS_2class.h5')\n",
        "    else:\n",
        "        model = keras.models.load_model(f'BiLSTM_PoS_2class_downweight_{weight}.h5')\n",
        "\n",
        "    preds = np.argmax(model.predict([test_seqs_bo_padded, test_pos_bo_padded]), axis=-1)\n",
        "\n",
        "    test_annot_seqs_bo['prediction'] = ''\n",
        "    for i in test_annot_seqs_bo.index:\n",
        "        this_seq_length = len(test_annot_seqs_bo['token'][i])\n",
        "        test_annot_seqs_bo['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    test_long = test_annot_seqs_bo.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = reverse_bio_from_bo(test_long['bio_only'])\n",
        "    test_long['bio_only'] = bio_labs\n",
        "    pred_labs = reverse_bio_from_bo(test_long['prediction'])\n",
        "    test_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(test_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 1.0:\n",
            "Sum of TP and FP = 1\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 0, False positives = 1, False negatives = 1067\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 5\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 0, False positives = 5, False negatives = 1067\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 9\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 0, False positives = 9, False negatives = 1067\n",
            "Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 24\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 1, False positives = 23, False negatives = 1066\n",
            "Precision = 0.042, Recall = 0.001, F1 = 0.002\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 57\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 7, False positives = 50, False negatives = 1060\n",
            "Precision = 0.123, Recall = 0.007, F1 = 0.012\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 147\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 26, False positives = 121, False negatives = 1041\n",
            "Precision = 0.177, Recall = 0.024, F1 = 0.043\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 340\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 95, False positives = 245, False negatives = 972\n",
            "Precision = 0.279, Recall = 0.089, F1 = 0.135\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 639\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 187, False positives = 452, False negatives = 880\n",
            "Precision = 0.293, Recall = 0.175, F1 = 0.219\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 1024\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 301, False positives = 723, False negatives = 766\n",
            "Precision = 0.294, Recall = 0.282, F1 = 0.288\n",
            "\n",
            "Weight = 0.1:\n",
            "Sum of TP and FP = 1452\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 378, False positives = 1074, False negatives = 689\n",
            "Precision = 0.260, Recall = 0.354, F1 = 0.300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdjS-pclAjTY",
        "outputId": "d8664870-1901-4e23-f8b8-44339d482ad8"
      },
      "source": [
        "# BiLSTM (PoS, 2-class, downsampled)\n",
        "for weight in [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]:\n",
        "    if weight == 1.0:\n",
        "        model = keras.models.load_model('BiLSTM_PoS_2class_downsampled.h5')\n",
        "    else:\n",
        "        model = keras.models.load_model(f'BiLSTM_PoS_2class_downsampled_downweight_{weight}.h5')\n",
        "\n",
        "    preds = np.argmax(model.predict([test_seqs_bo_padded, test_pos_bo_padded]), axis=-1)\n",
        "\n",
        "    test_annot_seqs_bo['prediction'] = ''\n",
        "    for i in test_annot_seqs_bo.index:\n",
        "        this_seq_length = len(test_annot_seqs_bo['token'][i])\n",
        "        test_annot_seqs_bo['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "    \n",
        "    test_long = test_annot_seqs_bo.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "    bio_labs = reverse_bio_from_bo(test_long['bio_only'])\n",
        "    test_long['bio_only'] = bio_labs\n",
        "    pred_labs = reverse_bio_from_bo(test_long['prediction'])\n",
        "    test_long['prediction'] = pred_labs\n",
        "\n",
        "    print(f'Weight = {weight}:')\n",
        "    wnut_evaluate(test_long)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight = 1.0:\n",
            "Sum of TP and FP = 161\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 29, False positives = 132, False negatives = 1038\n",
            "Precision = 0.180, Recall = 0.027, F1 = 0.047\n",
            "\n",
            "Weight = 0.9:\n",
            "Sum of TP and FP = 235\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 44, False positives = 191, False negatives = 1023\n",
            "Precision = 0.187, Recall = 0.041, F1 = 0.068\n",
            "\n",
            "Weight = 0.8:\n",
            "Sum of TP and FP = 326\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 73, False positives = 253, False negatives = 994\n",
            "Precision = 0.224, Recall = 0.068, F1 = 0.105\n",
            "\n",
            "Weight = 0.7:\n",
            "Sum of TP and FP = 434\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 105, False positives = 329, False negatives = 962\n",
            "Precision = 0.242, Recall = 0.098, F1 = 0.140\n",
            "\n",
            "Weight = 0.6:\n",
            "Sum of TP and FP = 587\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 154, False positives = 433, False negatives = 913\n",
            "Precision = 0.262, Recall = 0.144, F1 = 0.186\n",
            "\n",
            "Weight = 0.5:\n",
            "Sum of TP and FP = 813\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 243, False positives = 570, False negatives = 824\n",
            "Precision = 0.299, Recall = 0.228, F1 = 0.259\n",
            "\n",
            "Weight = 0.4:\n",
            "Sum of TP and FP = 1066\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 349, False positives = 717, False negatives = 718\n",
            "Precision = 0.327, Recall = 0.327, F1 = 0.327\n",
            "\n",
            "Weight = 0.3:\n",
            "Sum of TP and FP = 1301\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 428, False positives = 873, False negatives = 639\n",
            "Precision = 0.329, Recall = 0.401, F1 = 0.361\n",
            "\n",
            "Weight = 0.2:\n",
            "Sum of TP and FP = 1530\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 472, False positives = 1058, False negatives = 595\n",
            "Precision = 0.308, Recall = 0.442, F1 = 0.363\n",
            "\n",
            "Weight = 0.1:\n",
            "Sum of TP and FP = 1660\n",
            "Sum of TP and FN = 1067\n",
            "True positives = 456, False positives = 1204, False negatives = 611\n",
            "Precision = 0.275, Recall = 0.427, F1 = 0.334\n",
            "\n"
          ]
        }
      ]
    }
  ]
}