{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_L90_practical_session2_MT2021_xz398",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y2GAIotpgwn"
      },
      "source": [
        "**Overview of NLP (L90) Practical Session 2**\n",
        "\n",
        "Welcome to the second practical accompanying the Overview of NLP (L90) lecture course. Overall, the purpose of the practicals is to build and evaluate an NLP system.\n",
        "\n",
        "In this session we continue the 3-part practical task.\n",
        "\n",
        "1.   Explore and annotate a named entity recognition dataset (last time, worth 10%) [link to colab](https://colab.research.google.com/drive/1J_jXBEFfxbDDuI_NcJpZPmubpNQCG6Kf?usp=sharing)\n",
        "2.   **Attempt feature-based NER (this practical, assignment due 10 November 3pm, worth 10%)**\n",
        "3.   Attempt NER with neural networks and write a report (practical on 10 November, due 3 December 3pm, worth 80%)\n",
        "\n",
        "You might find it useful to watch the short video we recorded last year (on the [part II](https://www.cl.cam.ac.uk/teaching/2122/NLP/video/) and [ACS](https://www.cl.cam.ac.uk/teaching/2122/L90/video/) teaching pages). Also there will be an in-person discussion session on 27 October at 3pm in the Intel Lab: please come along to discuss the previous assignment and this new one!\n",
        "\n",
        "Note that all submissions are made via the [course Moodle page](https://www.vle.cam.ac.uk/course/view.php?id=206751), and that you can only view this Colab notebook (including the ability to run code blocks) not edit it: you can make a copy of your own in the File menu if you do want to edit anything, but there is no need to.\n",
        "\n",
        "Ok, let's begin the second practical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBFw8sHOsgvu"
      },
      "source": [
        "**Recap: Practical 1**\n",
        "\n",
        "Recall that we're working on the [W-NUT 2017 shared task](https://noisy-text.github.io/2017/emerging-rare-entities.html) on novel, emerging NER.\n",
        "\n",
        "In your first assignment you annotated 50 tweets for named entities. How did you find it? Tricky or easy, time-consuming or quick? We'll start to send you your agreement scores by email after the deadline (if you haven't received yours one week after the deadline, let us know: apc38). Were you surprised by your level of (dis)agreement with the original annotation and other students? Would you now revise or stand by your annotation decisions?\n",
        "\n",
        "Hopefully the assignment made you think a little more closely about the annotation side of machine learning & NLP: how subjective it can be, how important it is to get this part of research projects right, and how challenging that is -- especially when multiple annotation layers are required, and when the annotation task is more complex than word token labelling (such as syntactic parsing, or fact checking, for instance). Also, you might consider how the annotation process might contribute to bias in machine learning applications, depending on the task, data, annotator demographics, and so on.\n",
        "\n",
        "You may also have thought the task quite slow and laborious, but hopefully quite interesting as well. We only looked at 50 texts: bear in mind there are 5.5K in this dataset, so you'll realise why annotation is (a) hard work, (b) expensive -- because usually annotators need to be paid, (c) a little noisy (you probably noticed the non-English tweet, for instance). If you ever need to get some annotation done yourself, you might want to look into ways to distribute the task more widely (aka crowdsourcing), make the task more efficient with semi-supervised or active learning, or indeed explore some unsupervised machine learning methods (though you'll normally still want to annotate a test set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKqN_ij0BUFm"
      },
      "source": [
        "**Today: Practical 2**\n",
        "\n",
        "Onto today's task: we'll be constructing 'traditional' feature-based classifiers for named entity recognition (as opposed to neural networks which we'll look at next time).\n",
        "\n",
        "Your assignment is to write the code for one such classifier, perhaps using the features we'll describe below, but also some of your own. The requirement for a tick is to submit a labelled version of the test file which will allow us to evaluate your model. More about this below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU3K9ObkJXqZ"
      },
      "source": [
        "**Good old-fashioned NLP**\n",
        "\n",
        "Let's start with some good old-fashioned NLP on these texts (as opposed to modern NLP which often involves neural networks and word embeddings -- more on that next time). As mentioned in practical 1, the texts have already been word tokenized. \n",
        "\n",
        "Another common processing task is part-of-speech tagging. You'll have heard in the lectures that performance of PoS taggers for standard English text is very high. This means that we often use pre-trained taggers available in toolkits such as spaCy and Stanford NLP and trust them to work well out-of-the-box. Of course, this is a questionable assumption (for reasons presented in the lecture), but for the purpose of this assignment a pre-trained tagger is fine.\n",
        "\n",
        "Now, why are we PoS-tagging the text? Well we have an intuition that proper nouns are often named entities, and so we expect that PoS information will be a helpful feature for named entity recognition. Indeed, in the training set we find that a whopping 68% of named entities are proper nouns. It looks like PoS-tags will be a useful feature, even if we only craft a binary proper_noun=True/False feature.\n",
        "\n",
        "We already pre-processed the training, dev and test files with 'universal' PoS-tags, which are a much simplified set intended for cross-linguistic use as part of the Universal Dependencies project (see the list of all [17 universal tags](https://universaldependencies.org/u/pos/index.html)). Here's a preview:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr44_fWQH1Nr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "efb7d2d7-5cb2-4ff7-ce12-b12c0cebf3bb"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "wnuttrain = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt'\n",
        "train = pd.read_table(wnuttrain, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "train.head(n=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@paulwalk</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'s</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>AUX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>view</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>where</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>'m</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>living</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>for</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>two</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NUM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>weeks</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Empire</td>\n",
              "      <td>B-location</td>\n",
              "      <td>B</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>State</td>\n",
              "      <td>I-location</td>\n",
              "      <td>I</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Building</td>\n",
              "      <td>I-location</td>\n",
              "      <td>I</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>=</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>SYM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ESB</td>\n",
              "      <td>B-location</td>\n",
              "      <td>B</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token       label bio_only   upos\n",
              "0   @paulwalk           O        O   NOUN\n",
              "1          It           O        O   PRON\n",
              "2          's           O        O    AUX\n",
              "3         the           O        O    DET\n",
              "4        view           O        O   NOUN\n",
              "5        from           O        O    ADP\n",
              "6       where           O        O    ADV\n",
              "7           I           O        O   PRON\n",
              "8          'm           O        O      X\n",
              "9      living           O        O   NOUN\n",
              "10        for           O        O    ADP\n",
              "11        two           O        O    NUM\n",
              "12      weeks           O        O   NOUN\n",
              "13          .           O        O  PUNCT\n",
              "14     Empire  B-location        B  PROPN\n",
              "15      State  I-location        I  PROPN\n",
              "16   Building  I-location        I  PROPN\n",
              "17          =           O        O    SYM\n",
              "18        ESB  B-location        B  PROPN\n",
              "19          .           O        O  PUNCT"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uipkbis-Iqyk"
      },
      "source": [
        "You'll see that as before we have the word tokens and named entity labels in the first and second columns, as obtained from the original W-NUT 2017 shared task dataset. Now in the final column we've added UPOS tags, and in the third column you'll see that we've created a 'BIO only' value, which is the B, I or O copied from the full named entity label in column 2. We'll use this today for a less strict classification task.\n",
        "\n",
        "_Note that this is the starting format of all the files from now on: so however you work with the data for your assignments, make sure it can handle this format as input._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXjUXkCxKFzO"
      },
      "source": [
        "**Baseline classifiers**\n",
        "\n",
        "Let's load the dev data which we'll work with while developing our baseline classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx_xf2IrKX1z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "b7a52a77-b807-4fbd-83ff-5d48e5d7a494"
      },
      "source": [
        "wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'\n",
        "dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "dev.head(n=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stabilized</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>approach</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>or</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>CCONJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>not</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PART</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>?</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>That</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>´</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>SYM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>s</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PART</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>insane</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>and</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>CCONJ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token label bio_only   upos\n",
              "0  Stabilized     O        O  PROPN\n",
              "1    approach     O        O   NOUN\n",
              "2          or     O        O  CCONJ\n",
              "3         not     O        O   PART\n",
              "4           ?     O        O  PUNCT\n",
              "5        That     O        O   PRON\n",
              "6           ´     O        O    SYM\n",
              "7           s     O        O   PART\n",
              "8      insane     O        O    ADJ\n",
              "9         and     O        O  CCONJ"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql9p5Q43Ki1L"
      },
      "source": [
        "Note the tagging errors in rows 0 and 6: presumably because we haven't lower-cased all words (row 0) and because of the 'forward tick' rather than plain apostrophe (row 6). Re the former problem: what information would you lose if you do lower-case all words? We leave this as something for you to explore if you wish to.\n",
        "\n",
        "Now, how well can we do with a very naive baseline classifier? Often, your most basic baseline will be a random classifier, or majority-class (always predicting the most common label). Our labels are not evenly distributed (3.1% B, 1.9% I, 95% O), so a random baseline doesn't make sense. And the way that this task is evaluated, we have to positively make named entity predictions, so therefore using a majority-class baseline of labelling everything \"O\" isn't appropriate (there's more about evaluation below).\n",
        "\n",
        "Right, so let's use what we found out about proper nouns and try a baseline approach of identifying all proper nouns as named entities. This is what that looks like, first making a copy of dev so we can label it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1dPP9AFRAoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b740b2c-96cb-4b15-85a5-e5a17b0fa331"
      },
      "source": [
        "dev.dropna(inplace=True)  # drop empty rows between texts\n",
        "dev = dev.reset_index(drop=True)\n",
        "dev_copy = dev.copy()  #  make a copy of original data frame\n",
        "dev_copy['prediction'] = 'O'\n",
        "in_entity = 0\n",
        "for i in dev_copy.index:\n",
        "  if dev_copy['upos'][i]=='PROPN':\n",
        "    if in_entity==1:  # if a named entity in progress\n",
        "      dev_copy['prediction'][i] = 'I'\n",
        "    else:\n",
        "      dev_copy['prediction'][i] = 'B'\n",
        "      in_entity = 1\n",
        "  else:\n",
        "    in_entity = 0\n",
        "\n",
        "print(dev_copy.head())\n",
        "print('Total number of rows = %i' % len(dev_copy))\n",
        "dev_copy['prediction'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        token label bio_only   upos prediction\n",
            "0  Stabilized     O        O  PROPN          B\n",
            "1    approach     O        O   NOUN          O\n",
            "2          or     O        O  CCONJ          O\n",
            "3         not     O        O   PART          O\n",
            "4           ?     O        O  PUNCT          O\n",
            "Total number of rows = 15382\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O    14432\n",
              "B      837\n",
              "I      113\n",
              "Name: prediction, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET7Dqns8S6a8"
      },
      "source": [
        "Ok so that gives us 837 named entities, which seems reasonable. Let's see how many named entities were tagged in the dev set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zq_e_lmNmHN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7b128e-a743-4a4c-9960-32f08de85d20"
      },
      "source": [
        "dev_copy['bio_only'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O    14144\n",
              "B      826\n",
              "I      412\n",
              "Name: bio_only, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UsraK1zTfkP"
      },
      "source": [
        "How does the 'gold-standard' list of named entities compare to the ones we've naively predicted? This calls for an evaluation function: note that we're using the 'entity' rather than 'surface' F1 score used in the shared task (but without predicting entity type). The latter rewards repeating entities only once, so it's more like a vocabulary test, and it requires an entity type too, whereas we've so far only predicted BIO labels. And evaluation is done on whole named entity strings rather than token labels (hence it's a bit more involved than normal per-token evaluation).\n",
        "\n",
        "_Refer to the [shared task overview paper](https://aclanthology.org/W17-4418.pdf), section §4 for more about the evaluation methods used._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb1TyyTeTluO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5f68e9-5bc2-4ac1-b823-26024da52e76"
      },
      "source": [
        "def wnut_evaluate(txt):\n",
        "  '''entity evaluation: we evaluate by whole named entities'''\n",
        "  npred = 0; ngold = 0; tp = 0\n",
        "  nrows = len(txt)\n",
        "  for i in txt.index:\n",
        "    if txt['prediction'][i]=='B' and txt['bio_only'][i]=='B':\n",
        "      npred += 1\n",
        "      ngold += 1\n",
        "      for predfindbo in range((i+1),nrows):\n",
        "        if txt['prediction'][predfindbo]=='O' or txt['prediction'][predfindbo]=='B':\n",
        "          break  # find index of first O (end of entity) or B (new entity)\n",
        "      for goldfindbo in range((i+1),nrows):\n",
        "        if txt['bio_only'][goldfindbo]=='O' or txt['bio_only'][goldfindbo]=='B':\n",
        "          break  # find index of first O (end of entity) or B (new entity)\n",
        "      if predfindbo==goldfindbo:  # only count a true positive if the whole entity phrase matches\n",
        "        tp += 1\n",
        "    elif txt['prediction'][i]=='B':\n",
        "      npred += 1\n",
        "    elif txt['bio_only'][i]=='B':\n",
        "      ngold += 1\n",
        "  \n",
        "  fp = npred - tp  # n false predictions\n",
        "  fn = ngold - tp  # n missing gold entities\n",
        "  prec = tp / (tp+fp)\n",
        "  rec = tp / (tp+fn)\n",
        "  f1 = (2*(prec*rec)) / (prec+rec)\n",
        "  print('Sum of TP and FP = %i' % (tp+fp))\n",
        "  print('Sum of TP and FN = %i' % (tp+fn))\n",
        "  print('True positives = %i, False positives = %i, False negatives = %i' % (tp, fp, fn))\n",
        "  print('Precision = %.3f, Recall = %.3f, F1 = %.3f' % (prec, rec, f1))\n",
        " \n",
        "wnut_evaluate(dev_copy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of TP and FP = 837\n",
            "Sum of TP and FN = 826\n",
            "True positives = 371, False positives = 466, False negatives = 455\n",
            "Precision = 0.443, Recall = 0.449, F1 = 0.446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIexKcPp_LsW"
      },
      "source": [
        "Ok so we've got precision of .443, recall of .449 and an F1-measure (harmonic mean of precision and recall) of .446 on the dev set. That's actually not too bad given that the leading score (**on the test set**) in the 2017 shared task was .419 (we have checked and this approach _does not_ do so well on the test set, with <.4 entity F1).\n",
        "\n",
        "Why are there some false positives, you might wonder? Well recall that we noticed errors in the PoS-tagging, including incorrect PROPN tags. Also we recognise better than ever, after assignment 1, that human annotators can make mistakes or disagree about named entity labels. And finally, not all PROPN are named entities according to the annotation guidelines: we can for example find a name in text which does not unambiguously identify a person (i.e. it's one of their names only, a nickname, or a general comment about all Janes, all Harrys, etc).\n",
        "\n",
        "We know that the PROPN baseline is a really crude approach, and won't adequately capture the _novel_ and _emerging_ entities which are the target of this shared task. Note also that it wouldn't be an acceptable entry to the shared task because it makes no attempt to properly engage with the full range of entities, instead targeting proper nouns only. The errors and complications even with this simple baseline approach does reinforce that the task is hard. Bear in mind that participating in a shared task eventually requires writing a peer-reviewed paper, and that trivial entries would be rejected at this stage.\n",
        "\n",
        "This approach doesn't contribute anything new to our shared knowledge: it only makes use of existing technology (a pre-trained PoS-tagger). Besides, we've only made very basic use of the training data so far. So let's try to train a classifier based on that data. We'll augment the training data with new features ready for classifier training: these should be integer or logical values, which are suitable feature types for machine learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2OaRVblc0Hr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f5ccf65b-523f-45fb-d94f-7f25f7ecdbd3"
      },
      "source": [
        "# reload training file\n",
        "wnuttrain = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt'\n",
        "train = pd.read_table(wnuttrain, header=None, names=['token', 'label', 'bio_only', 'upos']).dropna()  # drop empty rows\n",
        "\n",
        "# in order to convert POS tags to integers: get the UPOS tagset\n",
        "pos_vocab = train.upos.unique().tolist()\n",
        "\n",
        "# feature 1: convert POS-tags to integers\n",
        "def pos_index(pos):\n",
        "  ind = pos_vocab.index(pos)\n",
        "  return ind\n",
        "\n",
        "# feature 2: is this a proper noun?\n",
        "def is_propn(pos):\n",
        "  resp = False\n",
        "  if pos=='PROPN':\n",
        "    resp = True\n",
        "  return resp\n",
        "\n",
        "# feature 3: is the first character a capital letter?\n",
        "def title_case(tok):\n",
        "  resp = False\n",
        "  if tok[0:1].isupper():\n",
        "    resp = True  # thanks Archie Barrett for spotting a typo here!\n",
        "  return resp\n",
        "\n",
        "# training labels: convert BIO to integers\n",
        "def bio_index(bio):\n",
        "  if bio=='B':\n",
        "    ind = 0\n",
        "  elif bio=='I':\n",
        "    ind = 1\n",
        "  elif bio=='O':\n",
        "    ind = 2\n",
        "  return ind\n",
        "\n",
        "# pass a data frame through our feature extractor\n",
        "def extract_features(txt):\n",
        "  txt.dropna(inplace=True)  # drop empty rows between texts\n",
        "  txt_copy = txt.reset_index(drop=True)\n",
        "  posinds = [pos_index(u) for u in txt_copy['upos']]\n",
        "  txt_copy['pos_indices'] = posinds\n",
        "  isprop = [is_propn(u) for u in txt_copy['upos']]\n",
        "  txt_copy['is_propn'] = isprop\n",
        "  tcase = [title_case(t) for t in txt_copy['token']]\n",
        "  txt_copy['title_case'] = tcase\n",
        "  bioints = [bio_index(b) for b in txt_copy['bio_only']]\n",
        "  txt_copy['bio_only'] = bioints\n",
        "  return txt_copy\n",
        "\n",
        "train_copy = extract_features(train)\n",
        "train_copy.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "      <th>pos_indices</th>\n",
              "      <th>is_propn</th>\n",
              "      <th>title_case</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@paulwalk</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>PRON</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'s</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>AUX</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>DET</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>view</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       token label  bio_only  upos  pos_indices  is_propn  title_case\n",
              "0  @paulwalk     O         2  NOUN            0     False       False\n",
              "1         It     O         2  PRON            1     False        True\n",
              "2         's     O         2   AUX            2     False       False\n",
              "3        the     O         2   DET            3     False       False\n",
              "4       view     O         2  NOUN            0     False       False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KttSejk9mLuu"
      },
      "source": [
        "Ok what we've done here is: (a) converted our PoS-tags to integers ('pos_indices'), (b) produced some logical features based on properties we associate with named entities, namely being a proper noun and having a capital first character ('is_propn', 'title_case'), and (c) converted the 'bio_only' values to integers. These are all suitable feature types for machine learning.\n",
        "\n",
        "We need to prepare the development data in the exact same way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfL7cIOnnM9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e0ff487d-bd3d-42ba-b4c1-e44cb1c021dd"
      },
      "source": [
        "wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'\n",
        "dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos']).dropna()\n",
        "\n",
        "dev_copy = extract_features(dev)\n",
        "dev_copy.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "      <th>pos_indices</th>\n",
              "      <th>is_propn</th>\n",
              "      <th>title_case</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stabilized</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>9</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>approach</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>or</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>CCONJ</td>\n",
              "      <td>15</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>not</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>PART</td>\n",
              "      <td>14</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>?</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token label  bio_only   upos  pos_indices  is_propn  title_case\n",
              "0  Stabilized     O         2  PROPN            9      True        True\n",
              "1    approach     O         2   NOUN            0     False       False\n",
              "2          or     O         2  CCONJ           15     False       False\n",
              "3         not     O         2   PART           14     False       False\n",
              "4           ?     O         2  PUNCT            8     False       False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4_BMQuhnQLd"
      },
      "source": [
        "Now we can drop the unnecessary columns ('token', 'label', 'upos') and split our training file into _X_ and _y_: where _X_ are the features, and _y_ are the 'bio_only' labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NbnG0UOn_GP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a9fe2b-70eb-436a-9482-defec868590b"
      },
      "source": [
        "X_train = train_copy.drop(['token', 'label', 'bio_only', 'upos'], axis=1)\n",
        "y_train = train_copy['bio_only']\n",
        "print(X_train.head())\n",
        "y_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   pos_indices  is_propn  title_case\n",
            "0            0     False       False\n",
            "1            1     False        True\n",
            "2            2     False       False\n",
            "3            3     False       False\n",
            "4            0     False       False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2\n",
              "1    2\n",
              "2    2\n",
              "3    2\n",
              "4    2\n",
              "Name: bio_only, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXG6CYyHoOCw"
      },
      "source": [
        "With _X_ and _y_ ready to go, we can use scikit-learn to load and fit a logistic regression model in 'multinomial' mode (as opposed to 'binomial' when there are only two classes): logistic regression being a relatively simple but reliably good classifier which can be hard to beat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqH-PolEl18U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9098c580-cb0c-46e5-d794-5c72434dbc00"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(random_state=0, multi_class='multinomial', penalty='none', solver='newton-cg').fit(X_train, y_train)\n",
        "\n",
        "logreg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='none',\n",
              "                   random_state=0, solver='newton-cg', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luPC89FfpGlw"
      },
      "source": [
        "We can evaluate this model on the development set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEqMpvXMpKIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09eadf7-efd1-4944-98f2-a9fce7b7316f"
      },
      "source": [
        "X_dev = dev_copy.drop(['token', 'label', 'bio_only', 'upos'], axis=1)\n",
        "y_dev = dev_copy['bio_only']\n",
        "preds = logreg.predict(X_dev)\n",
        "\n",
        "(unique, counts) = np.unique(preds, return_counts=True)\n",
        "print('Predicted label, Count of labels')\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label, Count of labels\n",
            "[[    2 15382]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItkcyV8-rLOG"
      },
      "source": [
        "Oh. But here's a problem: our classifier has only predicted outside=2 for all tokens in the dev file! No 'begin' or 'inside' predictions. Hmm, we expect this is because the training classes are so imbalanced. Recall that named entities are relatively rare in our training texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AtquAerfdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa7306a-ff3d-4e6e-d044-509cc3896b39"
      },
      "source": [
        "train.bio_only.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O    59095\n",
              "B     1964\n",
              "I     1177\n",
              "Name: bio_only, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inQIE7cJrwVB"
      },
      "source": [
        "The 2 'outside' class far outnumbers the others, meaning that in training the obvious strategy for the model is to learn to predict all labels as non named entities. One alternative is to randomly down-sample the 'outside' tokens, which means that we will lose all access to context (so don't do this if you plan to use n-gram features), but might encourage the model to predict all possible classes and not just one.\n",
        "\n",
        "Ok let's start again, and this time take just 3000 'O' tokens at random (approx the sum of 'B' and 'I' tokens in the training data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU0KIca-k8Tq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358a1cb9-d744-40e8-fc74-cce8f07c40a3"
      },
      "source": [
        "# reload\n",
        "wnuttrain = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt'\n",
        "train = pd.read_table(wnuttrain, header=None, names=['token', 'label', 'bio_only', 'upos']).dropna()\n",
        "\n",
        "# split into B&I versus O subsets\n",
        "is_inside = train['bio_only']!='O'\n",
        "is_outside = train['bio_only']=='O'\n",
        "bi = train[is_inside]\n",
        "outside = train[is_outside]\n",
        "outside = outside.sample(n=3000)  # approx the sum of B and I labels in train\n",
        "\n",
        "# recombine\n",
        "train = pd.concat([bi, outside])\n",
        "print('Down-sampled data:')\n",
        "train.bio_only.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Down-sampled data:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O    3000\n",
              "B    1964\n",
              "I    1177\n",
              "Name: bio_only, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhEyhviztXlR"
      },
      "source": [
        "Ok that's our new downsampled training set. We can run feature extraction and do the same for the dev set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtSferRQtUSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcfc6aff-ab86-41e3-a32a-1eda17275b78"
      },
      "source": [
        "train_copy = extract_features(train)\n",
        "print('Training file preview:')\n",
        "print(train_copy.head())\n",
        "\n",
        "# reload\n",
        "wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'\n",
        "dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos']).dropna()\n",
        "\n",
        "dev_copy = extract_features(dev)\n",
        "print('Dev file preview:')\n",
        "print(dev_copy.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file preview:\n",
            "      token       label  bio_only   upos  pos_indices  is_propn  title_case\n",
            "0    Empire  B-location         0  PROPN            9      True        True\n",
            "1     State  I-location         1  PROPN            9      True        True\n",
            "2  Building  I-location         1  PROPN            9      True        True\n",
            "3       ESB  B-location         0  PROPN            9      True        True\n",
            "4      AHFA     B-group         0  PROPN            9      True        True\n",
            "Dev file preview:\n",
            "        token label  bio_only   upos  pos_indices  is_propn  title_case\n",
            "0  Stabilized     O         2  PROPN            9      True        True\n",
            "1    approach     O         2   NOUN            0     False       False\n",
            "2          or     O         2  CCONJ           15     False       False\n",
            "3         not     O         2   PART           14     False       False\n",
            "4           ?     O         2  PUNCT            8     False       False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXB2oR3EtoW_"
      },
      "source": [
        "Then fit the model again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Ko6JO1tzHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c3cd5d-8ae3-4381-d4eb-7dc9534eacd4"
      },
      "source": [
        "X_train = train_copy.drop(['token', 'label', 'bio_only', 'upos'], axis=1)\n",
        "y_train = train_copy['bio_only']\n",
        "logreg = LogisticRegression(random_state=0, multi_class='multinomial', penalty='none', solver='newton-cg').fit(X_train, y_train)\n",
        "logreg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='none',\n",
              "                   random_state=0, solver='newton-cg', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FImw7zt2-A"
      },
      "source": [
        "And evaluate on dev:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTQhhD6mt9eq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7c5ee5-df1f-4aff-9321-02bf4f665740"
      },
      "source": [
        "X_dev = dev_copy.drop(['token', 'label', 'bio_only', 'upos'], axis=1)\n",
        "y_dev = dev_copy['bio_only']\n",
        "preds = logreg.predict(X_dev)\n",
        "\n",
        "(unique, counts) = np.unique(preds, return_counts=True)\n",
        "print('Predicted label, Count of labels')\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label, Count of labels\n",
            "[[    0  1983]\n",
            " [    2 13399]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi9kDJpPuC1B"
      },
      "source": [
        "This time the model has attempted to predict more than just 'outside' labels. And so we can get a measure of performance, having converted the BIO integers back to character values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJNBdmqnpYFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30aeaf64-e2a7-48c9-e0f8-fa3a8e4725d8"
      },
      "source": [
        "def reverse_bio(ind):\n",
        "  if ind==0:\n",
        "    bio = 'B'\n",
        "  elif ind==1:\n",
        "    bio = 'I'\n",
        "  elif ind==2:\n",
        "    bio = 'O'\n",
        "  return bio\n",
        "\n",
        "bio_labs = [reverse_bio(b) for b in dev_copy['bio_only']]\n",
        "dev_copy['bio_only'] = bio_labs\n",
        "bio_preds = [reverse_bio(p) for p in preds]\n",
        "dev_copy['prediction'] = bio_preds\n",
        "print(dev_copy.head())\n",
        "\n",
        "print('New evaluation:')\n",
        "wnut_evaluate(dev_copy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        token label bio_only  ... is_propn  title_case  prediction\n",
            "0  Stabilized     O        O  ...     True        True           B\n",
            "1    approach     O        O  ...    False       False           O\n",
            "2          or     O        O  ...    False       False           O\n",
            "3         not     O        O  ...    False       False           O\n",
            "4           ?     O        O  ...    False       False           O\n",
            "\n",
            "[5 rows x 8 columns]\n",
            "New evaluation:\n",
            "Sum of TP and FP = 1983\n",
            "Sum of TP and FN = 826\n",
            "True positives = 371, False positives = 1612, False negatives = 455\n",
            "Precision = 0.187, Recall = 0.449, F1 = 0.264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soaZHxrGuXSd"
      },
      "source": [
        "And there we are: a proper attempt at classification but this particular approach is worse than our baseline of predicting that all proper nouns are named entities! Clearly there's room for improvement: so far we only made use of 3 fairly basic features.\n",
        "\n",
        "As well as, or instead of these, we could try making proper use of the word tokens, context, or other properties of named entities you may be aware of or have noticed during annotation. This is where we pass the task over to you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPeY8Dq4D8HR"
      },
      "source": [
        "**Assignment 2**\n",
        "\n",
        "Your assignment involves writing a Python script which trains a feature-based named entity classifier using the W-NUT 2017 training file available [here](https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt). Note that the development file is also available to you [here](https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt). You can use the code in this notebook as your starting point, or start from scratch. Once you're happy with your classifier, you can apply it to predict labels on the test set, available [here](https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_clean_tagged.txt). \n",
        "\n",
        "To be clear, you'll need to read in the test file with something like the following, replacing the values in the 'prediction' column with those output by your classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmShN2uWmr8I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b676de61-0a2a-4330-fe88-a39ab19666db"
      },
      "source": [
        "wnuttest = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_clean_tagged.txt'\n",
        "testset = pd.read_table(wnuttest, header=None, names=['token', 'upos']).dropna()\n",
        "testset['prediction'] = 'O'\n",
        "testset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>upos</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&amp;</td>\n",
              "      <td>CCONJ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gt</td>\n",
              "      <td>X</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>;</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>*</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The</td>\n",
              "      <td>DET</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  token   upos prediction\n",
              "0     &  CCONJ          O\n",
              "1    gt      X          O\n",
              "2     ;  PUNCT          O\n",
              "3     *  PUNCT          O\n",
              "4   The    DET          O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITbC9R9emth4"
      },
      "source": [
        "Once you've done that, you need to write the table as a **tab-delimited file** which you can submit to us via Moodle. **Please do include the column headings in your output file, but not row names / indices**. We can then let you know your precision, recall and F-measure on the test set, which will not affect your tick but you might be interested to know these metrics all the same.\n",
        "\n",
        "You'll need to write more code than is given here, for a tick: it shouldn't just be a copy of the code found in this notebook. You might try: a different PoS-tagger which outputs a more fine-grained PoS-tagset, making use of the word tokens, other character casing patterns, context from n-grams, other sklearn classifiers, and so on. (But bear in mind this is only 10% of your grade, so don't get too carried away: the idea is to just to get you thinking about feature extraction and the NER task some more).\n",
        "\n",
        "Before the deadline in 2 weeks, please submit 2 files **with your CRSid in the filenames** in the appropriate place on [Moodle](https://www.vle.cam.ac.uk/course/view.php?id=206751): (1) your Python script, (2) the test set with your predicted labels in the 'prediction' column (**please check the format of this file: any formatting problems and we won't be able to evaluate your predictions --> no tick!**). Any queries please get in touch (apc38)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RM1OcM83prvc",
        "outputId": "0b991f72-e96f-4469-fcd4-98d02ad433d0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# reload training file\n",
        "wnuttrain = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt'\n",
        "train = pd.read_table(wnuttrain, header=None, names=['token', 'label', 'bio_only', 'upos']).dropna()  # drop empty rows\n",
        "\n",
        "# in order to convert POS tags to integers: get the UPOS tagset\n",
        "pos_vocab = train.upos.unique().tolist()\n",
        "\n",
        "# feature 1: convert POS-tags to integers\n",
        "def pos_index(pos):\n",
        "  ind = pos_vocab.index(pos)\n",
        "  return ind\n",
        "\n",
        "# feature 2: is this a proper noun?\n",
        "def is_propn(pos):\n",
        "  resp = False\n",
        "  if pos=='PROPN':\n",
        "    resp = True\n",
        "  return resp\n",
        "\n",
        "# feature 3: is this a noun?\n",
        "def is_noun(pos):\n",
        "  resp = False\n",
        "  if pos=='NOUN':\n",
        "    resp = True\n",
        "  return resp\n",
        "\n",
        "# feature 4: is the first character a capital letter?\n",
        "def title_case(tok):\n",
        "  resp = False\n",
        "  if tok[0:1].isupper():\n",
        "    resp = True \n",
        "  return resp\n",
        "\n",
        "# feature 5: does the word contain a capical letter?\n",
        "def has_capital_letter(tok):\n",
        "  resp = False\n",
        "  for letter in tok:\n",
        "    if letter.isupper():\n",
        "        resp = True\n",
        "  return resp\n",
        "\n",
        "# training labels: convert BIO to integers\n",
        "def bio_index(bio):\n",
        "  if bio=='B':\n",
        "    ind = 0\n",
        "  elif bio=='I':\n",
        "    ind = 1\n",
        "  elif bio=='O':\n",
        "    ind = 2\n",
        "  return ind\n",
        "\n",
        "# pass a data frame through my new feature extractor\n",
        "def extract_features(txt):\n",
        "  txt.dropna(inplace=True)  # drop empty rows between texts\n",
        "  txt_copy = txt.reset_index(drop=True)\n",
        "  posinds = [pos_index(u) for u in txt_copy['upos']]\n",
        "  txt_copy['pos_indices'] = posinds\n",
        "  isprop = [is_propn(u) for u in txt_copy['upos']]\n",
        "  txt_copy['is_propn'] = isprop\n",
        "  isnoun = [is_noun(u) for u in txt_copy['upos']]\n",
        "  txt_copy['is_noun'] = isnoun\n",
        "  tcase = [title_case(t) for t in txt_copy['token']]\n",
        "  txt_copy['title_case'] = tcase\n",
        "  hascaps = [has_capital_letter(t) for t in txt_copy['token']]\n",
        "  txt_copy['has_capital_letter'] = hascaps\n",
        "  bioints = [bio_index(b) for b in txt_copy['bio_only']]\n",
        "  txt_copy['bio_only'] = bioints\n",
        "  return txt_copy\n",
        "\n",
        "train_copy = extract_features(train)\n",
        "train_copy.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "      <th>pos_indices</th>\n",
              "      <th>is_propn</th>\n",
              "      <th>is_noun</th>\n",
              "      <th>title_case</th>\n",
              "      <th>has_capital_letter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@paulwalk</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>PRON</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'s</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>AUX</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>DET</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>view</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       token label  bio_only  ... is_noun  title_case  has_capital_letter\n",
              "0  @paulwalk     O         2  ...    True       False               False\n",
              "1         It     O         2  ...   False        True                True\n",
              "2         's     O         2  ...   False       False               False\n",
              "3        the     O         2  ...   False       False               False\n",
              "4       view     O         2  ...    True       False               False\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "bi2fV1NvrU9l",
        "outputId": "1aed88f0-ffed-4145-844f-4e76ea1f5ac0"
      },
      "source": [
        "wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'\n",
        "dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos']).dropna()\n",
        "\n",
        "dev_copy = extract_features(dev)\n",
        "dev_copy.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "      <th>pos_indices</th>\n",
              "      <th>is_propn</th>\n",
              "      <th>is_noun</th>\n",
              "      <th>title_case</th>\n",
              "      <th>has_capital_letter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stabilized</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>9</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>approach</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>or</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>CCONJ</td>\n",
              "      <td>15</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>not</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>PART</td>\n",
              "      <td>14</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>?</td>\n",
              "      <td>O</td>\n",
              "      <td>2</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token label  bio_only  ... is_noun  title_case  has_capital_letter\n",
              "0  Stabilized     O         2  ...   False        True                True\n",
              "1    approach     O         2  ...    True       False               False\n",
              "2          or     O         2  ...   False       False               False\n",
              "3         not     O         2  ...   False       False               False\n",
              "4           ?     O         2  ...   False       False               False\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqNxKYvTr2fz",
        "outputId": "a8db459b-f944-44e7-899c-92bb3810e911"
      },
      "source": [
        "# reload training file\n",
        "wnuttrain = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt'\n",
        "train = pd.read_table(wnuttrain, header=None, names=['token', 'label', 'bio_only', 'upos']).dropna()\n",
        "\n",
        "# split into B&I versus O subsets\n",
        "is_inside = train['bio_only']!='O'\n",
        "is_outside = train['bio_only']=='O'\n",
        "bi = train[is_inside]\n",
        "outside = train[is_outside]\n",
        "outside = outside.sample(n=3000)  # approx the sum of B and I labels in train\n",
        "\n",
        "# recombine\n",
        "train = pd.concat([bi, outside])\n",
        "print('Down-sampled data:')\n",
        "train.bio_only.value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Down-sampled data:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O    3000\n",
              "B    1964\n",
              "I    1177\n",
              "Name: bio_only, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THMlQqK_sAZ6",
        "outputId": "b1485e64-8a80-443c-f43e-ce15ff76fcc3"
      },
      "source": [
        "train_copy = extract_features(train)\n",
        "print('Training file preview:')\n",
        "print(train_copy.head())\n",
        "\n",
        "# reload dev file\n",
        "wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'\n",
        "dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos']).dropna()\n",
        "\n",
        "dev_copy = extract_features(dev)\n",
        "print('Dev file preview:')\n",
        "print(dev_copy.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file preview:\n",
            "      token       label  bio_only  ... is_noun  title_case  has_capital_letter\n",
            "0    Empire  B-location         0  ...   False        True                True\n",
            "1     State  I-location         1  ...   False        True                True\n",
            "2  Building  I-location         1  ...   False        True                True\n",
            "3       ESB  B-location         0  ...   False        True                True\n",
            "4      AHFA     B-group         0  ...   False        True                True\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "Dev file preview:\n",
            "        token label  bio_only  ... is_noun  title_case  has_capital_letter\n",
            "0  Stabilized     O         2  ...   False        True                True\n",
            "1    approach     O         2  ...    True       False               False\n",
            "2          or     O         2  ...   False       False               False\n",
            "3         not     O         2  ...   False       False               False\n",
            "4           ?     O         2  ...   False       False               False\n",
            "\n",
            "[5 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufecWqiCrlyL",
        "outputId": "8910dd72-2da1-4be1-ebb7-5ef5ef66ee4f"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train = train_copy.drop(['token', 'label', 'bio_only', 'upos'], axis=1)\n",
        "y_train = train_copy['bio_only']\n",
        "logreg = LogisticRegression(random_state=0, multi_class='multinomial', penalty='none', solver='newton-cg').fit(X_train, y_train)\n",
        "\n",
        "logreg"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='none',\n",
              "                   random_state=0, solver='newton-cg', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xaea5N1zrq2_",
        "outputId": "dc659791-06e3-48e2-db5b-ad4605eb4cfc"
      },
      "source": [
        "X_dev = dev_copy.drop(['token', 'label', 'bio_only', 'upos'], axis=1)\n",
        "y_dev = dev_copy['bio_only']\n",
        "preds = logreg.predict(X_dev)\n",
        "\n",
        "(unique, counts) = np.unique(preds, return_counts=True)\n",
        "print('Predicted label, Count of labels')\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label, Count of labels\n",
            "[[    0  1330]\n",
            " [    2 14052]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVOmhRMG6q54"
      },
      "source": [
        "def wnut_evaluate(txt):\n",
        "  '''entity evaluation: we evaluate by whole named entities'''\n",
        "  npred = 0; ngold = 0; tp = 0\n",
        "  nrows = len(txt)\n",
        "  for i in txt.index:\n",
        "    if txt['prediction'][i]=='B' and txt['bio_only'][i]=='B':\n",
        "      npred += 1\n",
        "      ngold += 1\n",
        "      for predfindbo in range((i+1),nrows):\n",
        "        if txt['prediction'][predfindbo]=='O' or txt['prediction'][predfindbo]=='B':\n",
        "          break  # find index of first O (end of entity) or B (new entity)\n",
        "      for goldfindbo in range((i+1),nrows):\n",
        "        if txt['bio_only'][goldfindbo]=='O' or txt['bio_only'][goldfindbo]=='B':\n",
        "          break  # find index of first O (end of entity) or B (new entity)\n",
        "      if predfindbo==goldfindbo:  # only count a true positive if the whole entity phrase matches\n",
        "        tp += 1\n",
        "    elif txt['prediction'][i]=='B':\n",
        "      npred += 1\n",
        "    elif txt['bio_only'][i]=='B':\n",
        "      ngold += 1\n",
        "  \n",
        "  fp = npred - tp  # n false predictions\n",
        "  fn = ngold - tp  # n missing gold entities\n",
        "  prec = tp / (tp+fp)\n",
        "  rec = tp / (tp+fn)\n",
        "  f1 = (2*(prec*rec)) / (prec+rec)\n",
        "  print('Sum of TP and FP = %i' % (tp+fp))\n",
        "  print('Sum of TP and FN = %i' % (tp+fn))\n",
        "  print('True positives = %i, False positives = %i, False negatives = %i' % (tp, fp, fn))\n",
        "  print('Precision = %.3f, Recall = %.3f, F1 = %.3f' % (prec, rec, f1))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6Rc7b-fsfnW",
        "outputId": "36870d7c-83d1-4521-9f3a-31666a9712a1"
      },
      "source": [
        "def reverse_bio(ind):\n",
        "  if ind==0:\n",
        "    bio = 'B'\n",
        "  elif ind==1:\n",
        "    bio = 'I'\n",
        "  elif ind==2:\n",
        "    bio = 'O'\n",
        "  return bio\n",
        "\n",
        "bio_labs = [reverse_bio(b) for b in dev_copy['bio_only']]\n",
        "dev_copy['bio_only'] = bio_labs\n",
        "bio_preds = [reverse_bio(p) for p in preds]\n",
        "dev_copy['prediction'] = bio_preds\n",
        "print(dev_copy.head())\n",
        "\n",
        "print('New evaluation:')\n",
        "wnut_evaluate(dev_copy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        token label bio_only  ... title_case  has_capital_letter  prediction\n",
            "0  Stabilized     O        O  ...       True                True           B\n",
            "1    approach     O        O  ...      False               False           O\n",
            "2          or     O        O  ...      False               False           O\n",
            "3         not     O        O  ...      False               False           O\n",
            "4           ?     O        O  ...      False               False           O\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "New evaluation:\n",
            "Sum of TP and FP = 1330\n",
            "Sum of TP and FN = 826\n",
            "True positives = 366, False positives = 964, False negatives = 460\n",
            "Precision = 0.275, Recall = 0.443, F1 = 0.340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa3J7WYGv1tN"
      },
      "source": [
        "# pass the test data frame through my new feature extractor\n",
        "def extract_test_features(txt):\n",
        "  txt.dropna(inplace=True)  # drop empty rows between texts\n",
        "  txt_copy = txt.reset_index(drop=True)\n",
        "  posinds = [pos_index(u) for u in txt_copy['upos']]\n",
        "  txt_copy['pos_indices'] = posinds\n",
        "  isprop = [is_propn(u) for u in txt_copy['upos']]\n",
        "  txt_copy['is_propn'] = isprop\n",
        "  isnoun = [is_noun(u) for u in txt_copy['upos']]\n",
        "  txt_copy['is_noun'] = isnoun\n",
        "  tcase = [title_case(t) for t in txt_copy['token']]\n",
        "  txt_copy['title_case'] = tcase\n",
        "  hascaps = [has_capital_letter(t) for t in txt_copy['token']]\n",
        "  txt_copy['has_capital_letter'] = hascaps\n",
        "  return txt_copy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12gDMTVOvQ3t",
        "outputId": "4d9acf1b-374c-4a51-c721-0a88bcb12ed1"
      },
      "source": [
        "# load test file\n",
        "wnuttest = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_clean_tagged.txt'\n",
        "test = pd.read_table(wnuttest, header=None, names=['token', 'upos']).dropna()\n",
        "\n",
        "test_copy = extract_test_features(test)\n",
        "print('Test file preview:')\n",
        "print(test_copy.head())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test file preview:\n",
            "  token   upos  pos_indices  is_propn  is_noun  title_case  has_capital_letter\n",
            "0     &  CCONJ           15     False    False       False               False\n",
            "1    gt      X            6     False    False       False               False\n",
            "2     ;  PUNCT            8     False    False       False               False\n",
            "3     *  PUNCT            8     False    False       False               False\n",
            "4   The    DET            3     False    False        True                True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GmVwq9Rv96x",
        "outputId": "a20d27e1-df71-4914-a451-68af93813f69"
      },
      "source": [
        "X_test = test_copy.drop(['token', 'upos'], axis=1)\n",
        "preds = logreg.predict(X_test)\n",
        "\n",
        "(unique, counts) = np.unique(preds, return_counts=True)\n",
        "print('Predicted label, Count of labels')\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label, Count of labels\n",
            "[[    0  2355]\n",
            " [    2 20968]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfhMRgoJwogA",
        "outputId": "43508d2f-14fe-43cf-fa5a-c9cd1eb799a6"
      },
      "source": [
        "bio_preds = [reverse_bio(p) for p in preds]\n",
        "test_copy['prediction'] = bio_preds\n",
        "print(test_copy[30:40])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        token   upos  pos_indices  ...  title_case  has_capital_letter  prediction\n",
            "30          *  PUNCT            8  ...       False               False           O\n",
            "31     Police   NOUN            0  ...        True                True           B\n",
            "32       last    ADJ           11  ...       False               False           O\n",
            "33       week   NOUN            0  ...       False               False           O\n",
            "34  evacuated   VERB           13  ...       False               False           O\n",
            "35         80    NUM            7  ...       False               False           O\n",
            "36  villagers   NOUN            0  ...       False               False           O\n",
            "37       from    ADP            4  ...       False               False           O\n",
            "38  Waltengoo  PROPN            9  ...        True                True           B\n",
            "39        Nar  PROPN            9  ...        True                True           B\n",
            "\n",
            "[10 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QC0cwS7wNC"
      },
      "source": [
        "test_output = test_copy.drop(['pos_indices', 'is_propn', 'is_noun', 'title_case', 'has_capital_letter'], axis=1)\n",
        "test_output.to_csv('wnut17test_clean_tagged_prediction_xz398.txt', sep='\\t', index=False, header=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "vHMl76Nz8Zxt",
        "outputId": "9d1684f4-41cb-42ad-a18c-130918549517"
      },
      "source": [
        "test_output_check = pd.read_table('wnut17test_clean_tagged_prediction_xz398.txt').dropna()\n",
        "test_output_check[30:40]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>upos</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>*</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Police</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>last</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>week</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>evacuated</td>\n",
              "      <td>VERB</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>80</td>\n",
              "      <td>NUM</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>villagers</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>from</td>\n",
              "      <td>ADP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Waltengoo</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Nar</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token   upos prediction\n",
              "30          *  PUNCT          O\n",
              "31     Police   NOUN          B\n",
              "32       last    ADJ          O\n",
              "33       week   NOUN          O\n",
              "34  evacuated   VERB          O\n",
              "35         80    NUM          O\n",
              "36  villagers   NOUN          O\n",
              "37       from    ADP          O\n",
              "38  Waltengoo  PROPN          B\n",
              "39        Nar  PROPN          B"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}